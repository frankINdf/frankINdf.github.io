<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阳阳的博客</title>
  
  <subtitle>学无止境</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://frankindf.github.io/"/>
  <updated>2018-04-22T10:47:31.519Z</updated>
  <id>https://frankindf.github.io/</id>
  
  <author>
    <name>阳阳</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="https://frankindf.github.io/2018/04/21/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://frankindf.github.io/2018/04/21/SVM支持向量机/</id>
    <published>2018-04-21T10:46:53.000Z</published>
    <updated>2018-04-22T10:47:31.519Z</updated>
    
    <content type="html"><![CDATA[<p>SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。</p><a id="more"></a><p>SVM分类使用sigmoid函数判断结果。</p><p><strong>几何角度的解释</strong>：不同分类之边界之间间隔最大化<br>$$<br>{\min_{w,b}}\frac{1}{2}\left | w \right |^2<br>$$</p><p>$$<br>s.t. y_i(w^Tx_i+b)\geqslant 1 , i=1,2…,m.<br>$$</p><p><strong>损失函数最小解释</strong>：$1/2||w||^2$为结构化风险的L2正则项，损失函数为合页函数$l_{0/1}$为$max(0,f(x))$，<br>$$<br>{\min_{w,b}}\frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}(y_i(w^Tx_i+b)-1)<br>$$</p><p>$$<br>s.t     (y_i(w^Tx_i+b)-1)\leqslant 1-\xi _i<br>$$</p><p>$$<br>\xi _i\geqslant 0<br>$$</p><p>采用拉格朗日法：<br>$$<br>L(w,b,\xi,\alpha,\mu)=\frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}+\sum_{m}^{i=1}\alpha_i[y_i(w^Tx_i+b-1+l_{0/1})]-\sum_{m}^{i=1}{\mu_i\xi_i}<br>$$<br>依次对$w,b,\xi_i$求偏导，得到<br>$$<br>\min_{w,b,\xi}L = \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$<br>原始问题满足KKT条件，对偶问题变为：<br>$$<br>\max \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$</p><p>$$<br>s.t.\space\sum_{j=1}^{N}a_iy_i=0<br>$$</p><p>$$<br>0 \leqslant a_iy_i \leqslant C<br>$$</p><p><strong>惩罚因子</strong>C: C代表对损失函数的惩罚程度，C越大损失函数对优化影响越大，损失函数取的系数C正无穷时，不允许有异常点，即为硬间隔。</p><p><strong>损失函数</strong>： $l_{0/1}$理解为$y_i(w^Tx_i+b)-1&gt;0$时无损失，无需继续优化，这里可以看到$l_{0/1}$是“没有追求”的，只要预测正确就不再努力使预测值向正方向增加，这也使SVM算法可以对异常值有一定“容忍”。</p><p><strong>核函数：</strong>把低维度的数据映射到高维度，用空间变换找数据的分割面，找到分割面后再转换到原空间，即可画出分界边界，例如$x^2+y^2+b$ 在新空间$\alpha+\beta+b$为直线，在原空间线性不可分的点在新空间里线性可分。</p><p>RBF核$K(x,x_i)=exp(\frac{||x-x_i||^2}{σ^2})$会将原始空间映射为无穷维空间,$σ$ 选得大，高次特征上的权衰减变快， <em>σ</em> 选得小，可以将任意的数据映射为线性可分，可能过拟合。<strong>高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。</strong></p><p><strong>SMO算法：</strong>简单理解SMO就是选定$a_i,a_j$将其他项看做常数，可以用$a_i$表示$a_j$,再在[0,C]这个区间求解极值，依次循环。</p><p><strong>注意</strong></p><p>SVM没有处理缺失值的策略，使用SVM前需要将数据进行处理</p><p>例1：SVM示例</p><p>绘制SVM分类示意图，用clf.coef_和clf.intercept获得w和b，用clf.support_vectors_获得支持向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import svm</span><br><span class="line">import pylab as pl</span><br><span class="line">np.random.seed(0) # 使用相同的seed()值，则每次生成的随即数都相同</span><br><span class="line"># 创建可线性分类的数据集与结果集</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20,2) + [2, 2]]</span><br><span class="line">Y = [0] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line"># 构造 SVM 模型</span><br><span class="line">clf = svm.SVC(kernel=&apos;linear&apos;)</span><br><span class="line">clf.fit(X, Y) # 训练 </span><br><span class="line">#利用clf.coef_得到系数w</span><br><span class="line">#clf.intercept_[0]是b</span><br><span class="line">xx = np.linspace(-5, 5) # 在区间[-5, 5] 中产生连续的值，用于画线</span><br><span class="line">yy =  -w[0] * xx / w[1]  - (clf.intercept_[0]) / w[1]</span><br><span class="line">b = clf.support_vectors_[0] # 第一个分类的支持向量，通过调整系数b</span><br><span class="line">yy_down = a * xx + (b[1] - a * b[0])</span><br><span class="line">b = clf.support_vectors_[-1] # 第二个分类中的支持向量</span><br><span class="line">yy_up = a * xx + (b[1] - a * b[0])</span><br><span class="line"></span><br><span class="line">pl.plot(xx, yy, &apos;k-&apos;)</span><br><span class="line">pl.plot(xx, yy_down, &apos;k--&apos;)</span><br><span class="line">pl.plot(xx, yy_up, &apos;k--&apos;)</span><br><span class="line">pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],</span><br><span class="line">           s=80, facecolors=&apos;none&apos;)</span><br><span class="line">pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)</span><br><span class="line"></span><br><span class="line">pl.axis(&apos;tight&apos;)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/21/SVM支持向量机/blog\source\_posts\SVM支持向量机\下载.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow中的softmax</title>
    <link href="https://frankindf.github.io/2018/04/20/tensorflow%E4%B8%AD%E7%9A%84softmax/"/>
    <id>https://frankindf.github.io/2018/04/20/tensorflow中的softmax/</id>
    <published>2018-04-20T13:58:53.000Z</published>
    <updated>2018-04-21T15:14:04.683Z</updated>
    
    <content type="html"><![CDATA[<p>softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。</p><a id="more"></a><p>神经网络中需要将正向传播的结果和的正确结果进行进行对比，softmax函数定义如下，它将分类结果映射到[0,1]这个区间,Vi、Vj表示V中第i，j个元素，ai可以看成第i个分类结果：<br>$$<br>a_i =\frac{e^{V _i}}{\sum_je^{V_j} }<br>$$<br>交叉熵C如下，其中yi代表真实值，ai在这里为softmax：<br>$$<br>C = -\sum_i{y_i{log(a_i)}}<br>$$<br>计算两者之间的差距,每项的Loss可以用下式表示，对于<strong>只有1个正确分类i的分类</strong>,softmax交叉熵计算公式如下：<br>$$<br>L_i = -log(\frac{e^{f _{y_i}}}{\sum_je^{f_{y_j}} }  )<br>$$<br>可以看到，括号里即为softmax的值，它越大，样本的Loss就越小，即与真实分布的差距越小。</p><p>在TensorFlow中交叉熵有下面几种计算方法：</p><ul><li><p>tf.nn.softmax_cross_entropy_with_logits（label, logits）</p><p>logits的shape=(m,n)，label的shape=(m,n),，如果真实分类logits为一维数组，则需要进行one-hot编码。</p></li></ul><ul><li>tf.nn.sparse_softmax_cross_entropy_with_logits（label, logits）</li><li>logits的shape=(m,n)，label的shape=(m,1),若label的shape=(m,n)阶则需要使用argmax函数变为(m,1)</li><li>tf.nn.sigmoid_cross_entropy_with_logits：张量中标量与标量间的运算,求两点分布 之间的交叉熵。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面举例进行交叉熵计算</span></span><br><span class="line"><span class="comment">#分别用softmax_cross_entropy_with_logits、tf.nn.sparse_softmax_cross_entropy_with_logits</span></span><br><span class="line"><span class="comment">#手算和tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment">#label代表真实分布</span></span><br><span class="line"><span class="comment">#采用one-hot编码，即真实分类分别为[3,2,1,1,2]</span></span><br><span class="line">labels = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=np.float32)</span><br><span class="line"><span class="comment">#logits代表前向传播的结果</span></span><br><span class="line"><span class="comment">#代表每个值得权重</span></span><br><span class="line">logits = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>],</span><br><span class="line">                   [<span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                   [<span class="number">8</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>]], dtype=np.float32)</span><br><span class="line">num_classes = labels.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#tf.nn.softmax用来求softmax值，即映射到[0,1]区间上的概率</span></span><br><span class="line">predicts = tf.nn.softmax(logits=logits, dim=<span class="number">-1</span>)</span><br><span class="line">sess.run(predicts)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out:array([[  2.45611509e-03,   6.67641265e-03,   9.90867496e-01],</span><br><span class="line">       [  1.14195190e-01,   8.43794703e-01,   4.20100652e-02],</span><br><span class="line">       [  9.46499169e-01,   6.37746137e-03,   4.71234173e-02],</span><br><span class="line">       [  9.97193694e-01,   2.47179624e-03,   3.34521203e-04],</span><br><span class="line">       [  4.71234173e-02,   9.46499169e-01,   6.37746137e-03]], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.softmax_cross_entropy_with_logits计算交叉熵，labels可以直接用one-hot编码的数组</span></span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)</span><br><span class="line">sess.run(cross_entropy)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.sparse_softmax_cross_entropy_with_logits计算交叉熵，labels要处理为(m,1)维</span></span><br><span class="line">classes = tf.argmax(labels, axis=<span class="number">1</span>)</span><br><span class="line">sess.run(classes)</span><br><span class="line">cross_entropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=classes)</span><br><span class="line">sess.run(cross_entropy2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用reduce_sum计算，其中</span></span><br><span class="line"><span class="comment">#用clip_by_value将取值限制在1e-10以上，防止出log(0)</span></span><br><span class="line"><span class="comment">#用labels/predicts相当于前面加负号</span></span><br><span class="line">labels = tf.clip_by_value(labels, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">predicts = tf.clip_by_value(predicts, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">cross_entropy4 = tf.reduce_sum(labels * tf.log(labels/predicts), axis=<span class="number">1</span>)</span><br><span class="line">sess.run(cross_entropy4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0.8</span></span><br><span class="line">x = <span class="number">1.3</span></span><br><span class="line">cross_entropy3 = tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)</span><br><span class="line"><span class="comment"># tf.nn.sigmoid_cross_entropy_with_logits的具体实现:</span></span><br><span class="line">cross_entropy5 = - z * tf.log(tf.nn.sigmoid(x))  - (<span class="number">1</span>-z) * tf.log(<span class="number">1</span>-tf.nn.sigmoid(x))</span><br><span class="line">sess.run(cross_entropy3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.50100845</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorboard应用</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorboard%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorboard用法/</id>
    <published>2018-04-01T09:28:48.000Z</published>
    <updated>2018-04-22T10:47:26.309Z</updated>
    
    <content type="html"><![CDATA[<p>通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。</p><a id="more"></a><p>标量数据汇总和记录使用</p><p>tf.summary.scalar(tags, values, collections=None, name=None)  </p><p>直接记录变量var的直方图</p><p>tf.summary.histogram(tag, values, collections=None, name=None）  </p><p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ <em>tag</em> /image/0’, ‘ <em>tag</em> /image/1’, etc.，如：input/image/0等</p><p>tf.summary.image(tag, tensor, max_images=3, collections=None, name=None)  </p><p>汇总再进行一次合并</p><p>tf.summary.merge(inputs, collections=None, name=None)</p><p>合并默认图形中的所有汇总</p><p>tf.summaries.merge_all(key=’summaries’)  </p><p>下面看一个TensorBoard的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">通过</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#输入数据，这里用的是mnist数据集</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">#学习率</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">#训练集的训练次数</span><br><span class="line">training_epochs = 25</span><br><span class="line">#每次输入的数据个数</span><br><span class="line">batch_size = 100</span><br><span class="line">display_epoch = 1</span><br><span class="line">#数据存储位置</span><br><span class="line">logs_path = &apos;/tmp/tensorflow_logs/example/&apos;</span><br><span class="line"># mnist数据集是28*28的图片，一共784个像素</span><br><span class="line">#训练集x</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784], name=&apos;InputData&apos;)</span><br><span class="line">#真实值y，对数字分类有10个类别</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10], name=&apos;LabelData&apos;)</span><br><span class="line"></span><br><span class="line">#定义神经网络参数y=wx+b中的x和b</span><br><span class="line">W = tf.Variable(tf.zeros([784, 10]), name=&apos;Weights&apos;)</span><br><span class="line">b = tf.Variable(tf.zeros([10]), name=&apos;Bias&apos;)</span><br><span class="line"></span><br><span class="line"># 在命名空间内定义模型、损失、优化方法</span><br><span class="line">with tf.name_scope(&apos;Model&apos;):</span><br><span class="line">    # 训练模型，采用全连接神经网络</span><br><span class="line">    #matmul进行矩阵相乘</span><br><span class="line">    #softmax得到输出结果</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W) + b) </span><br><span class="line">with tf.name_scope(&apos;Loss&apos;):</span><br><span class="line">#损失函数使用交叉熵，这里也可以用TF自带的函数</span><br><span class="line">    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span><br><span class="line">with tf.name_scope(&apos;SGD&apos;):</span><br><span class="line">    #用梯度下降法对损失函数进行优化</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line">with tf.name_scope(&apos;Accuracy&apos;):</span><br><span class="line">    # 计算准确率，这里argmax的参数就对应分类</span><br><span class="line">    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"># 计算之前需要将全局变量初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"># 添加变量到TensorBoard</span><br><span class="line"># 这里添加了损失、准确率，在输出的文件中可以看到这些参数的变化情况</span><br><span class="line">tf.summary.scalar(&quot;loss&quot;, cost)</span><br><span class="line">tf.summary.scalar(&quot;accuracy&quot;, acc)</span><br><span class="line"># Merge all summaries into a single op</span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">    # 初始化数据</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # 初始化TB，路径为logs_path</span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    # 开始训练过程</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        #所有数据训练一次要分成total_batch个数据集</span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # 训练数据batch_xs,batch_ys</span><br><span class="line">            _, c, summary = sess.run([optimizer, cost, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            # 将数据写入之前初始化的TB中</span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            #计算训练一次的平均损失函数</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # 显示每个Epoch的cost</span><br><span class="line">        if (epoch+1) % display_epoch == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &apos;%04d&apos; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # 测试集上显示准确率</span><br><span class="line">    print(&quot;Accuracy:&quot;, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    print(&quot;Run the command line:\n&quot; \</span><br><span class="line">          &quot;--&gt; tensorboard --logdir=/tmp/tensorflow_logs &quot; \</span><br><span class="line">          &quot;\nThen open http://0.0.0.0:6006/ into your web browser&quot;)</span><br></pre></td></tr></table></figure><p>运行完程序后，输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[out]：Epoch: 0001 cost= 1.183585649</span><br><span class="line">Epoch: 0002 cost= 0.665355021</span><br><span class="line">Epoch: 0003 cost= 0.552772734</span><br><span class="line">Epoch: 0004 cost= 0.498669290</span><br><span class="line">Epoch: 0005 cost= 0.465467163</span><br><span class="line">Epoch: 0006 cost= 0.442601420</span><br><span class="line">Epoch: 0007 cost= 0.425460528</span><br><span class="line">Epoch: 0008 cost= 0.412206892</span><br><span class="line">Epoch: 0009 cost= 0.401397175</span><br><span class="line">Epoch: 0010 cost= 0.392420013</span><br><span class="line">Epoch: 0011 cost= 0.384768393</span><br><span class="line">Epoch: 0012 cost= 0.378172010</span><br><span class="line">Epoch: 0013 cost= 0.372432202</span><br><span class="line">Epoch: 0014 cost= 0.367334918</span><br><span class="line">Epoch: 0015 cost= 0.362694857</span><br><span class="line">Epoch: 0016 cost= 0.358602089</span><br><span class="line">Epoch: 0017 cost= 0.354879144</span><br><span class="line">Epoch: 0018 cost= 0.351492124</span><br><span class="line">Epoch: 0019 cost= 0.348284597</span><br><span class="line">Epoch: 0020 cost= 0.345425291</span><br><span class="line">Epoch: 0021 cost= 0.342768804</span><br><span class="line">Epoch: 0022 cost= 0.340257174</span><br><span class="line">Epoch: 0023 cost= 0.337940815</span><br><span class="line">Epoch: 0024 cost= 0.335757064</span><br><span class="line">Epoch: 0025 cost= 0.333699012</span><br><span class="line">Optimization Finished!</span><br><span class="line">Accuracy: 0.9135</span><br></pre></td></tr></table></figure><p>运行tensorboard –logdir=/tmp/tensorflow_logs，打开浏览器进入<a href="http://localhost:6006查看结果。" target="_blank" rel="noopener">http://localhost:6006查看结果。</a></p><figure class="half"><br>    <img src="/2018/04/01/tensorboard用法/blog\source\_posts\tensorboard用法\1.png" alt="1"><br>    <img src="/2018/04/01/tensorboard用法/blog\source\_posts\tensorboard用法\2.png" alt="2"><br></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow卷积函数</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorflow%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorflow卷积过程/</id>
    <published>2018-04-01T08:36:34.000Z</published>
    <updated>2018-04-22T10:47:18.279Z</updated>
    
    <content type="html"><![CDATA[<p>卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。</p><a id="more"></a><p>tensorflow卷积函数：</p><p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</p><p>input为输入的数组，假设维度为[batch,矩阵高x,矩阵宽y,矩阵深度z]</p><p>filterw为卷积核，维度[矩阵高x,矩阵宽y,矩阵深度z, out_channels]</p><p>strides：卷积时在图像每一维的步长</p><p>padding：是否补全空白”SAME”或”VALID”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#定义全1的矩阵，维数[3,3,1]</span><br><span class="line">x1 = tf.constant(1.0, shape=[1,6,6,2])  </span><br><span class="line">#定义核函数，维度[1,1,1]</span><br><span class="line">#第一通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#第二通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">kernel = tf.constant(1.0, shape=[3,2,2,1])   </span><br><span class="line">y2 = tf.nn.conv2d(x1, kernel,strides=[1,1,1,1],padding=&apos;VALID&apos;)  </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run(session=sess)</span><br><span class="line">    #显示y2</span><br><span class="line">    print(sess.run(tf.shape(y2))</span><br><span class="line">#行6-3+1，列6-2+1</span><br><span class="line">OUT:[1 4 5 1]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>adaboost算法</title>
    <link href="https://frankindf.github.io/2018/03/29/adaboost%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/29/adaboost算法/</id>
    <published>2018-03-29T14:28:37.000Z</published>
    <updated>2018-04-21T16:17:56.985Z</updated>
    
    <content type="html"><![CDATA[<p>​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。</p><a id="more"></a><p>该算法首先定义辅助函数stumpClassify，输入数据，分界值后输出分类的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def stumpClassify(dataIn,dimen,threshVal,threshIneq):</span><br><span class="line">    #输入数据、采用哪列特征分类、分界的限值，是大于还是小于</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    retArray=np.ones((m,1))</span><br><span class="line">    if threshIneq==&apos;lt&apos;:</span><br><span class="line">    #该分类是小于，则大于分界值得数据预测错误，定义为-1</span><br><span class="line">        retArray[dataIn[:,dimen]&lt;=threshVal]=-1.0</span><br><span class="line">    else:</span><br><span class="line">    #该分类是大于，则小于分界值得数据预测错误</span><br><span class="line">        retArray[dataIn[:,dimen]&gt;threshVal]=-1.0</span><br><span class="line">    return retArray</span><br></pre></td></tr></table></figure><p>函数使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#定义输入数据</span><br><span class="line">dataIn=np.arange((12)).reshape(6,2)</span><br><span class="line">Out[43]: </span><br><span class="line">array([[ 0,  1],</span><br><span class="line">       [ 2,  3],</span><br><span class="line">       [ 4,  5],</span><br><span class="line">       [ 6,  7],</span><br><span class="line">       [ 8,  9],</span><br><span class="line">       [10, 11]])</span><br><span class="line">#以小于2的值作为第2列的分界点</span><br><span class="line">stumpClassify(dataIn,1,2,&apos;lt&apos;)</span><br><span class="line">Out[47]: </span><br><span class="line">array([[-1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.]])</span><br></pre></td></tr></table></figure><p>​          有了分类函数，接着需要定义buildStump选取当前情况下，最佳的分类点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">def buildStump(dataIn,classLabel,D):</span><br><span class="line">#D为输入的分类权重，后面会用到</span><br><span class="line">    #输入数据，正确分类，迭代系数</span><br><span class="line">    n=dataIn.shape[1]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    numStep=10.0</span><br><span class="line">    bestStump=&#123;&#125;</span><br><span class="line">    bestClass=np.zeros((m,1))</span><br><span class="line">    minError=np.inf</span><br><span class="line">    for i in range(0,n):</span><br><span class="line">        iAxis=dataIn[:,i]</span><br><span class="line">        minAxis=min(iAxis)</span><br><span class="line">        maxAxis=max(iAxis)</span><br><span class="line">        stepSize=(maxAxis-minAxis)/numStep</span><br><span class="line">    #考虑大于和小于两种分类情况</span><br><span class="line">        for ineq in [&apos;lt&apos;,&apos;gt&apos;]:</span><br><span class="line">            for j in range(-1,int(numStep)+1):</span><br><span class="line">            #每次计算错误值</span><br><span class="line">                threshVal=minAxis+stepSize*float(j)</span><br><span class="line">                #从threshVal值处对i特征值对应数据进行分类</span><br><span class="line">                predictCategory=stumpClassify(dataIn,i,threshVal,ineq)</span><br><span class="line">                #初始化误差值矩阵</span><br><span class="line">                errArr=np.ones((m,1))</span><br><span class="line">                #预测正确的点误差矩阵值为0，其他点为1</span><br><span class="line">                errArr[predictCategory==classLabel]=0</span><br><span class="line">                #误差矩阵乘以权重得到该threshVal的分类总误差，</span><br><span class="line">                errSum=np.dot(D.T,errArr)</span><br><span class="line">                #更新误差最小的threshVal</span><br><span class="line">                if errSum&lt;minError:</span><br><span class="line">                    minError=errSum</span><br><span class="line">                    bestClass=predictCategory.copy()</span><br><span class="line">                    bestStump[&apos;dim&apos;]=i</span><br><span class="line">                    bestStump[&apos;threshVal&apos;]=threshVal</span><br><span class="line">                    bestStump[&apos;ineq&apos;]=ineq</span><br><span class="line">    print(&apos;j&apos;,j,&apos;ineq&apos;,ineq,&apos;split&apos;,i,&apos;\nthresh&apos;,threshVal,&apos;\nerrorsum&apos;,errSum)</span><br><span class="line">    return bestStump,minError,bestClass</span><br><span class="line">    </span><br><span class="line">   classLabel=np.array(([[1],[-1],[-1],[1],[1],[1]]))    </span><br><span class="line">   buildStump(dataIn,classLabel,D)</span><br><span class="line">Out[56]: </span><br><span class="line">#得到当前权重下最佳分类</span><br><span class="line">#分类的特征为0列对应的特征，分类值为4，错误率0.167，分类结果[-1,-1,-1,1,1,1]</span><br><span class="line">(&#123;&apos;dim&apos;: 0, &apos;ineq&apos;: &apos;lt&apos;, &apos;threshVal&apos;: 4.0&#125;,</span><br><span class="line"> array([[ 0.16666667]]),</span><br><span class="line"> array([[-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.]]))</span><br></pre></td></tr></table></figure><p>​        得到了一列一种情况下的分类方法，就相当于有了一个分类器，adaboost的核心是将多个分类器的结果按照权重相加，得到最后的结果。下面通过addBoostTrainDS来训练得到各个分类器的权重。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def addBoostTrainDS(dataIn,classLabels,numIt=40):</span><br><span class="line">    weakClassArr=[]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">#初始化D为1/m，m为数据个数</span><br><span class="line">    D=np.ones((m,1))/m</span><br><span class="line">    aggBestClass=np.zeros((m,1))</span><br><span class="line">    for i in range(numIt):</span><br><span class="line">     #得到每次更新权重值后的最佳分类</span><br><span class="line">        bestStump,error,bestClass=buildStump(dataIn,classLabel,D)</span><br><span class="line">        #求出误差率alpha</span><br><span class="line">        alpha=0.5*np.log((1-error)/max(error,1e-12))</span><br><span class="line">        bestStump[&apos;alpha&apos;]=alpha</span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line">        #每一项的权重</span><br><span class="line">        expon=-1*alpha*(classLabels.T*bestClass)</span><br><span class="line">        print(&apos;expon&apos;,expon)</span><br><span class="line">        D=D*np.exp(expon)</span><br><span class="line">        D=D/D.sum()#更新D值</span><br><span class="line">        aggBestClass+=alpha*bestClass</span><br><span class="line">        #类别为1和-1</span><br><span class="line">        aggErrors=np.ones((m,1))</span><br><span class="line">        samePred=np.where(np.sign(aggBestClass)==classLabel)</span><br><span class="line">        difPred=np.where(np.sign(aggBestClass)!=classLabel)</span><br><span class="line">        aggErrors[samePred]=0</span><br><span class="line">        aggErrors[difPred]=1</span><br><span class="line">        print(&apos;D&apos;,D,&apos;best&apos;,bestClass,&apos;\naggBestClass&apos;,aggBestClass  </span><br><span class="line">        errorRate=aggErrors.sum()/m</span><br><span class="line">        print(&apos;totalError&apos;,errorRate)</span><br><span class="line">        if errorRate==0: break</span><br><span class="line">    return weakClassArr,aggErrors</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://frankindf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>numpy的argsort函数用法</title>
    <link href="https://frankindf.github.io/2018/03/28/numpy%E7%9A%84argsort%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/28/numpy的argsort函数用法/</id>
    <published>2018-03-28T14:17:28.000Z</published>
    <updated>2018-04-21T15:12:39.022Z</updated>
    
    <content type="html"><![CDATA[<p>数组操作时有时如果需要获取数组的索引可以使用argsort函数。</p><a id="more"></a><p>numpy中的argsort可以返回一个索引，具体参数如下</p><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)</p><p>a数组</p><p>axis行或者列，默认为-1，即最后一个维度，0为列，1为行</p><p>kind排序方式{‘quicksort’, ‘mergesort’, ‘heapsort’}</p><p>order</p><p>返回的是排序后的数组在原数组中索引的数组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.array(([[1,6,3,4,2]]))</span><br><span class="line">a.argsort()</span><br><span class="line">#返回的是ARRAY从小到大的索引</span><br><span class="line">Out[35]: array([[0, 4, 2, 3, 1]], dtype=int64)</span><br><span class="line">#默认为行排序的索引</span><br><span class="line">np.argsort(a)</span><br><span class="line">Out[41]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br><span class="line">#axis为1按列排序</span><br><span class="line">np.argsort(a,axis=0)</span><br><span class="line">Out[39]: </span><br><span class="line">array([[0, 1, 0, 0],</span><br><span class="line">       [1, 0, 1, 1]], dtype=int64)</span><br><span class="line">#axis为1，按行排序</span><br><span class="line">np.argsort(a,axis=1)</span><br><span class="line">Out[40]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数组操作时有时如果需要获取数组的索引可以使用argsort函数。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
