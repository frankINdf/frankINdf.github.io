<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阳阳的博客</title>
  
  <subtitle>学无止境</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://frankindf.github.io/"/>
  <updated>2018-05-18T15:24:21.517Z</updated>
  <id>https://frankindf.github.io/</id>
  
  <author>
    <name>阳阳</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PCA主成分析</title>
    <link href="https://frankindf.github.io/2018/05/18/PCA%E4%B8%BB%E6%88%90%E5%88%86%E6%9E%90/"/>
    <id>https://frankindf.github.io/2018/05/18/PCA主成分析/</id>
    <published>2018-05-18T13:55:07.000Z</published>
    <updated>2018-05-18T15:24:21.517Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是对矩阵运算和PCA主成分析的学习。</p><h4 id="矩阵乘法意义"><a href="#矩阵乘法意义" class="headerlink" title="矩阵乘法意义"></a>矩阵乘法意义</h4><p>矩阵是线性代数的基础，矩阵相乘运算大家都知道，但这到底代表了什么呢？</p><p>关于$AB^T$:矩阵$A$、$B$相乘为A的行向量和每个列向量相乘，第一行$[a1<em>b1,a1</em>b2,…]$其实就是$a1$在$b_1,b_2….$这些向量上的投影$|a_i||b_i|cos\theta$。</p><p>关于矩阵乘法：矩阵相乘就是把原来每个维度的坐标进行变换$Ax$将原来空间上的坐标旋转，这里说是旋转是因为向量长度不变，只是分量变了.</p><h4 id="矩阵特征值和特征向量"><a href="#矩阵特征值和特征向量" class="headerlink" title="矩阵特征值和特征向量"></a>矩阵特征值和特征向量</h4><p>将矩阵的行向量和特征向量相乘$a_ix_i$得到的是在特征向量$x_i$方向的投影,矩阵$Ax_i$得到的就是矩阵中每行在特征向量方向投影的向量，只有矩阵中的向量与特征向量垂直或平行时，才能满足$Ax_i=\lambda x_i$。这样就很好理解矩阵特征值和特征向量以及特征向量之间的关系。</p><h4 id="PCA主成分析"><a href="#PCA主成分析" class="headerlink" title="PCA主成分析"></a>PCA主成分析</h4><p>PCA的思想就是提取特征值较大的特征向量来表示原数据，变换过程可以参考下图</p><p><img src="/2018/05/18/PCA主成分析/pca.jpg" alt="c"></p><p>首先为什么要提取特征值较大的特征向量，因为$Ax = \lambda x$,特征值$\lambda$较大代表A中列向量在x方向的投影较大，即数据分布范围较广，在这个方向投影可以保证数据最多信息，即投影到该方向上后数据的方差最大。</p><p>其次怎么将原数据投影呢？根据之前矩阵乘法的意义，其实简单的将数据乘以特征向量就可以得到投影值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要是对矩阵运算和PCA主成分析的学习。&lt;/p&gt;
&lt;h4 id=&quot;矩阵乘法意义&quot;&gt;&lt;a href=&quot;#矩阵乘法意义&quot; class=&quot;headerlink&quot; title=&quot;矩阵乘法意义&quot;&gt;&lt;/a&gt;矩阵乘法意义&lt;/h4&gt;&lt;p&gt;矩阵是线性代数的基础，矩阵相乘运算大家都知道，但
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>鲸的名字</title>
    <link href="https://frankindf.github.io/2018/05/12/%E9%B2%B8%E7%9A%84%E5%90%8D%E5%AD%97/"/>
    <id>https://frankindf.github.io/2018/05/12/鲸的名字/</id>
    <published>2018-05-12T15:21:18.000Z</published>
    <updated>2018-05-13T15:25:05.484Z</updated>
    
    <content type="html"><![CDATA[<p>本文是Kraggle竞赛Humpback Whale ID的分析过程。</p><a id="more"></a><h3 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h3><h4 id="数据概览"><a href="#数据概览" class="headerlink" title="数据概览"></a>数据概览</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">INPUT_DIR = <span class="string">'D:/tens/kraggle/whale/train/'</span></span><br><span class="line"></span><br><span class="line">PATH = <span class="string">r"D:\tens\kraggle\whale\train.csv"</span></span><br><span class="line">train_df = pd.read_csv(PATH)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df.describe()</span><br></pre></td></tr></table></figure><table><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>Image</th><br>      <th>Id</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>count</th><br>      <td>9850</td><br>      <td>9850</td><br>    </tr><br>    <tr><br>      <th>unique</th><br>      <td>9850</td><br>      <td>4251</td><br>    </tr><br>    <tr><br>      <th>top</th><br>      <td>72046206.jpg</td><br>      <td>new_whale</td><br>    </tr><br>    <tr><br>      <th>freq</th><br>      <td>1</td><br>      <td>810</td><br>    </tr><br>  </tbody><br></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure><table><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>Image</th><br>      <th>Id</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>00022e1a.jpg</td><br>      <td>w_e15442c</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>000466c4.jpg</td><br>      <td>w_1287fbc</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>00087b01.jpg</td><br>      <td>w_da2efe0</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>001296d5.jpg</td><br>      <td>w_19e5482</td><br>   </tr><br>    <tr><br>      <th>4</th><br>      <td>0014cfdf.jpg</td><br>      <td>w_f22f3e3</td><br>    </tr><br>  </tbody><br></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">img = plt.imread(INPUT_DIR+train_df.Image[<span class="number">0</span>])</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_3_0.png" alt="1"></p><h4 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_categories = len(train_df.Id.unique())</span><br><span class="line">num_img = len(train_df.Id)</span><br></pre></td></tr></table></figure><p>类别分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">size_buckets = Counter(train_df.Id.value_counts())</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.bar(range(len(size_buckets)), list(size_buckets.values())[::<span class="number">-1</span>], align=<span class="string">'center'</span>)</span><br><span class="line">plt.xticks(range(len(size_buckets)), list(size_buckets.keys())[::<span class="number">-1</span>])</span><br><span class="line">plt.title(<span class="string">"Num of categories by images in the training set"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_5_1.png" alt="utput_5_"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df[<span class="string">'Id'</span>].value_counts().head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_whale    810</span><br><span class="line">w_1287fbc     34</span><br><span class="line">w_98baff9     27</span><br><span class="line">Name: Id, dtype: int64</span><br></pre></td></tr></table></figure><p>同一只鲸鱼的所有图片（w_98baff9 为例）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w_98baff9 = train_df[train_df[<span class="string">'Id'</span>] == <span class="string">'w_98baff9'</span>]</span><br><span class="line">plot_images_for_filenames(list(w_98baff9[<span class="string">'Image'</span>]), <span class="keyword">None</span>, rows=<span class="number">9</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_7_0.png" alt="utput_11_"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">one_image_ids = train_df[<span class="string">'Id'</span>].value_counts().tail(<span class="number">8</span>).keys()</span><br><span class="line">one_image_filenames = []</span><br><span class="line">labels = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> one_image_ids:</span><br><span class="line">    one_image_filenames.extend(list(train_df[train_df[<span class="string">'Id'</span>] == i][<span class="string">'Image'</span>]))</span><br><span class="line">    labels.append(i)</span><br><span class="line">    </span><br><span class="line">plot_images_for_filenames(one_image_filenames, labels, rows=<span class="number">3</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_8_0.png" alt="utput_8_"></p><p>图像尺寸分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">img_sizes = Counter([Image.open(<span class="string">f'<span class="subst">&#123;INPUT_DIR&#125;</span><span class="subst">&#123;i&#125;</span>'</span>).size <span class="keyword">for</span> i <span class="keyword">in</span> train_df[<span class="string">'Image'</span>]])</span><br><span class="line"></span><br><span class="line">size, freq = zip(*Counter(&#123;i: v <span class="keyword">for</span> i, v <span class="keyword">in</span> img_sizes.items() <span class="keyword">if</span> v &gt; <span class="number">1</span>&#125;).most_common(<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.bar(range(len(freq)), list(freq), align=<span class="string">'center'</span>)</span><br><span class="line">plt.xticks(range(len(size)), list(size), rotation=<span class="number">70</span>)</span><br><span class="line">plt.title(<span class="string">"Image size frequencies (where freq &gt; 1)"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_9_0.png" alt="utput_11_"></p><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> (</span><br><span class="line">    random_rotation, random_shift, random_shear, random_zoom,</span><br><span class="line">    random_channel_shift, transform_matrix_offset_center, img_to_array)</span><br><span class="line">img = Image.open(<span class="string">f'<span class="subst">&#123;INPUT_DIR&#125;</span>ff38054f.jpg'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#图像旋转</span></span><br><span class="line">img_arr = img_to_array(img)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">imgs = [</span><br><span class="line">    random_rotation(img_arr, <span class="number">30</span>, row_axis=<span class="number">0</span>, col_axis=<span class="number">1</span>, channel_axis=<span class="number">2</span>, fill_mode=<span class="string">'nearest'</span>) * <span class="number">255</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line">img_max = np.array(imgs).max()</span><br><span class="line">imgs = imgs/img_max</span><br><span class="line">plot_images(imgs, <span class="keyword">None</span>, rows=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_11_0.png" alt="utput_11_"></p><p><img src="/2018/05/12/鲸的名字/output_11_1.png" alt="utput_11_"></p><p>图像平移</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">imgs = [</span><br><span class="line">    random_shift(img_arr, wrg=<span class="number">0.1</span>, hrg=<span class="number">0.3</span>, row_axis=<span class="number">0</span>, col_axis=<span class="number">1</span>, channel_axis=<span class="number">2</span>, fill_mode=<span class="string">'nearest'</span>) * <span class="number">255</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line">img_max = np.array(imgs).max()</span><br><span class="line">imgs = imgs/img_max</span><br><span class="line">plot_images(imgs, <span class="keyword">None</span>, rows=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_12_0.png" alt="utput_11_"></p><p>图像锐度改变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">imgs = [</span><br><span class="line">    random_shear(img_arr, intensity=<span class="number">0.4</span>, row_axis=<span class="number">0</span>, col_axis=<span class="number">1</span>, channel_axis=<span class="number">2</span>, fill_mode=<span class="string">'nearest'</span>) * <span class="number">255</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line">img_max = np.array(imgs).max()</span><br><span class="line">imgs = imgs/img_max</span><br><span class="line">plot_images(imgs, <span class="keyword">None</span>, rows=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_13_0.png" alt="utput_11_"></p><p>图像拉伸和压缩</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">imgs = [</span><br><span class="line">    random_zoom(img_arr, zoom_range=(<span class="number">1.5</span>, <span class="number">0.7</span>), row_axis=<span class="number">0</span>, col_axis=<span class="number">1</span>, channel_axis=<span class="number">2</span>, fill_mode=<span class="string">'nearest'</span>) * <span class="number">255</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line">img_max = np.array(imgs).max()</span><br><span class="line">imgs = imgs/img_max</span><br><span class="line">plot_images(imgs, <span class="keyword">None</span>, rows=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_14_0.png" alt="utput_11_"></p><p>图像灰度改变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_greyscale</span><span class="params">(img, p)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; p:</span><br><span class="line">        <span class="keyword">return</span> np.dot(img[...,:<span class="number">3</span>], [<span class="number">0.299</span>, <span class="number">0.587</span>, <span class="number">0.114</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">imgs = [</span><br><span class="line">    random_greyscale(img_arr, <span class="number">0.5</span>) * <span class="number">255</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line">plot_images(imgs, <span class="keyword">None</span>, rows=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/05/12/鲸的名字/output_15_0.png" alt="utput_11_"></p><h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><h4 id="方法简介"><a href="#方法简介" class="headerlink" title="方法简介"></a>方法简介</h4><p>通过之前对图集进行探索，可以发现有下面明显的特征：</p><ol><li>种类多，一共有9850张图片，有4251类</li><li>训练集分布不均匀，最多的一个ID有2000余张照片，很多类只有几张照片</li></ol><p>针对这种种类相对样本数量数量较多，训练样本较少，采用Triplet Loss可以有效对各个类进行分离。Triplet Loss选取样本后再选取一个同类样本$x_2$和一个异类样本$x’_2$，学习的目标是样本和同类样本之间的距离最小，和异类距离最大,即$max(dis(x_1,x_2)),min(dis(x_1,x’_2))$。</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>1.加载Resnet 50预训练的权重</p><p>2.提取权重后增加池化层，droupout层，全连接层</p><p>3.定义Triplet Loss损失函数，加载全部训练集数据，最小化损失，得到全连接层的权重</p><p>4.加载训练集和第3步得到的权重，得到全连接层的输出，维数为（50,1）</p><p>5.加载测试集和第3步得到的权重，得到全连接层的输出维数为（50,1）</p><p>6.通过第5步的输出训练得到KNN模型</p><p>7.通过KNN模型预测第5步得到的数据的分类</p><p>具体代码见<a href="https://github.com/frankINdf/MachineLearning-DEMO/tree/master/kraggle-%E9%B2%B8%E9%B1%BCID%E9%A2%84%E6%B5%8B" target="_blank" rel="noopener">github</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是Kraggle竞赛Humpback Whale ID的分析过程。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow保存模型</title>
    <link href="https://frankindf.github.io/2018/05/08/tensorflow%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    <id>https://frankindf.github.io/2018/05/08/tensorflow保存模型/</id>
    <published>2018-05-08T14:27:50.000Z</published>
    <updated>2018-05-13T15:10:41.630Z</updated>
    
    <content type="html"><![CDATA[<p>tensorflow模型可以保存为meta文件或者pb文件本文主要是这两种方式的实践。</p><a id="more"></a><p>保存为meta后，文件夹会出现4个文件：</p><p><code>checkpoint</code>文件保存目录下所有的模型文件列表</p><p><code>model.ckpt.meta</code>文件保存了TensorFlow计算图的结构</p><p><code>model.ckpt</code>文件保存了TensorFlow程序中每一个变量的取值</p><p><strong>保存为meta文件</strong></p><p>模型保存：</p><p>使用<code>tf.train.Saver()</code></p><p>在构建图时定义<code>saver = tf.train.Saver()</code></p><p>运行后使用<code>saver.save(sess ,&#39;d:/MMNIST/model2.ckpt&#39;)</code>将模型保存，此时文件夹中会出现上述的文件。</p><p>模型加载</p><p>tensorflow可以直接加载持久化模型</p><p>使用<code>saver = tf.train.import_meta_graph(&#39;D:/MMNIST/model2.ckpt.meta&#39;)</code>导入持久化模型，此时导入的是<code>meta</code>文件</p><p>使用<code>saver.restore(sess, tf.train.latest_checkpoint(&#39;D:/MMNIST/&#39;))</code>加载模型</p><p><code>reader = tf.train.NewCheckpointReader(&#39;D:/MMNIST/model2.ckpt&#39;)</code>可以读取ckpt文件中的变量</p><p>使用<code>get_variable_to_shape_map()</code>获得计算图中的所有变量，可以通过它打印出变量名</p><p><strong>注意</strong>：</p><p>保存模型的路径不能含有中文</p><p>保存完成之后如果再次保存，需要将旧模型删除</p><p><strong>保存为pb文件</strong></p><p>PB 文件是表示 MetaGraph 的 protocol buffer格式的文件，MetaGraph 包括计算图，数据流，以及相关的变量和输入输出signature以及 asserts 指创建计算图时额外的文件。</p><p>使用<code>tf.SavedModelBuilder</code>类来完成这个工作，并且可以把多个计算图保存到一个 PB 文件中，如果有多个MetaGraph，只会保留第一个 MetaGraph 的版本号，并且必须为每个MetaGraph 指定特殊的名称 tag 用以区分，通常这个名称 tag 以该计算图的功能和使用到的设备命名，比如 serving or training， CPU or GPU。</p><p>模型保存：</p><p>使用<code>tensorflow.python.framework.convert_variables_to_constants(sess,sess.graph_def,[&#39;op_to_store&#39;])</code>将模型持久化</p><p>使用<code>tf.gfile.FastGFile(path,mode)</code>创建pb文件</p><p>使用f.write(contant_graph.SerializeToString())将文件写入</p><p>可以看到制定文件夹中会增加**.pb文件</p><p>模型读取：</p><p><code>with gfile.FastGFile(&#39;D:/MMNIST/modelpb.pb&#39;, &#39;rb&#39;) as f:</code>打开pb文件</p><p><code>graph_def.ParseFromString(f.read())</code>读取文件f中的内容</p><p><code>tf.import_graph_def(graph_def)</code>导入计算图</p><p>导入计算图后进行变量初始化<code>sess.run(tf.global_variables_initializer())</code></p><p>此时模型已经导入到当前计算图，可以读取保存的计算图中的内容或者传入placeholder计算结果</p><p>保存为meta文件代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">import os</span><br><span class="line"># Import MNIST data</span><br><span class="line">from tensorflow.python.framework import graph_util</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">logs_path = os.getcwd()</span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = 0.1</span><br><span class="line">num_steps = 10</span><br><span class="line">batch_size = 128</span><br><span class="line">display_step = 20</span><br><span class="line"></span><br><span class="line"># Network Parameters</span><br><span class="line">n_hidden_1 = 256 # 1st layer number of neurons</span><br><span class="line">n_hidden_2 = 256 # 2nd layer number of neurons</span><br><span class="line">num_input = 784 # MNIST data input (img shape: 28*28)</span><br><span class="line">num_classes = 10 # MNIST total classes (0-9 digits)</span><br><span class="line"></span><br><span class="line"># tf Graph input</span><br><span class="line">X = tf.placeholder(&quot;float&quot;, [None, num_input])</span><br><span class="line">Y = tf.placeholder(&quot;float&quot;, [None, num_classes])</span><br><span class="line"></span><br><span class="line"># Store layers weight &amp; bias</span><br><span class="line">weights = &#123;</span><br><span class="line">    &apos;h1&apos;: tf.Variable(tf.random_normal([num_input, n_hidden_1]),name = &apos;h1_w&apos;),</span><br><span class="line">    &apos;h2&apos;: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name = &apos;h2_w&apos;),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name = &apos;out_w&apos;)</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    &apos;b1&apos;: tf.Variable(tf.random_normal([n_hidden_1]),name = &apos;b1&apos;),</span><br><span class="line">    &apos;b2&apos;: tf.Variable(tf.random_normal([n_hidden_2]),name = &apos;b2&apos;),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([num_classes]),name = &apos;out&apos;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Create model</span><br><span class="line">def neural_net(x):</span><br><span class="line">    # Hidden fully connected layer with 256 neurons</span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[&apos;h1&apos;]), biases[&apos;b1&apos;])</span><br><span class="line">    # Hidden fully connected layer with 256 neurons</span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[&apos;h2&apos;]), biases[&apos;b2&apos;])</span><br><span class="line">    # Output fully connected layer with a neuron for each class</span><br><span class="line">    out_layer = tf.matmul(layer_2, weights[&apos;out&apos;]) + biases[&apos;out&apos;]</span><br><span class="line">    return out_layer</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">logits = neural_net(X)</span><br><span class="line">prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line"># Define loss and optimizer</span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=logits, labels=Y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">train_op = optimizer.minimize(loss_op)</span><br><span class="line"></span><br><span class="line"># Evaluate model</span><br><span class="line">correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"># Initialize the variables (i.e. assign their default value)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># Start training</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line">    # Run the initializer</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    constant_graph = graph_util.convert_variables_to_constants(sess,sess.graph_def)    </span><br><span class="line">    for step in range(1, num_steps+1):</span><br><span class="line">        batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">        # Run optimization op (backprop)</span><br><span class="line">        _,c=sess.run([train_op,loss_op], feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">        if step % display_step == 0 or step == 1:</span><br><span class="line">            # Calculate batch loss and accuracy</span><br><span class="line">            loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X: batch_x,</span><br><span class="line">                                                                 Y: batch_y&#125;)</span><br><span class="line">            print(&quot;Step &quot; + str(step) + &quot;, Minibatch Loss= &quot; + \</span><br><span class="line">                  &quot;&#123;:.4f&#125;&quot;.format(loss) + &quot;, Training Accuracy= &quot; + \</span><br><span class="line">                  &quot;&#123;:.3f&#125;&quot;.format(acc))</span><br><span class="line">    saver.save(sess,&apos;D:/MMNIST/model2.ckpt.meta&apos;)</span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br></pre></td></tr></table></figure><p>读取meta文件的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import tensorflow  as tf</span><br><span class="line">from tensorflow.python.platform import gfile</span><br><span class="line">from tensorflow.python import pywrap_tensorflow</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">meta_file_path = os.getcwd()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">saver = tf.train.import_meta_graph(&apos;D:/MMNIST/model2.ckpt.meta&apos;)</span><br><span class="line">saver.restore(sess, tf.train.latest_checkpoint(&apos;D:/MMNIST/&apos;))</span><br><span class="line">reader = tf.train.NewCheckpointReader(&apos;D:/MMNIST/model2.ckpt&apos;)  </span><br><span class="line">  </span><br><span class="line">variables = reader.get_variable_to_shape_map()  </span><br><span class="line">  </span><br><span class="line">for ele in variables:  </span><br><span class="line">    print(ele)</span><br></pre></td></tr></table></figure><p>保存pb文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">from __future__ import print_function</span><br><span class="line">import os</span><br><span class="line"># Import MNIST data</span><br><span class="line">from tensorflow.python.framework import graph_util</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">logs_path = os.getcwd()</span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = 0.1</span><br><span class="line">num_steps = 10</span><br><span class="line">batch_size = 128</span><br><span class="line">display_step = 20</span><br><span class="line"></span><br><span class="line"># Network Parameters</span><br><span class="line">n_hidden_1 = 256 # 1st layer number of neurons</span><br><span class="line">n_hidden_2 = 256 # 2nd layer number of neurons</span><br><span class="line">num_input = 784 # MNIST data input (img shape: 28*28)</span><br><span class="line">num_classes = 10 # MNIST total classes (0-9 digits)</span><br><span class="line"></span><br><span class="line"># tf Graph input</span><br><span class="line">X = tf.placeholder(&quot;float&quot;, [None, num_input],name = &apos;x&apos;)</span><br><span class="line">Y = tf.placeholder(&quot;float&quot;, [None, num_classes], name = &apos;y&apos;)</span><br><span class="line"></span><br><span class="line"># Store layers weight &amp; bias</span><br><span class="line">weights = &#123;</span><br><span class="line">    &apos;h1&apos;: tf.Variable(tf.random_normal([num_input, n_hidden_1]),name = &apos;h1_w&apos;),</span><br><span class="line">    &apos;h2&apos;: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name = &apos;h2_w&apos;),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name = &apos;out_w&apos;)</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    &apos;b1&apos;: tf.Variable(tf.random_normal([n_hidden_1]),name = &apos;b1&apos;),</span><br><span class="line">    &apos;b2&apos;: tf.Variable(tf.random_normal([n_hidden_2]),name = &apos;b2&apos;),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([num_classes]),name = &apos;out&apos;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Create model</span><br><span class="line">def neural_net(x):</span><br><span class="line">    # Hidden fully connected layer with 256 neurons</span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[&apos;h1&apos;]), biases[&apos;b1&apos;])</span><br><span class="line">    # Hidden fully connected layer with 256 neurons</span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[&apos;h2&apos;]), biases[&apos;b2&apos;])</span><br><span class="line">    # Output fully connected layer with a neuron for each class</span><br><span class="line">    out_layer = tf.matmul(layer_2, weights[&apos;out&apos;]) + biases[&apos;out&apos;]</span><br><span class="line">    return out_layer</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">logits = neural_net(X)</span><br><span class="line">prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line"># Define loss and optimizer</span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=logits, labels=Y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">train_op = optimizer.minimize(loss_op)</span><br><span class="line"></span><br><span class="line"># Evaluate model</span><br><span class="line">correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32),name = &apos;op_to_store&apos;)</span><br><span class="line"></span><br><span class="line"># Initialize the variables (i.e. assign their default value)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Start training</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    </span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    constant_graph = graph_util.convert_variables_to_constants(sess,sess.graph_def,[&apos;op_to_store&apos;])    </span><br><span class="line">    for step in range(1, num_steps+1):</span><br><span class="line">        batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">        # Run optimization op (backprop)</span><br><span class="line">        _,c=sess.run([train_op,loss_op], feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">        if step % display_step == 0 or step == 1:</span><br><span class="line">            # Calculate batch loss and accuracy</span><br><span class="line">            loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X: batch_x,</span><br><span class="line">                                                                 Y: batch_y&#125;)</span><br><span class="line">            print(&quot;Step &quot; + str(step) + &quot;, Minibatch Loss= &quot; + \</span><br><span class="line">                  &quot;&#123;:.4f&#125;&quot;.format(loss) + &quot;, Training Accuracy= &quot; + \</span><br><span class="line">                  &quot;&#123;:.3f&#125;&quot;.format(acc))</span><br><span class="line">    with tf.gfile.FastGFile(&apos;D:/MMNIST/modelpb.pb&apos;,mode = &apos;wb&apos;) as f:</span><br><span class="line">        f.write(constant_graph.SerializeToString())</span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # Calculate accuracy for MNIST test images</span><br><span class="line">    print(&quot;Testing Accuracy:&quot;, \</span><br><span class="line">        sess.run(accuracy, feed_dict=&#123;X: mnist.test.images,</span><br><span class="line">                                      Y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><p>读取pb文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.platform import gfile</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">with gfile.FastGFile(&apos;D:/MMNIST/modelpb.pb&apos;, &apos;rb&apos;) as f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def) # 导入计算图</span><br><span class="line"></span><br><span class="line"># 初始化的过程    </span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"># 需要先复原变量</span><br><span class="line">print(sess.run(&apos;b1:0&apos;))</span><br><span class="line">## 1</span><br><span class="line">#</span><br><span class="line">## 输入</span><br><span class="line">input_x = sess.graph.get_tensor_by_name(&apos;x:0&apos;)</span><br><span class="line">print(input_x)</span><br><span class="line">#input_y = sess.graph.get_tensor_by_name(&apos;y:0&apos;)</span><br><span class="line">#</span><br><span class="line">op = sess.graph.get_tensor_by_name(&apos;op_to_store:0&apos;)</span><br><span class="line">print(op)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;tensorflow模型可以保存为meta文件或者pb文件本文主要是这两种方式的实践。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>能生成文本的RNN</title>
    <link href="https://frankindf.github.io/2018/04/29/%E8%83%BD%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC%E7%9A%84RNN/"/>
    <id>https://frankindf.github.io/2018/04/29/能生成文本的RNN/</id>
    <published>2018-04-29T13:54:53.000Z</published>
    <updated>2018-05-13T15:09:44.958Z</updated>
    
    <content type="html"><![CDATA[<p>本文是一个RNN文本生成器的实践。</p><a id="more"></a><p>首先定义一些工具函数，这些函数可以将文本生成字典，并实现向量化。</p><p>神经网络在进行计算时，样本过大，整个训练集同时训练需要巨大的资源，因此需要将其分为数个Batch进行批训练。<code>batch_generator</code>函数用来生成Batch。函数流程如下：</p><p>复制数据 -&gt; 计算每批参数大小 -&gt; 计算分批数 -&gt;  改变arr维度 -&gt; 打乱训练集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import copy</span><br><span class="line">import time</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import pickle</span><br><span class="line">#生成</span><br><span class="line">def batch_generator(arr, n_seqs, n_steps):</span><br><span class="line">    #拷贝输入的数组，保证不改变原对象</span><br><span class="line">    arr = copy.copy(arr)</span><br><span class="line">    #单个训练集大小=单个数据大小*训练次数</span><br><span class="line">    batch_size = n_seqs * n_steps</span><br><span class="line">    #训练集个数为数据集长度除以batch集大小</span><br><span class="line">    n_batches = int(len(arr) / batch_size)</span><br><span class="line">    #将arr转换size</span><br><span class="line">    arr = arr[:batch_size * n_batches]</span><br><span class="line">    arr = arr.reshape((n_seqs, -1))</span><br><span class="line">    while True:</span><br><span class="line">    #将数据集打乱</span><br><span class="line">        np.random.shuffle(arr)</span><br><span class="line">        for n in range(0, arr.shape[1], n_steps):</span><br><span class="line">            x = arr[:, n:n + n_steps]</span><br><span class="line">            #生成一个和x相同的0矩阵</span><br><span class="line">            y = np.zeros_like(x)</span><br><span class="line">            #对时间序列</span><br><span class="line">            #x=[a,b,c,d]则y=[b,c,d,a]</span><br><span class="line">            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]</span><br><span class="line">            yield x, y</span><br></pre></td></tr></table></figure><p><code>TextConverter</code>类初始化一个长度为50000的字典，并保存为pickle文件，如果文件已经存在直接打开文件，加载单词表vocab</p><p>如果不存在则按照如下步骤产生vocab，需要将单词中出现次数较少的舍弃：</p><p>set生成单词集合 -&gt; 统计单词出现次数并存储在vocab_count中 -&gt; 生成列表vocab_count_list元素为单词和出现次数 -&gt; 将vocab_count_list按出现次数排列 -&gt; 舍弃超过max_vocab的部分 -&gt; 定义vocab</p><p>其中<code>word_to_int_table</code>为单词转换为数字的字典,<code>int_to_word_table</code>为数字转换为单词的字典.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class TextConverter(object):</span><br><span class="line">    def __init__(self, text=None, max_vocab=5000, filename=None):</span><br><span class="line">        if filename is not None:</span><br><span class="line">            with open(filename, &apos;rb&apos;) as f:</span><br><span class="line">                #使用pickle模块加载文件</span><br><span class="line">                self.vocab = pickle.load(f)</span><br><span class="line">        else:</span><br><span class="line">            vocab = set(text)</span><br><span class="line">            print(len(vocab))</span><br><span class="line">            # 统计单词出现次数</span><br><span class="line">            vocab_count = &#123;&#125;</span><br><span class="line">            for word in vocab:</span><br><span class="line">                vocab_count[word] = 0</span><br><span class="line">            for word in text:</span><br><span class="line">                vocab_count[word] += 1</span><br><span class="line">            vocab_count_list = []</span><br><span class="line">            #生成单词-频率列表</span><br><span class="line">            for word in vocab_count:</span><br><span class="line">                vocab_count_list.append((word, vocab_count[word]))</span><br><span class="line">            vocab_count_list.sort(key=lambda x: x[1], reverse=True)</span><br><span class="line">            #当单词数超限值，提取限值内的部分</span><br><span class="line">            if len(vocab_count_list) &gt; max_vocab:</span><br><span class="line">                vocab_count_list = vocab_count_list[:max_vocab]</span><br><span class="line">            vocab = [x[0] for x in vocab_count_list]</span><br><span class="line">            #生成前5000个单词的列表</span><br><span class="line">            self.vocab = vocab</span><br><span class="line">        #生成字典</span><br><span class="line">        self.word_to_int_table = &#123;c: i for i, c in enumerate(self.vocab)&#125;</span><br><span class="line">        self.int_to_word_table = dict(enumerate(self.vocab))</span><br></pre></td></tr></table></figure><p><code>vocab_size</code>统计单词数量</p><p><code>word_to_int</code>将单词转换为对应数字</p><p><code>int_to_word</code>将数字转换为对应单词，将不再字典中的单词输出为<code>&lt;unk&gt;</code></p><p><code>text_to_arr</code>使用<code>word_to_int</code>将文章所有的词转换为向量</p><p><code>arr_to_text</code>使用<code>int_to_word</code>将数组转换为文章</p><p><code>save_to_file</code>将单词保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#单词数量</span><br><span class="line">def vocab_size(self):</span><br><span class="line">    return len(self.vocab) + 1</span><br><span class="line">#单词对应到数字</span><br><span class="line">def word_to_int(self, word):</span><br><span class="line">    if word in self.word_to_int_table:</span><br><span class="line">        return self.word_to_int_table[word]</span><br><span class="line">    else:</span><br><span class="line">        #不在字典中的统一记作len(vocab)</span><br><span class="line">        return len(self.vocab)</span><br><span class="line">#数字对应到单词</span><br><span class="line">#如果不在字典，输出&lt;unk&gt;</span><br><span class="line">def int_to_word(self, index):</span><br><span class="line">    if index == len(self.vocab):</span><br><span class="line">        return &apos;&lt;unk&gt;&apos;</span><br><span class="line">    elif index &lt; len(self.vocab):</span><br><span class="line">        return self.int_to_word_table[index]</span><br><span class="line">    else:</span><br><span class="line">        raise Exception(&apos;Unknown index!&apos;)</span><br><span class="line">#文章转换为数组</span><br><span class="line">def text_to_arr(self, text):</span><br><span class="line">    arr = []</span><br><span class="line">    for word in text:</span><br><span class="line">        arr.append(self.word_to_int(word))</span><br><span class="line">    return np.array(arr)</span><br><span class="line">#数组转换为文章</span><br><span class="line">def arr_to_text(self, arr):</span><br><span class="line">    words = []</span><br><span class="line">    for index in arr:</span><br><span class="line">        words.append(self.int_to_word(index))</span><br><span class="line">    return &quot;&quot;.join(words)</span><br><span class="line">#保存文件</span><br><span class="line">def save_to_file(self, filename):</span><br><span class="line">    with open(filename, &apos;wb&apos;) as f:</span><br><span class="line">        pickle.dump(self.vocab, f)</span><br></pre></td></tr></table></figure><p><strong>模型部分</strong></p><p>定义一个CharRNN类，初始化以下参数</p><pre><code>self.num_classes = num_classes #self.num_seqs = num_seqs #句子数量self.num_steps = num_steps #训练步self.lstm_size = lstm_size #lstm的层数self.num_layers = num_layers #神经网络层数self.learning_rate = learning_rate #学习率self.grad_clip = grad_clip #？？？？self.train_keep_prob = train_keep_prob #？？？self.use_embedding = use_embedding #？？self.embedding_size = embedding_size #？？tf.reset_default_graph()self.build_inputs()self.build_lstm()self.build_loss()self.build_optimizer()self.saver = tf.train.Saver()</code></pre><p>构造输入函数：</p><p>CNN神经网络是使用句子向量的前n-1个单词预测第n个单词，因此inputs和targets维度相同</p><p>定义输入序列数、样本值</p><p><code>tf.one_hot(self.inputs, self.num_classes)</code>将inputs按照num_classes进行编码，张量中数据对应类为1其余参数为0</p><p><code>tf.nn.embedding_lookup(embedding, self.inputs)</code>相当于numpy中按索引查找元素，其中inputs相当于索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def build_inputs(self):</span><br><span class="line">        with tf.name_scope(&apos;inputs&apos;):</span><br><span class="line">        #定义输入变量，有num_seqs个句子，num_steps列</span><br><span class="line">            self.inputs = tf.placeholder(tf.int32, shape=(</span><br><span class="line">                self.num_seqs, self.num_steps), name=&apos;inputs&apos;)</span><br><span class="line">        #定义参数分类，和inputs维度相同        </span><br><span class="line">            self.targets = tf.placeholder(tf.int32, shape=(</span><br><span class="line">                self.num_seqs, self.num_steps), name=&apos;targets&apos;)</span><br><span class="line">            #定义droup_out的keep_prob参数</span><br><span class="line">            self.keep_prob = tf.placeholder(tf.float32, name=&apos;keep_prob&apos;)</span><br><span class="line">            # 对于中文，需要使用embedding层</span><br><span class="line">            # 英文字母没有必要用embedding层</span><br><span class="line">            if self.use_embedding is False:</span><br><span class="line">                self.lstm_inputs = tf.one_hot(self.inputs, self.num_classes)</span><br><span class="line">            else:</span><br><span class="line">                with tf.device(&quot;/cpu:0&quot;):</span><br><span class="line">                    embedding = tf.get_variable(&apos;embedding&apos;, [self.num_classes, self.embedding_size])</span><br><span class="line">                    self.lstm_inputs = tf.nn.embedding_lookup(embedding, self.inputs)</span><br></pre></td></tr></table></figure><p>构建lstm神经网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def build_lstm(self):</span><br><span class="line">    # 创建单个cell并堆叠多层</span><br><span class="line">    def get_a_cell(lstm_size, keep_prob):</span><br><span class="line">    #建立lstm实例</span><br><span class="line">        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)</span><br><span class="line">        #设置每次有多少神经元不激活</span><br><span class="line">        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)</span><br><span class="line">        return drop</span><br><span class="line"></span><br><span class="line">    with tf.name_scope(&apos;lstm&apos;):</span><br><span class="line">        cell = tf.nn.rnn_cell.MultiRNNCell(</span><br><span class="line">            [get_a_cell(self.lstm_size, self.keep_prob) for _ in range(self.num_layers)]</span><br><span class="line">        )</span><br><span class="line">        self.initial_state = cell.zero_state(self.num_seqs, tf.float32)</span><br><span class="line"></span><br><span class="line">        # 通过dynamic_rnn对cell展开时间维度</span><br><span class="line">        self.lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_inputs, initial_state=self.initial_state)</span><br><span class="line"></span><br><span class="line">        # 通过lstm_outputs得到概率</span><br><span class="line">        seq_output = tf.concat(self.lstm_outputs, 1)</span><br><span class="line">        x = tf.reshape(seq_output, [-1, self.lstm_size])</span><br><span class="line"></span><br><span class="line">        with tf.variable_scope(&apos;softmax&apos;):</span><br><span class="line">            softmax_w = tf.Variable(tf.truncated_normal([self.lstm_size, self.num_classes], stddev=0.1))</span><br><span class="line">            softmax_b = tf.Variable(tf.zeros(self.num_classes))</span><br><span class="line"></span><br><span class="line">        self.logits = tf.matmul(x, softmax_w) + softmax_b</span><br><span class="line">        self.proba_prediction = tf.nn.softmax(self.logits, name=&apos;predictions&apos;)</span><br></pre></td></tr></table></figure><p>定义损失函数</p><p><code>tf.one_hot(self.targets, self.num_classes)</code>对targets进行one-hot编码</p><p>损失函数为交叉熵损失，关于交叉熵损失可以参考另外一篇文章</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def build_loss(self):</span><br><span class="line">    with tf.name_scope(&apos;loss&apos;):</span><br><span class="line">        y_one_hot = tf.one_hot(self.targets, self.num_classes)</span><br><span class="line">        y_reshaped = tf.reshape(y_one_hot, self.logits.get_shape())</span><br><span class="line">        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=y_reshaped)</span><br><span class="line">        self.loss = tf.reduce_mean(loss)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def build_optimizer(self):</span><br><span class="line">    # 使用clipping gradients</span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line">    grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), self.grad_clip)</span><br><span class="line">    train_op = tf.train.AdamOptimizer(self.learning_rate)</span><br><span class="line">    self.optimizer = train_op.apply_gradients(zip(grads, tvars))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def train(self, batch_generator, max_steps, save_path, save_every_n, log_every_n):</span><br><span class="line">     self.session = tf.Session()</span><br><span class="line">     with self.session as sess:</span><br><span class="line">         sess.run(tf.global_variables_initializer())</span><br><span class="line">         # Train network</span><br><span class="line">         step = 0</span><br><span class="line">         new_state = sess.run(self.initial_state)</span><br><span class="line">         for x, y in batch_generator:</span><br><span class="line">             step += 1</span><br><span class="line">             start = time.time()</span><br><span class="line">             feed = &#123;self.inputs: x,</span><br><span class="line">                     self.targets: y,</span><br><span class="line">                     self.keep_prob: self.train_keep_prob,</span><br><span class="line">                     self.initial_state: new_state&#125;</span><br><span class="line">             batch_loss, new_state, _ = sess.run([self.loss,</span><br><span class="line">                                                  self.final_state,</span><br><span class="line">                                                  self.optimizer],</span><br><span class="line">                                                 feed_dict=feed)</span><br><span class="line"></span><br><span class="line">             end = time.time()</span><br><span class="line">             # control the print lines</span><br><span class="line">             if step % log_every_n == 0:</span><br><span class="line">                 print(&apos;step: &#123;&#125;/&#123;&#125;... &apos;.format(step, max_steps),</span><br><span class="line">                       &apos;loss: &#123;:.4f&#125;... &apos;.format(batch_loss),</span><br><span class="line">                       &apos;&#123;:.4f&#125; sec/batch&apos;.format((end - start)))</span><br><span class="line">             if (step % save_every_n == 0):</span><br><span class="line">                 self.saver.save(sess, os.path.join(save_path, &apos;model&apos;), global_step=step)</span><br><span class="line">             if step &gt;= max_steps:</span><br><span class="line">                 break</span><br><span class="line">         self.saver.save(sess, os.path.join(save_path, &apos;model&apos;), global_step=step)</span><br></pre></td></tr></table></figure><pre><code>def sample(self, n_samples, prime, vocab_size):    samples = [c for c in prime]    sess = self.session    new_state = sess.run(self.initial_state)    preds = np.ones((vocab_size, ))  # for prime=[]    for c in prime:        x = np.zeros((1, 1))        # 输入单个字符        x[0, 0] = c        feed = {self.inputs: x,                self.keep_prob: 1.,                self.initial_state: new_state}        preds, new_state = sess.run([self.proba_prediction, self.final_state],                                    feed_dict=feed)    c = pick_top_n(preds, vocab_size)    # 添加字符到samples中    samples.append(c)    # 不断生成字符，直到达到指定数目    for i in range(n_samples):        x = np.zeros((1, 1))        x[0, 0] = c        feed = {self.inputs: x,                self.keep_prob: 1.,                self.initial_state: new_state}        preds, new_state = sess.run([self.proba_prediction, self.final_state],                                    feed_dict=feed)        c = pick_top_n(preds, vocab_size)        samples.append(c)    return np.array(samples)def load(self, checkpoint):    self.session = tf.Session()    self.saver.restore(self.session, checkpoint)    print(&apos;Restored from: {}&apos;.format(checkpoint))</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是一个RNN文本生成器的实践。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python的pickle模块</title>
    <link href="https://frankindf.github.io/2018/04/28/python%E7%9A%84pickle%E6%A8%A1%E5%9D%97/"/>
    <id>https://frankindf.github.io/2018/04/28/python的pickle模块/</id>
    <published>2018-04-28T15:01:12.000Z</published>
    <updated>2018-05-13T15:11:06.010Z</updated>
    
    <content type="html"><![CDATA[<p>python的pickle模块可以存储python数据文件，本文是该模块的学习记录。</p><a id="more"></a><h4 id="pickle常用方法"><a href="#pickle常用方法" class="headerlink" title="pickle常用方法"></a><strong>pickle常用方法</strong></h4><h5 id="序列化和反序列化："><a href="#序列化和反序列化：" class="headerlink" title="序列化和反序列化："></a>序列化和反序列化：</h5><p>序列化就是把计算得到的数据保存起来，当需要使用时反序列化把数据恢复，这样有如下好处：</p><ol><li>被pickle的数据，在被多次reload时，不需要重新去计算得到这些数据，这样节省计算机资源，如果你不pickle，你每调用一次数据，就要计算一次。</li><li>通过pickle的数据，被reload时，可以更好的被内存调用，不需要经过数据格式的转换。</li></ol><p>pickle.dump(obj,file,protocol,)</p><p>将obj保存为文件</p><p>pickle.load(file)</p><p>读取pickle文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pickle  </span><br><span class="line">t1=[&apos;a&apos;,3,4]</span><br><span class="line">with open(&apos;temp.pkl&apos;,&apos;wb+&apos;) as f: </span><br><span class="line">v1 = pickle.dump(t1)</span><br><span class="line">#可以看到文件夹里多了temp.pkl</span><br><span class="line">with open(&apos;d:temp.pkl&apos;,&apos;rb&apos;) as f: </span><br><span class="line">    v2=pickle.load(f)</span><br><span class="line">    print(v2)</span><br></pre></td></tr></table></figure><p>pickle.dumps(obj)</p><p>将obj保存为pickle对象</p><p>pickle.loads(obj)</p><p>读取pickle对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># dumps功能</span><br><span class="line">import pickle</span><br><span class="line">data = [3,6,8]  </span><br><span class="line"># dumps 将数据通过特殊的形式转换为只有python语言认识的字符串</span><br><span class="line">v2 = pickle.dumps(data)</span><br><span class="line">print(v2)            </span><br><span class="line">mes = pickle.loads(v2)</span><br><span class="line">print(mes)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python的pickle模块可以存储python数据文件，本文是该模块的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow学习率衰减实现</title>
    <link href="https://frankindf.github.io/2018/04/28/tensorflow%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E5%AE%9E%E7%8E%B0/"/>
    <id>https://frankindf.github.io/2018/04/28/tensorflow学习率衰减实现/</id>
    <published>2018-04-28T14:40:06.000Z</published>
    <updated>2018-04-28T14:40:06.961Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow中的滑动平均值</title>
    <link href="https://frankindf.github.io/2018/04/28/tensorflow%E4%B8%AD%E7%9A%84%E6%BB%91%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%80%BC/"/>
    <id>https://frankindf.github.io/2018/04/28/tensorflow中的滑动平均值/</id>
    <published>2018-04-28T12:10:50.000Z</published>
    <updated>2018-05-13T15:11:30.893Z</updated>
    
    <content type="html"><![CDATA[<p>在tensorflow中使用滑动平均值可以有效提高计算效率，本文为滑动平均值的学习记录。</p><a id="more"></a><p><strong>计算原理</strong></p><p>TensorFlow中采用<code>tf.train.ExponentialMovingAverage</code>函数更新参数，函数每次更新参数后需要和上一轮的变量按照下面公式进行更新：</p><p>$new_value = (1-\alpha)<em>value+\alpha </em>old_ value$</p><p>可以看到参数更新的本质就是采用一阶低通滤波法对本次采样值与上次滤波输出值进行加权，得到有效滤波值，使得输出对输入有反馈作用，同时参数波动会减小，$\alpha​$越大参数更新越慢，$\alpha​$越小参数越灵敏。</p><p><strong>直接定义滑动平均值对象</strong></p><ol><li>定义需要更新的参数v、本次更新为第几步的参数step(可选)</li><li>定义滑动平均值<code>ema = tf.train.ExponentialMovingAverage(decay,step)</code></li><li>将需要更新的参数v添加到ema，使用<code>ema,apply([v])</code></li><li><code>sess.run</code>更新滑动平均值</li><li>重复1-4</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">var0 = tf.Variable(...)</span><br><span class="line">var1 = tf.Variable(...)</span><br><span class="line"></span><br><span class="line">opt_op = opt.minimize(my_loss, [var0, var1])</span><br><span class="line"></span><br><span class="line"># </span><br><span class="line">ema = tf.train.ExponentialMovingAverage(decay=0.9999)</span><br><span class="line"></span><br><span class="line">with tf.control_dependencies([opt_op]):</span><br><span class="line">    # Create the shadow variables, and add ops to maintain moving averages</span><br><span class="line">    # of var0 and var1. This also creates an op that will update the moving</span><br><span class="line">    # averages after each training step.  This is what we will use in place</span><br><span class="line">    # of the usual training op.</span><br><span class="line">    training_op = ema.apply([var0, var1])</span><br></pre></td></tr></table></figure><p><strong>定义变量将其保存为滑动平均值</strong></p><p>1.定义滑动平均值对象</p><p>2.定义滑动平均值变量名</p><p>3.将变量保存到对应滑动平均值名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 定义滑动平均值变量名shadow_var0_name，shadow_var1_name，将var0和var1保存对应这两个滑动平均值</span><br><span class="line"># 加载后var0和var1以滑动平均的方式计算</span><br><span class="line">shadow_var0_name = ema.average_name(var0)</span><br><span class="line">shadow_var1_name = ema.average_name(var1)</span><br><span class="line">saver = tf.train.Saver(&#123;shadow_var0_name: var0, shadow_var1_name: var1&#125;)</span><br><span class="line">saver.restore(...checkpoint filename...)</span><br></pre></td></tr></table></figure><p><strong>函数的使用</strong></p><p><code>tf.train.ExponentialMovingAverage</code>定义滑动平均值计算，定义一个滑动平均值计算对象</p><p>初始化参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(   decay,  #衰减率</span><br><span class="line">    num_updates=None, #更新的步数，由函数自己维护</span><br><span class="line">    zero_debias=False, #</span><br><span class="line">    name=&apos;ExponentialMovingAverage&apos; #滑动平均值的名称</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>在tensorflow中，为了保证开始时参数更新够快,实际的更新公式如下：</p><p>$min(decay, (1 + num_updates) / (10 + num_updates))$</p><p><strong>apply（var_list）</strong></p><p>将需要更新的参数赋给计算函数，使用<code>trainable=False</code>可以创建影子变量,通过<code>tf.global_variables()</code>可以返回step.</p><ul><li>​</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    decay,</span><br><span class="line">    num_updates=None,</span><br><span class="line">    zero_debias=False,</span><br><span class="line">    name=&apos;ExponentialMovingAverage&apos;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>average(var)</strong></p><p>使用average方法返回滑动平均值</p><p><strong>average_name(var)</strong></p><p>返回滑动平均值的变量名</p><p><strong>计算滑动平均值实践</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">v1 = tf.Variable(0, dtype=tf.float32)   # 定义一个变量，初始值为0</span><br><span class="line">step = tf.Variable(0, trainable=False)    # step为迭代轮数变量，控制衰减率</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(0.5)  # 初始设定衰减率为0.99</span><br><span class="line">maintain_averages_op = ema.apply([v1])                 # 更新列表中的变量</span><br><span class="line">init_op = tf.global_variables_initializer()        # 初始化所有变量</span><br><span class="line">sess.run(init_op)</span><br><span class="line">print(sess.run([v1, ema.average(v1)]))                # 输出初始化后变量v1的值和v1的滑动平均值</span><br><span class="line">sess.run(tf.assign(v1, 5))                            # 更新v1的值</span><br><span class="line">sess.run(maintain_averages_op)                        # 更新v1的滑动平均值</span><br><span class="line">print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">sess.run(tf.assign(step, 10000))                      # 更新迭代轮转数step</span><br><span class="line">sess.run(tf.assign(v1, 10))</span><br><span class="line">sess.run(maintain_averages_op)</span><br><span class="line">print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">                                                      # 再次更新滑动平均值，</span><br><span class="line">sess.run(maintain_averages_op)</span><br><span class="line">print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">                                                      # 更新v1的值为15</span><br><span class="line">sess.run(tf.assign(v1, 15))</span><br><span class="line"></span><br><span class="line">sess.run(maintain_averages_op)</span><br><span class="line">print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">out：</span><br><span class="line">[0.0, 0.0]</span><br><span class="line">[5.0, 2.5]#更新值为5，decay=0.5,结果=(5*0.5+0*0.5)</span><br><span class="line">[10.0, 6.25]#更新值为10，结果=(2.5*0.5+10*0.5)</span><br><span class="line">[10.0, 8.125]</span><br><span class="line">[15.0, 11.5625]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在tensorflow中使用滑动平均值可以有效提高计算效率，本文为滑动平均值的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python实践记录</title>
    <link href="https://frankindf.github.io/2018/04/28/python%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"/>
    <id>https://frankindf.github.io/2018/04/28/python实践记录/</id>
    <published>2018-04-27T16:24:01.000Z</published>
    <updated>2018-04-27T16:50:10.831Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录python实践过程中的一些知识点。</p><a id="more"></a><p><strong>os模块</strong></p><p>os.walk(path) 得到</p><p>输入：文件夹路径</p><p>返回：三元tupple依次是(dirpath,dirnames,filenames)</p><p>os.path.basename()</p><p>输入：文件路径</p><p>输出：当前文件夹名</p><p>os.paht.dirname</p><p>输入：文件路径</p><p>输出：根目录名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">path = r&apos;D:\path&apos;</span><br><span class="line">print(list(os.walk(path)))</span><br><span class="line">out:</span><br><span class="line">[(&apos;D:\\path&apos;, [&apos;to&apos;], [&apos;FILEINto.txt&apos;]),</span><br><span class="line">(&apos;D:\\path\\to&apos;, [&apos;MNIST_data&apos;], [&apos;fileInMNIST.rar&apos;]), </span><br><span class="line">(&apos;D:\\path\\to\\MNIST_data&apos;, [], [&apos;t10k-images-idx3-ubyte.gz&apos;, &apos;t10k-labels-idx1-ubyte.gz&apos;, &apos;train-images-idx3-ubyte.gz&apos;, &apos;train-labels-idx1-ubyte.gz&apos;])]</span><br></pre></td></tr></table></figure><p>打印文件夹结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def fileCntIn(currPath):    </span><br><span class="line">    #计算总路径内文件数，通过将每个文件夹内文件数相加</span><br><span class="line">    return sum([len(files) for root, dirs, files in os.walk(currPath)])    </span><br><span class="line">    </span><br><span class="line">def dirsTree(startPath):    </span><br><span class="line">    &apos;&apos;&apos;&apos;&apos;&apos;&apos;树形打印出目录结构&apos;&apos;&apos;    </span><br><span class="line">    for root, dirs, files in os.walk(startPath):    </span><br><span class="line">        #获取当前目录下文件数    </span><br><span class="line">        fileCount = fileCntIn(root)    </span><br><span class="line">        #获取当前目录相对输入目录的层级关系,整数类型，os.sep为跨平台分割符    </span><br><span class="line">        level = root.replace(startPath, &apos;&apos;).count(os.sep)    </span><br><span class="line">        #树形结构显示关键语句    </span><br><span class="line">        #根据目录的层级关系，重复显示&apos;| &apos;间隔符，    </span><br><span class="line">        #第一层 &apos;| &apos;    </span><br><span class="line">        #第二层 &apos;| | &apos;    </span><br><span class="line">        #第三层 &apos;| | | &apos;    </span><br><span class="line">        #依此类推...    </span><br><span class="line">        #在每一层结束时，合并输出 &apos;|____&apos;    </span><br><span class="line">        indent = &apos;| &apos; * 1 * level + &apos;|____&apos;    </span><br><span class="line">        print（&apos;%s%s -r:%s&apos; % (indent, os.path.split(root)[1], fileCount))</span><br><span class="line">        for file in files:  </span><br><span class="line">            indent = &apos;| &apos; * 1 * (level+1) + &apos;|____&apos;    </span><br><span class="line">            print(&apos;%s%s&apos; % (indent, file))   </span><br><span class="line">    </span><br><span class="line">if __name__ == &apos;__main__&apos;:    </span><br><span class="line">dirsTree(path)  </span><br><span class="line">out:</span><br><span class="line">|____path -r:6</span><br><span class="line">| |____FILEINto.txt</span><br><span class="line">| |____to -r:5</span><br><span class="line">| | |____fileInMNIST.rar</span><br><span class="line">| | |____MNIST_data -r:4</span><br><span class="line">| | | |____t10k-images-idx3-ubyte.gz</span><br><span class="line">| | | |____t10k-labels-idx1-ubyte.gz</span><br><span class="line">| | | |____train-images-idx3-ubyte.gz</span><br><span class="line">| | | |____train-labels-idx1-ubyte.gz</span><br></pre></td></tr></table></figure><p><strong>glob模块</strong></p><p>glob 查找符合特定规则的文件路径名<br>glob.glob(path)</p><p>输入文件夹路径</p><p>返回特定文件路径</p><p>glob.iglob(path)</p><p>返回迭代器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">path = r&apos;D:\path\to\MNIST_data\*.rar&apos;</span><br><span class="line">print(glob.glob(path))</span><br><span class="line">out:</span><br><span class="line">[&apos;D:\\path\\to\\MNIST_data\\t10k-images-idx3-ubyte.gz&apos;, &apos;D:\\path\\to\\MNIST_data\\t10k-labels-idx1-ubyte.gz&apos;, &apos;D:\\path\\to\\MNIST_data\\train-images-idx3-ubyte.gz&apos;, &apos;D:\\path\\to\\MNIST_data\\train-labels-idx1-ubyte.gz&apos;]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录python实践过程中的一些知识点。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>jieba分词实践</title>
    <link href="https://frankindf.github.io/2018/04/27/jieba%E5%88%86%E8%AF%8D%E5%AE%9E%E8%B7%B5/"/>
    <id>https://frankindf.github.io/2018/04/27/jieba分词实践/</id>
    <published>2018-04-26T16:00:22.000Z</published>
    <updated>2018-04-27T16:19:35.759Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是jieba分词的学习记录。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">s = u&apos;我想要在中广核的海滩边上走一走&apos;</span><br><span class="line"></span><br><span class="line"># cut方法</span><br><span class="line"></span><br><span class="line">cut = jieba.cut(s)</span><br><span class="line">list(cut)</span><br><span class="line">[&apos;我&apos;, &apos;想要&apos;, &apos;在&apos;, &apos;中广核&apos;, &apos;的&apos;, &apos;海滩&apos;, &apos;边上&apos;, &apos;走&apos;, &apos;一&apos;, &apos;走&apos;]</span><br><span class="line">s = u&apos;武汉市长江大桥和武汉市长姜大桥。&apos;</span><br></pre></td></tr></table></figure><p>全模式<br>尽量分成更多的词</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&apos;,&apos;.join(jieba.cut(s,cut_all = True))</span><br><span class="line">&apos;武汉,武汉市,市长,长江,长江大桥,大桥,和,武汉,武汉市,市长,姜,大桥&apos;</span><br><span class="line">&apos;,&apos;.join(jieba.cut_for_search(s))</span><br><span class="line">&apos;武汉,武汉市,长江,大桥,长江大桥,和,武汉,武汉市,长姜,大桥&apos;</span><br></pre></td></tr></table></figure><p>获取词性可以用jieba.posseg</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import jieba.posseg as psg</span><br><span class="line">print([(x.word,x.flag) for x in psg.cut(s)])</span><br><span class="line">[(&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长江大桥&apos;, &apos;ns&apos;), (&apos;和&apos;, &apos;c&apos;), (&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长&apos;, &apos;a&apos;), (&apos;姜&apos;, &apos;n&apos;), (&apos;大桥&apos;, &apos;ns&apos;)]</span><br></pre></td></tr></table></figure><p>把姜前面的长识别成了形容词，哈哈哈<br>显示了每个词的词性<br>还可以对分词进行筛选，用startswith，获得名词</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print([(x.word,x.flag) for x in psg.cut(s) if x.flag.startswith(&apos;n&apos;)])</span><br><span class="line">[(&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长江大桥&apos;, &apos;ns&apos;), (&apos;武汉市&apos;, &apos;ns&apos;), (&apos;姜&apos;, &apos;n&apos;), (&apos;大桥&apos;, &apos;ns&apos;)]</span><br></pre></td></tr></table></figure><p>获取词频<br>Counter().most_common(20)<br>添加用户字典，定义用户字典<br>词语    词频 词性<br>姜大桥   5    ‘ns’<br>其中词频是一个数字，词性为自定义的词性，要注意的是词频数字和空格都要是半角的。<br>再进行分词</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jieba.load_userdict(&apos;user_dict.txt&apos;)</span><br><span class="line">print([(x.word,x.flag) for x in psg.cut(s)])</span><br><span class="line">[(&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长江大桥&apos;, &apos;ns&apos;), (&apos;和&apos;, &apos;c&apos;), (&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长&apos;, &apos;a&apos;), (&apos;姜大桥&apos;, &apos;ns&apos;)]姜大桥已经变成人名了</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是jieba分词的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>隐马尔科夫模型</title>
    <link href="https://frankindf.github.io/2018/04/26/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>https://frankindf.github.io/2018/04/26/隐马尔科夫模型/</id>
    <published>2018-04-26T15:51:53.000Z</published>
    <updated>2018-04-26T15:59:40.206Z</updated>
    
    <content type="html"><![CDATA[<p>使用隐马尔科夫模型可以解决有隐藏状态的观测序列问题，本文是对隐马尔科夫模型的学习记录。</p><a id="more"></a><p>隐马尔科夫要解决的问题：<br>前提：<br>初始状态矩阵$\pi$<br>状态转移矩阵A<br>观测概率矩阵B</p><p>假设条件：<br>齐次假设：$t$时刻的状态只依赖于$t-1$时刻的状态<br>观测独立性假设：任意时刻状态值依赖此刻的马尔科夫链的状态</p><p>在有观测不到的隐藏状态时，解决下面3个问题：<br>1）给定一个观测序列，怎计算这个序列出现的概率,即概率问题<br>2）已经有一个观测序列，怎么估计出A、B、$\pi$使其在该序列下出现可能最大，即学习问题<br>3）在给定观测序列后计算条件概率最大的<strong>状态序列</strong>，预测问题<br>针对问题1通常使用前向算法和后向算法：<br>定义<br>$$<br>\alpha_t(i) = P(O_1,O_2,…,O_t,i_t=q_i|\lambda)<br>$$<br>$A{ij}$代表状态由i转移到j的概率，$b_{j}(o_1)$代表状态j时，观测到$o_1$的概率</p><p>对于初始值,在i状态下观测到o_1<br>$$<br>\alpha_1 = \pi_ib_i(o1)<br>$$</p><p>对于$t=1,2,…,T-1$,t时刻在j状态，t+1时刻在i状态，在t+1观测到$o_{t+1}$的概率如下：<br>$$<br>\alpha_{t+1}(i) = [\sum _{j=1} ^{N} \alpha_{t}(j)a_ji]b_i(o_{t+1})<br>$$</p><p>观测到序列O：<br>$$<br>P(O|\lambda) = \sum_{i=1}^{N}\alpha_{T}(i)<br>$$<br>后向算法：<br>$$<br>\beta_t(i) = P(o_{t+1},o_{t+2},…,o_T|i_t=q_i,\lambda)<br>$$</p><p>$t$时刻的状态为$q_i$,从$t+1$到$T$部分观测序列为$o_t+1,o_t+2,…,o_T$的概率为后向概率<br>最终时刻为T，T+1不存在，所以令$\beta_T(i) = 1$<br>在$t = T-1,T-2,…,1$可以看到从$T-1$开始递减<br>$\beta_t(i) = \sum_{j=1}^{N}a_ijb_j(o_{t+1}\beta_{t+1}(j))$<br>将前面的项代入：<br>$$<br>P(O|\lambda) = \sum_{i=1}^{N}\pi_ib_i(o_1)\beta_1(i)<br>$$<br>可以统一写成<br>$$<br>P(O|\lambda) = \sum_{i=1} ^{N}\sum_{j=1} ^{N}\alpha _t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)<br>$$<br>单个状态概率的计算公式：<br>时刻t处于状态$q_i$的概率，<br>$$<br>\gamma_t(i) = P(i_t = q_i|O,\lambda)=\frac{i_t=q_i,O|\lambda}{P(O|lambda)}<br>$$</p><p>由于前t个数据观测到qi的概率乘以从后面开始观察到$q_i$的概率<br>$$<br>\alpha_ t(i)\beta _t(i) = P(i_t=q_i,O|\lambda)<br>$$<br>得到：<br>$$<br>\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)}<br>$$<br>在t时刻处于q_i,t+1时刻处于q_j的概率<br>$$<br>\xi_t(i,j) = \frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i,i_{t+1}=q_j,O|\lambda)}<br>$$</p><p>$$<br>P(i_t=q_i,i_{t+1}=q_j,O|\lambda) = \alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)<br>$$</p><p>$$<br>\xi_t(i,j) = \frac{\alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)}<br>$$</p><p>Baum-Welch算法<br>1.确定对数似然函数$logP(O,I|\lambda)$<br>2.E步，推断概率分布，并求期望<br>$$<br>Q(\lambda,\bar{\lambda}) = E_Ilog(P(O,I|\lambda))<br>$$</p><p>$$<br>Q(\lambda,\bar{\lambda}) = \sum_{I}logP(O,I|\lambda)P(O,I|\bar{\lambda})<br>$$</p><p>3.M步,计算估计参数下<br>$$<br>\pi_i = \frac{P(O,i_1=i|\bar{\lambda})}{P(O|\bar{\lambda})}=\gamma_1(i)<br>$$</p><p>$$<br>a_{ij} = \frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\bar{lambda})}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\gamma _t(i)}<br>$$</p><p>$$<br>b_j(k) = \frac{\sum_{t=1}^{T}P(O,i_t=j)|\bar{\lambda})I(o_t=v_k)}{\sum_{t=1}^{T}P(O,i_t=j|bar{lambda})} = \frac{\sum _{t=1} ^{T}\gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}<br>$$</p><p>维比特算法<br>贪心算法，如果当前路径为最优路径，则当前路径前一步也是最优路径。<br>初始化<br>$$<br>\delta_1(i) = pi_ib_i(o_1)<br>$$</p><p>$$<br>\psi_1(i)=0<br>$$</p><p>递推<br>$$<br>\delta_t(i) = max_{1\leqslant j \leqslant N} [\delta _{t-1}(j)a_{ji}]b_i(o_1)<br>$$</p><p>$$<br>\psi_t(i) = arg max_{1\leqslant j \leqslant N} [\delta _{t-1}(j)a_{ji}]<br>$$</p><p>终止<br>$$<br>\delta_t(i) = max_{1\leqslant j \leqslant N} \delta _T(i)<br>$$</p><p>$$<br>\psi_t(i) = arg max_{1\leqslant j \leqslant N} [\delta _{T}(i)]<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用隐马尔科夫模型可以解决有隐藏状态的观测序列问题，本文是对隐马尔科夫模型的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>拉格朗日对偶</title>
    <link href="https://frankindf.github.io/2018/04/26/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/"/>
    <id>https://frankindf.github.io/2018/04/26/拉格朗日对偶/</id>
    <published>2018-04-26T15:49:44.000Z</published>
    <updated>2018-04-26T15:50:58.736Z</updated>
    
    <content type="html"><![CDATA[<p>在求解约束条件下最优化的问题时，需要使用拉格朗日乘子法，本文是拉格朗日乘子法的学习记录。</p><a id="more"></a><p>拉格朗日乘子法可以分为3个问题：<br>(1)原始问题，即求f(x)最小值的问题等价于求拉格朗日乘子的最大值的最小值。<br>在求解下面约束条件最小值时<br>$$<br>min f(x)​<br>$$</p><p>$$<br>s.t. c_i(x) \leqslant 0,i=1,2,…,k<br>h_i(x) = 0,i=1,2,…,l<br>$$</p><p>拉格朗日乘子可以写成：<br>$$<br>L = max_{\alpha \beta} [ f(x) + \sum _{i=1} ^{k}\alpha _ic_i(x) + \sum _{j=1} ^{l}\beta _j h_j(x)<br>$$</p><p>max(L)表达式如下<br>$$<br>\theta _p(x) = max _{\alpha ,\beta }\left [ f(x) + \sum _{i=1} ^{k}\alpha _ic_i(x) + \sum _{j=1} ^{l}\beta _j h_j(x) \right ]<br>$$</p><p>若要满足边界条件$c_i(x) \leqslant 0, h_i(x) = 0$<br>$$<br>\sum _{i=1} ^{k}\alpha _ic_i(x) = 0<br>$$</p><p>$$<br>\sum _{j=1} ^{l}\beta _j h_j(x) = 0<br>$$</p><p>否则$\theta _p(x) $最大值为正无穷<br>此时：<br>$min f(x) $与 $min _{x}max{L(x,\alpha,\beta)}$等价。</p><p>(2)对偶问题，对偶问题如果和原始问题等价可以方便求解。<br>$$<br>max _{x}min_{L(x,\alpha,\beta)}<br>$$<br>(3) 什么时候等价？<br>假设函数f和c是凸函数，h是仿射函数<br>对偶问题小于等于$f(x)$,$f(x)$又小于等于原始问题,但是当满足KTT条件时，对偶问题和原始问题是等价的。<br>KTT条件如下:<br>$$<br>\bigtriangledown _xL(x, \alpha ,\beta )=0<br>$$</p><p>$$<br>\bigtriangledown _\alpha L(x, \alpha ,\beta )=0<br>$$</p><p>$$<br>\bigtriangledown _\beta L(x, \alpha ,\beta )=0<br>$$</p><p>$$<br>\alpha_i c_i(x) = 0<br>$$</p><p>$$<br>c_i(x) \leqslant  0<br>$$</p><p>$$<br>\alpha_i \geqslant  0<br>$$</p><p>$$<br>h_j(x)=0<br>$$</p><p>解决了上面问题，就可以将原始问题和对偶问题等价进行求解，问题也变成了凸函数。</p><p>SVM问题里面都是满足KKT条件的，所以SVM里面求取对偶解就相当于求取原问题的解。为什么要求它的对偶呢？<br>因为kernel，通过对偶之后得到一个向量内积的形式，也就是$xTx$这种形式，而这种形式是kernel所擅长处理的。如果没有对偶，就没有后面的kernel映射，SVM也实现不了非线性分割。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在求解约束条件下最优化的问题时，需要使用拉格朗日乘子法，本文是拉格朗日乘子法的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>最大熵算法</title>
    <link href="https://frankindf.github.io/2018/04/23/%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/23/最大熵算法/</id>
    <published>2018-04-23T12:24:20.000Z</published>
    <updated>2018-04-23T13:35:49.659Z</updated>
    
    <content type="html"><![CDATA[<p>最大熵原理和$logistic$、最大熵马科夫模型都有关，本文是最大熵模型的学习记录。</p><a id="more"></a><p>模型在满足已有的约束条件的情况下，约束外的部分熵越大，模型越好。<br>即对于已知部分，要尽可能确定。<br>对于未知的部分，要保证不确定性。<br>任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。<br>举个例子，若只有约束$P_a+P_b+P_c=1$，则最优的模型每次a、b、c出现的概率均相同，因为只有这种情况才没有新增信息。</p><p>最大熵模型就是要使熵最大化，同时要满足经验分布的期望和模型的期望相同即$E_p = E _\tilde p$:<br>$$<br>{max} H(P) = - \sum _{x,y}\tilde{P}(x)P(x|y)logP(x|y)<br>$$</p><p>$$<br>s.t.\: \: E_p(f_i) = E_{\tilde{p}}(f_i) , i=1,2,3…<br>$$</p><p>$$<br>\sum _{y} P(y|x) = 1<br>$$</p><p>其中,$\tilde{p}$为经验分布<br>约束条件下求极值，可以用拉格朗日乘子法进行求解。</p><p>1）写出拉格朗日乘子方程：<br>$$<br>L(P,W) = -H(P) + w_0(1-\sum _{y}P(y|x)) + \sum _{i=1}^{n}(E_{\tilde{p}}(f_i) - E_p(f_i))<br>$$<br>由$E_\tilde   p = \sum \tilde{P}(x,y)f(x,y)$和$E_p =\sum \tilde{P}(x)P(y|x)f(x,y)$可得：<br>$$<br>L(P,W) = \sum\tilde{P}(x) P(y|x)logP(y|x)+w_0(\sum_{y} P(y|x)-1) \\</p><ul><li>w_i(\sum \tilde{P}(x,y)f(x,y)-\sum \tilde{P}(x)P(y|x)f(x,y))<br>$$</li></ul><p>L是相当于求$min _{P\in C}max _{w}L(P,w)$</p><p>根据对偶原理，可以将$minL(P,w)$转化为求$maxL(P,w)$,问题转化为：<br>$$\min _{P\in C}\max _{w}L(P,w) = \max _{w} \min _{P\in C}L(P,w)$$<br>最终需要求解：<br>$$P_w = arg \min _{P\in C}L(P,w) = L(P,w)$$<br>求解方法一：<br>求偏导<br>$$\frac{\partial L}{\partial P(y|x)} =\sum _{x,y}\tilde{P}(x)(logP(y|x)+1-w_0-\sum ^{n}_{i=1}w_if_i(x,y))$$<br>偏导为0，可以求出$P(y|x)$:<br>$$P(y|x) = exp(\sum ^{n} _{i=1}w_if_i(x,y) + w_0-1)$$<br>由$\sum _{y} P(y|x) = 1$可得<br>$$exp(1-w_0) = \sum exp(\sum^{n} _{i=1}w_if_i(x,y)) $$<br>可以得到<br>$$<br>P(y|x) = \frac{exp(\sum ^{n} _{i=1}w_if_i(x,y))}{Z_w(x)}<br>$$<br>代入L(x)可以求得:<br>$$<br>L(P,W) = \sum\tilde{P}(x) P(y|x)logP(y|x) +  w_i(\sum \tilde{P}(x,y)f(x,y)-\sum \tilde{P}(x)P(y|x)f(x,y))<br>\\=\sum_{x,y}\tilde{P}(x) P(y|x)(logP(y|x) - \sum _{i=1} ^{n}w_if_i(x,y))+\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)<br>\\=\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x,y}\tilde{P}(x) P(y|x)logZ_w(x)<br>\\=\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x}\tilde{P}(x) logZ_w(x)<br>$$<br>求解方法二：<br>最大似然估计求解对数似然函数，参考$logistic$回归中的内容。</p><p>条件概率分布$P(X|Y)$的对数似然函数为：<br>$$<br>L_\tilde{p}(P_w) = log \prod_{x,y}P(y|x)^{\tilde{p}(x,y)}<br>$$<br>将$P(y|x) = \frac{exp(\sum ^{n} _{i=1}w_if_i(x,y))}{Z_w(x)}$代入<br>$$<br>L_\tilde{p}(P_w) =\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x}\tilde{P}(x) logZ_w(x)<br>$$<br><strong>一点思考</strong><br>1.最大熵为什么可以保证模型最优化<br>2.最大熵算法和LR的联系</p><p>可以看到当</p><p>3.最大熵模型原理比较重要的推导：</p><ul><li>p(y|x)求和为1</li><li>经验分布和模型的期望相等<br>​</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最大熵原理和$logistic$、最大熵马科夫模型都有关，本文是最大熵模型的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://frankindf.github.io/2018/04/22/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://frankindf.github.io/2018/04/22/决策树/</id>
    <published>2018-04-22T15:27:25.000Z</published>
    <updated>2018-04-22T15:36:29.089Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录决策树的相关知识和两个例子。</p><a id="more"></a><p>决策树划分<br><strong>信息增益</strong><br>ID3<br>$$<br>Gain = Ent(D)-\sum_{v = 1}^{V}\frac{|D^v|}{|D|}Ent(D^v)<br>$$</p><p>$$<br>Ent (D) = \sum_{k=1}^{|y|}p_klog_{2} p_k<br>$$<br>缺点：趋向子节点多的分类<br>无法处理连续特征<br>无法处理缺失值<br>可能过拟合<br><strong>信息增益率</strong><br>C4.5<br>$$<br>Gain_ratio = \frac{Gain(D,a)}{IV(a)}<br>$$</p><p>$$<br>IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}<br>$$<br>缺点：趋向子节点少的</p><p><strong>基尼系数</strong><br>CART<br>$$<br>Gini(D) = \sum_{k=1}^{|y|}a\sum_{k’\neq k}p_kp_{k’}<br>$$<br><strong>CART回归树</strong><br>对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。</p><p><strong>剪枝</strong><br>预剪枝：划分前计算准确率，如果分类后的准确率降低就不进行分类<br>后剪枝：划分后坍塌，计算准确率，如果准确率提升就不分类</p><p><strong>连续值的处理：</strong><br>将连续值划分为N个区间，计算信息增益</p><p><strong>缺失值的处理</strong><br>1）对于含有缺失值的数据，分类的属性需要做修正<br>$$<br>Gain = e\times Gain( \tilde{D},a)<br>$$<br>2）对于含有缺失值的数据，以属性各值所占的概率分配到不同的值当中,概率为$\tilde{\gamma _v}$</p><p><strong>多变量决策树</strong><br>非叶节点为分类器$\sum _{d}^{i=1} w_ia_i=t$</p><p><strong>SKLEARN实现决策树</strong><br>class sklearn.tree.DecisionTreeClassifier(<br>criterion=’gini’,  /分类特征选择标准，可以用基尼系数’gini’或者熵’entropy’<br>splitter=’best’,   /遍历所有特征用’best’,随机选取局部最优解用’random’,’random’适用数据量大时<br>max_depth=None,    /最大深度<br>min_samples_split=2,/多少个特征以下停止划分<br> min_samples_leaf=1, /最小叶子数，子节点少于多少进行剪枝<br>min_weight_fraction_leaf=0.0,/针对缺失值，权重小于多少进行剪枝<br>max_features=None, /最大特征数，’auto’,’sqrt’,’log2’或者数字，寻找最佳分割点时的最大特征数。<br>random_state=None,<br>max_leaf_nodes=None,<br>min_impurity_decrease=0.0,\不纯度减少多少就要进行分类<br>min_impurity_split=None, \最小不纯度，即纯度大时不再生成子树<br>class_weight=None, /样本权重,可选’balanced’进行自动计算权重，样本量少的类权重高<br>presort=False /是否预先排序<br>)</p><p>决策树可视化样例，使用sklearn中的鸢尾花数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn import tree</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier, export_graphviz</span><br><span class="line">import subprocess</span><br><span class="line">import pydot</span><br><span class="line"># 使用dot文件进行可视化</span><br><span class="line"># sklearn.tree下面的export_graphviz可以输出dot文件</span><br><span class="line"># 定义决策树，使用默认参数</span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">iris = load_iris()</span><br><span class="line"># 进行训练</span><br><span class="line">clf = clf.fit(iris.data, iris.target)</span><br><span class="line"># 输出tree.dot</span><br><span class="line">tree.export_graphviz(clf, out_file=&apos;tree.dot&apos;)</span><br></pre></td></tr></table></figure><p>生成的决策树如下：</p><p><img src="/2018/04/22/决策树/tree.png" alt="re"></p><p>回归树生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">rng = np.random.RandomState(1)</span><br><span class="line"># rng.rand(80, 1)生成一个80行1列的随机数，范围为0到1</span><br><span class="line">X = np.sort(5 * rng.rand(80, 1), axis=0)</span><br><span class="line"># 生成y并展开</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line"># 以5为步长进行切片，这些位置的数为原来的数字加3*（0.5-随机数）</span><br><span class="line">y[::5] += 3 * (0.5 - rng.rand(16))</span><br><span class="line"># 生成模型判别</span><br><span class="line"># 这里regr_1深度为2，regr_2深度为5</span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=2)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line"># 测试数据，从0到5生成500个</span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line"># 对x进行预测</span><br><span class="line"># 深度为2的树</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line"># 深度为5的树</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line"># Plot the results</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=20, edgecolor=&quot;black&quot;,</span><br><span class="line">c=&quot;darkorange&quot;, label=&quot;data&quot;)</span><br><span class="line">plt.plot(X_test, y_1, color=&quot;cornflowerblue&quot;,</span><br><span class="line">label=&quot;max_depth=2&quot;, linewidth=2)</span><br><span class="line">plt.plot(X_test, y_2, color=&quot;yellowgreen&quot;, label=&quot;max_depth=5&quot;, linewidth=2)</span><br><span class="line">plt.xlabel(&quot;data&quot;)</span><br><span class="line">plt.ylabel(&quot;target&quot;)</span><br><span class="line">plt.title(&quot;Decision Tree Regression&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/22/决策树/决策树.JPG" alt="策"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录决策树的相关知识和两个例子。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LR算法</title>
    <link href="https://frankindf.github.io/2018/04/22/LR%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/22/LR算法/</id>
    <published>2018-04-22T12:36:45.000Z</published>
    <updated>2018-04-22T15:26:14.301Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要分析逻辑回归的原理和自己的一点思考。</p><a id="more"></a><p>算法原理：</p><p>选取sigmoid函数作为logistic的概率分布函数<br>$$<br>P(Y=1|x) = \frac{\mathrm{exp}(wx+b)}{1+\mathrm{exp}(wx+b)}<br>$$</p><p>$$<br>P(Y=0|x) = \frac{1}{1+\mathrm{exp}(wx+b)}<br>$$</p><p>用线性模型$$wx+b$$逼近对数几率$$log \frac{y}{1-y}$$即<br>$$<br>log\frac{P(Y=1|x)}{P(Y=0|x)} = wx+b<br>$$<br>对于0-1二分类，概率为p和1-p,符合伯努利分布，参数的似然函数为：<br>$$<br>L(W)=P(D|p_{yi})=\prod p^{yi}(1-p)^{1-yi}<br>$$<br>取对数<br>$$<br>logL = \sum  [ y_ilogp + (1-y_i)log(1-p) ]<br>$$</p><p>$$<br>logL = \sum [ y_ilog \frac{p}{1-p} +log(1-p) ]<br>$$</p><p>$$<br>logL = \sum [ y_i(w_ix+b) +\mathrm log(1+e^{w_ix+b)}<br>]<br>$$</p><p>采用梯度下降法和牛顿法可以求解，下面介绍梯度下降法。</p><p>对w求偏导,其中：<br>$$<br>\frac{\partial logL}{\partial w_i} = \sum x_i(yi-p_i)<br>$$<br>得到梯度后就可以迭代下个w<br>$$<br>w_{new}  =  w_{old} + \alpha \frac{\partial logL}{\partial w_i}<br>$$<br>也可以从损失函数的角度理解，$$y_i=1$$和$$y_i = 0$$时对数损失函数$$log(p(y|x))$$如下（将yi为0和1带入对数内部）：</p><p>$$<br>cost  ( h  _ \theta (x) , y )  =  \left{\begin {matrix} -y_ilog(h_ \theta(x)) ! \: \: \: \:  if\:  y = 1\ -(1-y_i)log(1-h_ \theta(x)) ! \: \: \: \:  if\:  y = 0\end{matrix}\right.<br>$$</p><p>联合起来，用一个式子表示：<br>$$<br>cost(h_ \theta(x),y ) = -y_ilog(h_ \theta(x)) -(1-y_i)log(1-h_ \theta(x))<br>$$</p><p>$$<br>cost(h_ \theta(x),y ) =  -\frac{1}{2}\sum (y_ilog(h_ \theta(x)) +(1-y_i)log(1-h_ \theta(x)))<br>$$</p><p>接下来计算和之前一样。</p><p><strong>LR损失函数的形式，其实就是交叉熵。</strong></p><p><strong>损失函数：</strong><br>LR使用对数损失对于sigmoid函数损失函数也可以表示成：<br>$$<br>L(y_i(wx+b)) = log_2(1+e^{y_i(wx+b)})<br>$$<br>损失函数的图像如下：</p><p><img src="/2018/04/22/LR算法/损失函数曲线.png" alt="失函数曲"></p><p><strong>损失函数的意义:</strong></p><p>损失函数的值总大于0-1分段函数，这保证了求解的准确性，使损失函数向正方向变化。可以从损失函数看到，即使预测值与实际值完全一样也就是横坐标大于0，损失函数还是不为0，说明有一个正方向的梯度，这就使LR即使全部预测值分类正确，但还是要优化到分界曲线的位置，如果有异常值，也会将其考虑进去，这点和SVM不同。</p><p><strong>优点</strong>:速度快</p><p><strong>一点思考：</strong><br>sigmoid函数图像接近分段函数，但是这个函数有个缺点，在两端导数很小，所以用梯度下降法时，偏离正确值时可能出现迭代太慢，但是对于sigmoid函数y’ = y(1-y)，对损失函数求导后与sigmoid的导数无关，函数的特性使其方便计算，如果用的是欧式距离，求导会出现y’，对迭代求解不利。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要分析逻辑回归的原理和自己的一点思考。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="https://frankindf.github.io/2018/04/21/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://frankindf.github.io/2018/04/21/SVM支持向量机/</id>
    <published>2018-04-21T10:46:53.000Z</published>
    <updated>2018-04-22T10:51:03.359Z</updated>
    
    <content type="html"><![CDATA[<p>SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。</p><a id="more"></a><p>SVM分类使用sigmoid函数判断结果。</p><p><strong>几何角度的解释</strong>：不同分类之边界之间间隔最大化<br>$$<br> {  min _ { w , b } } \frac{1}{2}\left | w \right |^2<br>$$</p><p>$$<br>s . t .  y_i(w^Tx_i+b)\geqslant 1 , i=1,2…,m.<br>$$</p><p><strong>损失函数最小解释</strong>：$1/2||w||^2$为结构化风险的L2正则项，损失函数为合页函数$l_{0/1}$为$max(0,f(x))$，<br>$$<br>{\min_{w,b}}\frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}(y_i(w^Tx_i+b)-1)<br>$$</p><p>$$<br>s.t     (y_i(w^Tx_i+b)-1)\leqslant 1-\xi _i<br>$$</p><p>$$<br>\xi _ i \geqslant 0<br>$$</p><p>采用拉格朗日法：<br>$$<br>L(w,b,\xi,\alpha,\mu) = \frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}+\sum_{m}^{i=1}\alpha_i[y_i(w^Tx_i+b-1+l_{0/1})]-\sum_{m}^{i=1}{\mu_i\xi_i}<br>$$<br>依次对$w,b,\xi_i$求偏导，得到<br>$$<br>\min_{w,b,\xi}L = \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$<br>原始问题满足KKT条件，对偶问题变为：<br>$$<br>\max \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$</p><p>$$<br>s.t.\space\sum_{j=1}^{N}a_iy_i=0<br>$$</p><p>$$<br>0 \leqslant a_iy_i \leqslant C<br>$$</p><p><strong>惩罚因子</strong>C: C代表对损失函数的惩罚程度，C越大损失函数对优化影响越大，损失函数取的系数C正无穷时，不允许有异常点，即为硬间隔。</p><p><strong>损失函数</strong>： $l_{0/1}$理解为$y_i(w^Tx_i+b)-1&gt;0$时无损失，无需继续优化，这里可以看到$l_{0/1}$是“没有追求”的，只要预测正确就不再努力使预测值向正方向增加，这也使SVM算法可以对异常值有一定“容忍”。</p><p><strong>核函数：</strong>把低维度的数据映射到高维度，用空间变换找数据的分割面，找到分割面后再转换到原空间，即可画出分界边界，例如$x^2+y^2+b$ 在新空间$\alpha+\beta+b$为直线，在原空间线性不可分的点在新空间里线性可分。</p><p>RBF核$K(x,x_i)=exp(\frac{||x-x_i||^2}{σ^2})$会将原始空间映射为无穷维空间,$σ$ 选得大，高次特征上的权衰减变快， <em>σ</em> 选得小，可以将任意的数据映射为线性可分，可能过拟合。<strong>高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。</strong></p><p><strong>SMO算法：</strong>简单理解SMO就是选定$a_i,a_j$将其他项看做常数，可以用$a_i$表示$a_j$,再在[0,C]这个区间求解极值，依次循环。</p><p><strong>注意</strong></p><p>SVM没有处理缺失值的策略，使用SVM前需要将数据进行处理</p><p>例1：SVM示例</p><p>绘制SVM分类示意图，用clf.coef_和clf.intercept获得w和b，用clf.support_vectors_获得支持向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import svm</span><br><span class="line">import pylab as pl</span><br><span class="line">np.random.seed(0) # 使用相同的seed()值，则每次生成的随即数都相同</span><br><span class="line"># 创建可线性分类的数据集与结果集</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20,2) + [2, 2]]</span><br><span class="line">Y = [0] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line"># 构造 SVM 模型</span><br><span class="line">clf = svm.SVC(kernel=&apos;linear&apos;)</span><br><span class="line">clf.fit(X, Y) # 训练 </span><br><span class="line">#利用clf.coef_得到系数w</span><br><span class="line">#clf.intercept_[0]是b</span><br><span class="line">xx = np.linspace(-5, 5) # 在区间[-5, 5] 中产生连续的值，用于画线</span><br><span class="line">yy =  -w[0] * xx / w[1]  - (clf.intercept_[0]) / w[1]</span><br><span class="line">b = clf.support_vectors_[0] # 第一个分类的支持向量，通过调整系数b</span><br><span class="line">yy_down = a * xx + (b[1] - a * b[0])</span><br><span class="line">b = clf.support_vectors_[-1] # 第二个分类中的支持向量</span><br><span class="line">yy_up = a * xx + (b[1] - a * b[0])</span><br><span class="line"></span><br><span class="line">pl.plot(xx, yy, &apos;k-&apos;)</span><br><span class="line">pl.plot(xx, yy_down, &apos;k--&apos;)</span><br><span class="line">pl.plot(xx, yy_up, &apos;k--&apos;)</span><br><span class="line">pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],</span><br><span class="line">           s=80, facecolors=&apos;none&apos;)</span><br><span class="line">pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)</span><br><span class="line"></span><br><span class="line">pl.axis(&apos;tight&apos;)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/21/SVM支持向量机/blog\source\_posts\SVM支持向量机\下载.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow中的softmax</title>
    <link href="https://frankindf.github.io/2018/04/20/tensorflow%E4%B8%AD%E7%9A%84softmax/"/>
    <id>https://frankindf.github.io/2018/04/20/tensorflow中的softmax/</id>
    <published>2018-04-20T13:58:53.000Z</published>
    <updated>2018-04-21T15:14:04.683Z</updated>
    
    <content type="html"><![CDATA[<p>softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。</p><a id="more"></a><p>神经网络中需要将正向传播的结果和的正确结果进行进行对比，softmax函数定义如下，它将分类结果映射到[0,1]这个区间,Vi、Vj表示V中第i，j个元素，ai可以看成第i个分类结果：<br>$$<br>a_i =\frac{e^{V _i}}{\sum_je^{V_j} }<br>$$<br>交叉熵C如下，其中yi代表真实值，ai在这里为softmax：<br>$$<br>C = -\sum_i{y_i{log(a_i)}}<br>$$<br>计算两者之间的差距,每项的Loss可以用下式表示，对于<strong>只有1个正确分类i的分类</strong>,softmax交叉熵计算公式如下：<br>$$<br>L_i = -log(\frac{e^{f _{y_i}}}{\sum_je^{f_{y_j}} }  )<br>$$<br>可以看到，括号里即为softmax的值，它越大，样本的Loss就越小，即与真实分布的差距越小。</p><p>在TensorFlow中交叉熵有下面几种计算方法：</p><ul><li><p>tf.nn.softmax_cross_entropy_with_logits（label, logits）</p><p>logits的shape=(m,n)，label的shape=(m,n),，如果真实分类logits为一维数组，则需要进行one-hot编码。</p></li></ul><ul><li>tf.nn.sparse_softmax_cross_entropy_with_logits（label, logits）</li><li>logits的shape=(m,n)，label的shape=(m,1),若label的shape=(m,n)阶则需要使用argmax函数变为(m,1)</li><li>tf.nn.sigmoid_cross_entropy_with_logits：张量中标量与标量间的运算,求两点分布 之间的交叉熵。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面举例进行交叉熵计算</span></span><br><span class="line"><span class="comment">#分别用softmax_cross_entropy_with_logits、tf.nn.sparse_softmax_cross_entropy_with_logits</span></span><br><span class="line"><span class="comment">#手算和tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment">#label代表真实分布</span></span><br><span class="line"><span class="comment">#采用one-hot编码，即真实分类分别为[3,2,1,1,2]</span></span><br><span class="line">labels = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=np.float32)</span><br><span class="line"><span class="comment">#logits代表前向传播的结果</span></span><br><span class="line"><span class="comment">#代表每个值得权重</span></span><br><span class="line">logits = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>],</span><br><span class="line">                   [<span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                   [<span class="number">8</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>]], dtype=np.float32)</span><br><span class="line">num_classes = labels.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#tf.nn.softmax用来求softmax值，即映射到[0,1]区间上的概率</span></span><br><span class="line">predicts = tf.nn.softmax(logits=logits, dim=<span class="number">-1</span>)</span><br><span class="line">sess.run(predicts)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out:array([[  2.45611509e-03,   6.67641265e-03,   9.90867496e-01],</span><br><span class="line">       [  1.14195190e-01,   8.43794703e-01,   4.20100652e-02],</span><br><span class="line">       [  9.46499169e-01,   6.37746137e-03,   4.71234173e-02],</span><br><span class="line">       [  9.97193694e-01,   2.47179624e-03,   3.34521203e-04],</span><br><span class="line">       [  4.71234173e-02,   9.46499169e-01,   6.37746137e-03]], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.softmax_cross_entropy_with_logits计算交叉熵，labels可以直接用one-hot编码的数组</span></span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)</span><br><span class="line">sess.run(cross_entropy)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.sparse_softmax_cross_entropy_with_logits计算交叉熵，labels要处理为(m,1)维</span></span><br><span class="line">classes = tf.argmax(labels, axis=<span class="number">1</span>)</span><br><span class="line">sess.run(classes)</span><br><span class="line">cross_entropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=classes)</span><br><span class="line">sess.run(cross_entropy2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用reduce_sum计算，其中</span></span><br><span class="line"><span class="comment">#用clip_by_value将取值限制在1e-10以上，防止出log(0)</span></span><br><span class="line"><span class="comment">#用labels/predicts相当于前面加负号</span></span><br><span class="line">labels = tf.clip_by_value(labels, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">predicts = tf.clip_by_value(predicts, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">cross_entropy4 = tf.reduce_sum(labels * tf.log(labels/predicts), axis=<span class="number">1</span>)</span><br><span class="line">sess.run(cross_entropy4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0.8</span></span><br><span class="line">x = <span class="number">1.3</span></span><br><span class="line">cross_entropy3 = tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)</span><br><span class="line"><span class="comment"># tf.nn.sigmoid_cross_entropy_with_logits的具体实现:</span></span><br><span class="line">cross_entropy5 = - z * tf.log(tf.nn.sigmoid(x))  - (<span class="number">1</span>-z) * tf.log(<span class="number">1</span>-tf.nn.sigmoid(x))</span><br><span class="line">sess.run(cross_entropy3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.50100845</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorboard应用</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorboard%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorboard用法/</id>
    <published>2018-04-01T09:28:48.000Z</published>
    <updated>2018-04-22T12:30:36.516Z</updated>
    
    <content type="html"><![CDATA[<p>通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。</p><a id="more"></a><p>标量数据汇总和记录使用</p><p>tf.summary.scalar(tags, values, collections=None, name=None)  </p><p>直接记录变量var的直方图</p><p>tf.summary.histogram(tag, values, collections=None, name=None）  </p><p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ <em>tag</em> /image/0’, ‘ <em>tag</em> /image/1’, etc.，如：input/image/0等</p><p>tf.summary.image(tag, tensor, max_images=3, collections=None, name=None)  </p><p>汇总再进行一次合并</p><p>tf.summary.merge(inputs, collections=None, name=None)</p><p>合并默认图形中的所有汇总</p><p>tf.summaries.merge_all(key=’summaries’)  </p><p>下面看一个TensorBoard的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">通过</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#输入数据，这里用的是mnist数据集</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">#学习率</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">#训练集的训练次数</span><br><span class="line">training_epochs = 25</span><br><span class="line">#每次输入的数据个数</span><br><span class="line">batch_size = 100</span><br><span class="line">display_epoch = 1</span><br><span class="line">#数据存储位置</span><br><span class="line">logs_path = &apos;/tmp/tensorflow_logs/example/&apos;</span><br><span class="line"># mnist数据集是28*28的图片，一共784个像素</span><br><span class="line">#训练集x</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784], name=&apos;InputData&apos;)</span><br><span class="line">#真实值y，对数字分类有10个类别</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10], name=&apos;LabelData&apos;)</span><br><span class="line"></span><br><span class="line">#定义神经网络参数y=wx+b中的x和b</span><br><span class="line">W = tf.Variable(tf.zeros([784, 10]), name=&apos;Weights&apos;)</span><br><span class="line">b = tf.Variable(tf.zeros([10]), name=&apos;Bias&apos;)</span><br><span class="line"></span><br><span class="line"># 在命名空间内定义模型、损失、优化方法</span><br><span class="line">with tf.name_scope(&apos;Model&apos;):</span><br><span class="line">    # 训练模型，采用全连接神经网络</span><br><span class="line">    #matmul进行矩阵相乘</span><br><span class="line">    #softmax得到输出结果</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W) + b) </span><br><span class="line">with tf.name_scope(&apos;Loss&apos;):</span><br><span class="line">#损失函数使用交叉熵，这里也可以用TF自带的函数</span><br><span class="line">    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span><br><span class="line">with tf.name_scope(&apos;SGD&apos;):</span><br><span class="line">    #用梯度下降法对损失函数进行优化</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line">with tf.name_scope(&apos;Accuracy&apos;):</span><br><span class="line">    # 计算准确率，这里argmax的参数就对应分类</span><br><span class="line">    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"># 计算之前需要将全局变量初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"># 添加变量到TensorBoard</span><br><span class="line"># 这里添加了损失、准确率，在输出的文件中可以看到这些参数的变化情况</span><br><span class="line">tf.summary.scalar(&quot;loss&quot;, cost)</span><br><span class="line">tf.summary.scalar(&quot;accuracy&quot;, acc)</span><br><span class="line"># Merge all summaries into a single op</span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">    # 初始化数据</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # 初始化TB，路径为logs_path</span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    # 开始训练过程</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        #所有数据训练一次要分成total_batch个数据集</span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # 训练数据batch_xs,batch_ys</span><br><span class="line">            _, c, summary = sess.run([optimizer, cost, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            # 将数据写入之前初始化的TB中</span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            #计算训练一次的平均损失函数</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # 显示每个Epoch的cost</span><br><span class="line">        if (epoch+1) % display_epoch == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &apos;%04d&apos; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # 测试集上显示准确率</span><br><span class="line">    print(&quot;Accuracy:&quot;, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    print(&quot;Run the command line:\n&quot; \</span><br><span class="line">          &quot;--&gt; tensorboard --logdir=/tmp/tensorflow_logs &quot; \</span><br><span class="line">          &quot;\nThen open http://0.0.0.0:6006/ into your web browser&quot;)</span><br></pre></td></tr></table></figure><p>运行完程序后，输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[out]：Epoch: 0001 cost= 1.183585649</span><br><span class="line">Epoch: 0002 cost= 0.665355021</span><br><span class="line">Epoch: 0003 cost= 0.552772734</span><br><span class="line">Epoch: 0004 cost= 0.498669290</span><br><span class="line">Epoch: 0005 cost= 0.465467163</span><br><span class="line">Epoch: 0006 cost= 0.442601420</span><br><span class="line">Epoch: 0007 cost= 0.425460528</span><br><span class="line">Epoch: 0008 cost= 0.412206892</span><br><span class="line">Epoch: 0009 cost= 0.401397175</span><br><span class="line">Epoch: 0010 cost= 0.392420013</span><br><span class="line">Epoch: 0011 cost= 0.384768393</span><br><span class="line">Epoch: 0012 cost= 0.378172010</span><br><span class="line">Epoch: 0013 cost= 0.372432202</span><br><span class="line">Epoch: 0014 cost= 0.367334918</span><br><span class="line">Epoch: 0015 cost= 0.362694857</span><br><span class="line">Epoch: 0016 cost= 0.358602089</span><br><span class="line">Epoch: 0017 cost= 0.354879144</span><br><span class="line">Epoch: 0018 cost= 0.351492124</span><br><span class="line">Epoch: 0019 cost= 0.348284597</span><br><span class="line">Epoch: 0020 cost= 0.345425291</span><br><span class="line">Epoch: 0021 cost= 0.342768804</span><br><span class="line">Epoch: 0022 cost= 0.340257174</span><br><span class="line">Epoch: 0023 cost= 0.337940815</span><br><span class="line">Epoch: 0024 cost= 0.335757064</span><br><span class="line">Epoch: 0025 cost= 0.333699012</span><br><span class="line">Optimization Finished!</span><br><span class="line">Accuracy: 0.9135</span><br></pre></td></tr></table></figure><p>运行tensorboard –logdir=/tmp/tensorflow_logs，打开浏览器进入<a href="http://localhost:6006查看结果。" target="_blank" rel="noopener">http://localhost:6006查看结果。</a></p><p>​    <img src="/2018/04/01/tensorboard用法/1.png" alt="1"></p><p>‘    <img src="/2018/04/01/tensorboard用法/2.png" alt="2"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow卷积函数</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorflow%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorflow卷积过程/</id>
    <published>2018-04-01T08:36:34.000Z</published>
    <updated>2018-04-22T10:47:18.279Z</updated>
    
    <content type="html"><![CDATA[<p>卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。</p><a id="more"></a><p>tensorflow卷积函数：</p><p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</p><p>input为输入的数组，假设维度为[batch,矩阵高x,矩阵宽y,矩阵深度z]</p><p>filterw为卷积核，维度[矩阵高x,矩阵宽y,矩阵深度z, out_channels]</p><p>strides：卷积时在图像每一维的步长</p><p>padding：是否补全空白”SAME”或”VALID”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#定义全1的矩阵，维数[3,3,1]</span><br><span class="line">x1 = tf.constant(1.0, shape=[1,6,6,2])  </span><br><span class="line">#定义核函数，维度[1,1,1]</span><br><span class="line">#第一通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#第二通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">kernel = tf.constant(1.0, shape=[3,2,2,1])   </span><br><span class="line">y2 = tf.nn.conv2d(x1, kernel,strides=[1,1,1,1],padding=&apos;VALID&apos;)  </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run(session=sess)</span><br><span class="line">    #显示y2</span><br><span class="line">    print(sess.run(tf.shape(y2))</span><br><span class="line">#行6-3+1，列6-2+1</span><br><span class="line">OUT:[1 4 5 1]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>adaboost算法</title>
    <link href="https://frankindf.github.io/2018/03/29/adaboost%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/29/adaboost算法/</id>
    <published>2018-03-29T14:28:37.000Z</published>
    <updated>2018-04-21T16:17:56.985Z</updated>
    
    <content type="html"><![CDATA[<p>​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。</p><a id="more"></a><p>该算法首先定义辅助函数stumpClassify，输入数据，分界值后输出分类的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def stumpClassify(dataIn,dimen,threshVal,threshIneq):</span><br><span class="line">    #输入数据、采用哪列特征分类、分界的限值，是大于还是小于</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    retArray=np.ones((m,1))</span><br><span class="line">    if threshIneq==&apos;lt&apos;:</span><br><span class="line">    #该分类是小于，则大于分界值得数据预测错误，定义为-1</span><br><span class="line">        retArray[dataIn[:,dimen]&lt;=threshVal]=-1.0</span><br><span class="line">    else:</span><br><span class="line">    #该分类是大于，则小于分界值得数据预测错误</span><br><span class="line">        retArray[dataIn[:,dimen]&gt;threshVal]=-1.0</span><br><span class="line">    return retArray</span><br></pre></td></tr></table></figure><p>函数使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#定义输入数据</span><br><span class="line">dataIn=np.arange((12)).reshape(6,2)</span><br><span class="line">Out[43]: </span><br><span class="line">array([[ 0,  1],</span><br><span class="line">       [ 2,  3],</span><br><span class="line">       [ 4,  5],</span><br><span class="line">       [ 6,  7],</span><br><span class="line">       [ 8,  9],</span><br><span class="line">       [10, 11]])</span><br><span class="line">#以小于2的值作为第2列的分界点</span><br><span class="line">stumpClassify(dataIn,1,2,&apos;lt&apos;)</span><br><span class="line">Out[47]: </span><br><span class="line">array([[-1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.]])</span><br></pre></td></tr></table></figure><p>​          有了分类函数，接着需要定义buildStump选取当前情况下，最佳的分类点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">def buildStump(dataIn,classLabel,D):</span><br><span class="line">#D为输入的分类权重，后面会用到</span><br><span class="line">    #输入数据，正确分类，迭代系数</span><br><span class="line">    n=dataIn.shape[1]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    numStep=10.0</span><br><span class="line">    bestStump=&#123;&#125;</span><br><span class="line">    bestClass=np.zeros((m,1))</span><br><span class="line">    minError=np.inf</span><br><span class="line">    for i in range(0,n):</span><br><span class="line">        iAxis=dataIn[:,i]</span><br><span class="line">        minAxis=min(iAxis)</span><br><span class="line">        maxAxis=max(iAxis)</span><br><span class="line">        stepSize=(maxAxis-minAxis)/numStep</span><br><span class="line">    #考虑大于和小于两种分类情况</span><br><span class="line">        for ineq in [&apos;lt&apos;,&apos;gt&apos;]:</span><br><span class="line">            for j in range(-1,int(numStep)+1):</span><br><span class="line">            #每次计算错误值</span><br><span class="line">                threshVal=minAxis+stepSize*float(j)</span><br><span class="line">                #从threshVal值处对i特征值对应数据进行分类</span><br><span class="line">                predictCategory=stumpClassify(dataIn,i,threshVal,ineq)</span><br><span class="line">                #初始化误差值矩阵</span><br><span class="line">                errArr=np.ones((m,1))</span><br><span class="line">                #预测正确的点误差矩阵值为0，其他点为1</span><br><span class="line">                errArr[predictCategory==classLabel]=0</span><br><span class="line">                #误差矩阵乘以权重得到该threshVal的分类总误差，</span><br><span class="line">                errSum=np.dot(D.T,errArr)</span><br><span class="line">                #更新误差最小的threshVal</span><br><span class="line">                if errSum&lt;minError:</span><br><span class="line">                    minError=errSum</span><br><span class="line">                    bestClass=predictCategory.copy()</span><br><span class="line">                    bestStump[&apos;dim&apos;]=i</span><br><span class="line">                    bestStump[&apos;threshVal&apos;]=threshVal</span><br><span class="line">                    bestStump[&apos;ineq&apos;]=ineq</span><br><span class="line">    print(&apos;j&apos;,j,&apos;ineq&apos;,ineq,&apos;split&apos;,i,&apos;\nthresh&apos;,threshVal,&apos;\nerrorsum&apos;,errSum)</span><br><span class="line">    return bestStump,minError,bestClass</span><br><span class="line">    </span><br><span class="line">   classLabel=np.array(([[1],[-1],[-1],[1],[1],[1]]))    </span><br><span class="line">   buildStump(dataIn,classLabel,D)</span><br><span class="line">Out[56]: </span><br><span class="line">#得到当前权重下最佳分类</span><br><span class="line">#分类的特征为0列对应的特征，分类值为4，错误率0.167，分类结果[-1,-1,-1,1,1,1]</span><br><span class="line">(&#123;&apos;dim&apos;: 0, &apos;ineq&apos;: &apos;lt&apos;, &apos;threshVal&apos;: 4.0&#125;,</span><br><span class="line"> array([[ 0.16666667]]),</span><br><span class="line"> array([[-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.]]))</span><br></pre></td></tr></table></figure><p>​        得到了一列一种情况下的分类方法，就相当于有了一个分类器，adaboost的核心是将多个分类器的结果按照权重相加，得到最后的结果。下面通过addBoostTrainDS来训练得到各个分类器的权重。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def addBoostTrainDS(dataIn,classLabels,numIt=40):</span><br><span class="line">    weakClassArr=[]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">#初始化D为1/m，m为数据个数</span><br><span class="line">    D=np.ones((m,1))/m</span><br><span class="line">    aggBestClass=np.zeros((m,1))</span><br><span class="line">    for i in range(numIt):</span><br><span class="line">     #得到每次更新权重值后的最佳分类</span><br><span class="line">        bestStump,error,bestClass=buildStump(dataIn,classLabel,D)</span><br><span class="line">        #求出误差率alpha</span><br><span class="line">        alpha=0.5*np.log((1-error)/max(error,1e-12))</span><br><span class="line">        bestStump[&apos;alpha&apos;]=alpha</span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line">        #每一项的权重</span><br><span class="line">        expon=-1*alpha*(classLabels.T*bestClass)</span><br><span class="line">        print(&apos;expon&apos;,expon)</span><br><span class="line">        D=D*np.exp(expon)</span><br><span class="line">        D=D/D.sum()#更新D值</span><br><span class="line">        aggBestClass+=alpha*bestClass</span><br><span class="line">        #类别为1和-1</span><br><span class="line">        aggErrors=np.ones((m,1))</span><br><span class="line">        samePred=np.where(np.sign(aggBestClass)==classLabel)</span><br><span class="line">        difPred=np.where(np.sign(aggBestClass)!=classLabel)</span><br><span class="line">        aggErrors[samePred]=0</span><br><span class="line">        aggErrors[difPred]=1</span><br><span class="line">        print(&apos;D&apos;,D,&apos;best&apos;,bestClass,&apos;\naggBestClass&apos;,aggBestClass  </span><br><span class="line">        errorRate=aggErrors.sum()/m</span><br><span class="line">        print(&apos;totalError&apos;,errorRate)</span><br><span class="line">        if errorRate==0: break</span><br><span class="line">    return weakClassArr,aggErrors</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://frankindf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>numpy的argsort函数用法</title>
    <link href="https://frankindf.github.io/2018/03/28/numpy%E7%9A%84argsort%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/28/numpy的argsort函数用法/</id>
    <published>2018-03-28T14:17:28.000Z</published>
    <updated>2018-04-21T15:12:39.022Z</updated>
    
    <content type="html"><![CDATA[<p>数组操作时有时如果需要获取数组的索引可以使用argsort函数。</p><a id="more"></a><p>numpy中的argsort可以返回一个索引，具体参数如下</p><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)</p><p>a数组</p><p>axis行或者列，默认为-1，即最后一个维度，0为列，1为行</p><p>kind排序方式{‘quicksort’, ‘mergesort’, ‘heapsort’}</p><p>order</p><p>返回的是排序后的数组在原数组中索引的数组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.array(([[1,6,3,4,2]]))</span><br><span class="line">a.argsort()</span><br><span class="line">#返回的是ARRAY从小到大的索引</span><br><span class="line">Out[35]: array([[0, 4, 2, 3, 1]], dtype=int64)</span><br><span class="line">#默认为行排序的索引</span><br><span class="line">np.argsort(a)</span><br><span class="line">Out[41]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br><span class="line">#axis为1按列排序</span><br><span class="line">np.argsort(a,axis=0)</span><br><span class="line">Out[39]: </span><br><span class="line">array([[0, 1, 0, 0],</span><br><span class="line">       [1, 0, 1, 1]], dtype=int64)</span><br><span class="line">#axis为1，按行排序</span><br><span class="line">np.argsort(a,axis=1)</span><br><span class="line">Out[40]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数组操作时有时如果需要获取数组的索引可以使用argsort函数。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
