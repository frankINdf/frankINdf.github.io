<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>go go go</title>
  
  <subtitle>fire in the hole</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://frankindf.github.io/"/>
  <updated>2018-03-28T13:27:35.782Z</updated>
  <id>https://frankindf.github.io/</id>
  
  <author>
    <name>frank</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>numpy的argsort函数用法</title>
    <link href="https://frankindf.github.io/2018/03/28/numpy%E7%9A%84argsort%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95-1/"/>
    <id>https://frankindf.github.io/2018/03/28/numpy的argsort函数用法-1/</id>
    <published>2018-03-28T13:27:35.000Z</published>
    <updated>2018-03-28T13:27:35.782Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>“</title>
    <link href="https://frankindf.github.io/2018/03/28/%E2%80%9C/"/>
    <id>https://frankindf.github.io/2018/03/28/“/</id>
    <published>2018-03-28T13:27:04.000Z</published>
    <updated>2018-03-28T13:27:04.703Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>“博客名”</title>
    <link href="https://frankindf.github.io/2018/03/28/%E2%80%9C%E5%8D%9A%E5%AE%A2%E5%90%8D%E2%80%9D/"/>
    <id>https://frankindf.github.io/2018/03/28/“博客名”/</id>
    <published>2018-03-28T13:27:01.000Z</published>
    <updated>2018-03-28T13:27:01.138Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://frankindf.github.io/2018/03/28/numpy%E7%9A%84argsort%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/28/numpy的argsort函数用法/</id>
    <published>2018-03-28T13:25:13.569Z</published>
    <updated>2018-03-28T13:25:19.646Z</updated>
    
    <content type="html"><![CDATA[<h1 id="numpy的argsort函数用法"><a href="#numpy的argsort函数用法" class="headerlink" title="numpy的argsort函数用法"></a>numpy的argsort函数用法</h1><p>numpy中的argsort可以返回一个索引</p><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)</p><p>a数组</p><p>axis行或者列，默认为-1，即最后一个维度，0为列，1为行</p><p>kind排序方式{‘quicksort’, ‘mergesort’, ‘heapsort’}</p><p>order</p><p>f返回</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.array(([[1,6,3,4,2]]))</span><br><span class="line">a.argsort()</span><br><span class="line">#返回的是ARRAY从小到大的索引</span><br><span class="line">Out[35]: array([[0, 4, 2, 3, 1]], dtype=int64)</span><br><span class="line">#默认为行排序的索引</span><br><span class="line">np.argsort(a)</span><br><span class="line">Out[41]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br><span class="line">#axis为1按列排序</span><br><span class="line">np.argsort(a,axis=0)</span><br><span class="line">Out[39]: </span><br><span class="line">array([[0, 1, 0, 0],</span><br><span class="line">       [1, 0, 1, 1]], dtype=int64)</span><br><span class="line">#axis为1，按行排序</span><br><span class="line">np.argsort(a,axis=1)</span><br><span class="line">Out[40]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br></pre></td></tr></table></figure><p>## </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;numpy的argsort函数用法&quot;&gt;&lt;a href=&quot;#numpy的argsort函数用法&quot; class=&quot;headerlink&quot; title=&quot;numpy的argsort函数用法&quot;&gt;&lt;/a&gt;numpy的argsort函数用法&lt;/h1&gt;&lt;p&gt;numpy中的arg
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>svm</title>
    <link href="https://frankindf.github.io/2018/02/03/svm/"/>
    <id>https://frankindf.github.io/2018/02/03/svm/</id>
    <published>2018-02-03T15:00:00.000Z</published>
    <updated>2018-02-05T14:44:21.140Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、读取数据"><a href="#一、读取数据" class="headerlink" title="一、读取数据"></a>一、读取数据</h1><p>1.读取数据为矩阵<br>2.画散点图scatter（）</p><h1 id="二、计算过程"><a href="#二、计算过程" class="headerlink" title="二、计算过程"></a>二、计算过程</h1><p>1.随机生成n个seed<br>2.求出所有点到每个seed的距离<br>3.将该点标记为最近seed所属的类<br>4.计算该类的中心点<br>5.将该中心点作为新的seed<br>6.重复2-5的过程</p><h1 id="三、计算距离"><a href="#三、计算距离" class="headerlink" title="三、计算距离"></a>三、计算距离</h1><p>计算距离<br>对数据进行标记</p><h1 id="四、K值确定"><a href="#四、K值确定" class="headerlink" title="四、K值确定"></a>四、K值确定</h1><p>法1：(轮廓系数)<br>法2：(Calinski-Harabasz准则)</p><h1 id="五、初始点选择"><a href="#五、初始点选择" class="headerlink" title="五、初始点选择"></a>五、初始点选择</h1><p>法1(kmeans++):<br>法2：选用层次聚类或Canopy算法进行初始聚类，然后从k个类别中分别随机选取k个点，来作为kmeans的初始聚类中心点<br>‘’’<br>import numpy as np<br>import matplotlib.pyplot as plt</p><p>from sklearn.cluster import KMeans<br>from sklearn.datasets import make_blobs</p><p>#生成散点图<br>centers = [[1, 1], [-1, -1], [1, -1]]<br>X, labels_true = make_blobs(n_samples=250, centers=centers, cluster_std=0.4,<br>                            random_state=0)<br>plt.scatter(X[:,0],X[:,1],c=labels_true)</p><p>#使用k-means进行分类<br>from sklearn.cluster import KMeans<br>kmeans = KMeans(n_clusters=3, random_state=0,c=labels_true)<br>y_pred1 = kmeans.fit_predict(X)</p><p>plt.subplot(2,2,1)<br>plt.scatter(X[:,0],X[:,1],c=y_pred1)</p><p>#调节各个参数<br>plt.subplot(2,2,2)<br>kmeans = KMeans(n_clusters=5, random_state=0).fit(X)<br>y_pred2 = kmeans.fit_predict(X)<br>plt.scatter(X[:,0],X[:,1],c=y_pred2)</p><p>plt.subplot(2,2,3)<br>kmeans = KMeans(n_clusters=3,init=’random’).fit(X)<br>y_pred3 = kmeans.fit_predict(X)<br>plt.scatter(X[:,0],X[:,1],c=y_pred3)</p><p>plt.subplot(2,2,4)<br>kmeans = KMeans(n_clusters=3, random_state=0,max_iter=1)<br>y_pred2 = kmeans.fit_predict(X)<br>plt.scatter(X[:,0],X[:,1],c=y_pred2)</p><p>plt.show()<br>‘’’</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、读取数据&quot;&gt;&lt;a href=&quot;#一、读取数据&quot; class=&quot;headerlink&quot; title=&quot;一、读取数据&quot;&gt;&lt;/a&gt;一、读取数据&lt;/h1&gt;&lt;p&gt;1.读取数据为矩阵&lt;br&gt;2.画散点图scatter（）&lt;/p&gt;
&lt;h1 id=&quot;二、计算过程&quot;&gt;&lt;a hr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>3rd</title>
    <link href="https://frankindf.github.io/2018/02/02/3rd/"/>
    <id>https://frankindf.github.io/2018/02/02/3rd/</id>
    <published>2018-02-02T15:43:59.000Z</published>
    <updated>2018-02-16T05:35:25.452Z</updated>
    
    <content type="html"><![CDATA[<p>线性回归的基本公式<br><img src="/2018/02/02/3rd/基本公式1.jpg"></p><p>求解方法两种<br>1.<br>2.梯度下降法</p><p>numpy.loadtext(fileName,dtype=数据类型,comments=’注释’,delimiter=分隔符,skiprow=跳过前几行,usecols=读取的列,unpack=true分列读取)<br>converters : dict, optional</p><p>A dictionary mapping column number to a function that will convert that column to a float. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt): converters = {3: lambda s: float(s.strip() or 0)}. Default: None.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;线性回归的基本公式&lt;br&gt;&lt;img src=&quot;/2018/02/02/3rd/基本公式1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;求解方法两种&lt;br&gt;1.&lt;br&gt;2.梯度下降法&lt;/p&gt;
&lt;p&gt;numpy.loadtext(fileName,dtype=数据类型,comments=’注释’
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2nd day</title>
    <link href="https://frankindf.github.io/2018/02/02/2nd-day/"/>
    <id>https://frankindf.github.io/2018/02/02/2nd-day/</id>
    <published>2018-02-02T15:35:33.000Z</published>
    <updated>2018-02-02T15:38:02.111Z</updated>
    
    <content type="html"><![CDATA[<p>线性回归属于回归问题。对于回归问题，解决流程为：<br>给定数据集中每个样本及其正确答案，选择一个模型函数h（hypothesis，假设），并为h找到适应数据的（未必是全局）最优解，即找出最优解下的h的参数。这里给定的数据集取名叫训练集（Training Set）。不能所有数据都拿来训练，要留一部分验证模型好不好使，这点以后说。先列举几个几个典型的模型：</p><p>最基本的单变量线性回归：<br>形如h(x)=theta0+theta1*x1</p><p>多变量线性回归：<br>形如h(x)=theta0+theta1<em>x1+theta2</em>x2+theta3*x3</p><p>多项式回归（Polynomial Regression）：<br>形如h(x)=theta0+theta1<em>x1+theta2</em>(x2^2)+theta3<em>(x3^3)<br>或者h(x)=ttheta0+theta1</em>x1+theta2*sqr(x2)<br>但是我们可以令x2=x2^2，x3=x3^3，于是又将其转化为了线性回归模型。虽然不能说多项式回归问题属于线性回归问题，但是一般我们就是这么做的。</p><p>所以最终通用表达式就是：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;线性回归属于回归问题。对于回归问题，解决流程为：&lt;br&gt;给定数据集中每个样本及其正确答案，选择一个模型函数h（hypothesis，假设），并为h找到适应数据的（未必是全局）最优解，即找出最优解下的h的参数。这里给定的数据集取名叫训练集（Training Set）。不能所有
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>hello</title>
    <link href="https://frankindf.github.io/2018/01/31/hello/"/>
    <id>https://frankindf.github.io/2018/01/31/hello/</id>
    <published>2018-01-31T14:12:31.000Z</published>
    <updated>2018-02-02T12:56:07.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h1><p>‘’’<br>from sklearn import datasets<br>from sklearn.model_selection import cross_val_predict<br>from sklearn import linear_model<br>import matplotlib.pyplot as plt</p><a id="more"></a><p>lr = linear_model.LinearRegression() #线性回归模型<br>boston = datasets.load_boston() #读取数据506x13<br>y = boston.target #结果数据</p><h1 id="cross-val-predict-returns-an-array-of-the-same-size-as-y-where-each-entry"><a href="#cross-val-predict-returns-an-array-of-the-same-size-as-y-where-each-entry" class="headerlink" title="cross_val_predict returns an array of the same size as y where each entry"></a>cross_val_predict returns an array of the same size as <code>y</code> where each entry</h1><h1 id="is-a-prediction-obtained-by-cross-validation"><a href="#is-a-prediction-obtained-by-cross-validation" class="headerlink" title="is a prediction obtained by cross validation:"></a>is a prediction obtained by cross validation:</h1><p>predicted = cross_val_predict(lr, boston.data, y, cv=10) #cv=10代表10折</p><p>fig, ax = plt.subplots()<br>ax.scatter(y, predicted, edgecolors=(0, 0, 0))<br>ax.plot([y.min(), y.max()], [y.min(), y.max()], ‘k–’, lw=4)<br>ax.set_xlabel(‘Measured’)<br>ax.set_ylabel(‘Predicted’)<br>plt.show()<br>‘’’</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;线性回归算法&quot;&gt;&lt;a href=&quot;#线性回归算法&quot; class=&quot;headerlink&quot; title=&quot;线性回归算法&quot;&gt;&lt;/a&gt;线性回归算法&lt;/h1&gt;&lt;p&gt;‘’’&lt;br&gt;from sklearn import datasets&lt;br&gt;from sklearn.model_selection import cross_val_predict&lt;br&gt;from sklearn import linear_model&lt;br&gt;import matplotlib.pyplot as plt&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
