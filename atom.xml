<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阳阳的博客</title>
  
  <subtitle>学无止境</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://frankindf.github.io/"/>
  <updated>2018-04-22T15:36:29.089Z</updated>
  <id>https://frankindf.github.io/</id>
  
  <author>
    <name>阳阳</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>决策树</title>
    <link href="https://frankindf.github.io/2018/04/22/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://frankindf.github.io/2018/04/22/决策树/</id>
    <published>2018-04-22T15:27:25.000Z</published>
    <updated>2018-04-22T15:36:29.089Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录决策树的相关知识和两个例子。</p><a id="more"></a><p>决策树划分<br><strong>信息增益</strong><br>ID3<br>$$<br>Gain = Ent(D)-\sum_{v = 1}^{V}\frac{|D^v|}{|D|}Ent(D^v)<br>$$</p><p>$$<br>Ent (D) = \sum_{k=1}^{|y|}p_klog_{2} p_k<br>$$<br>缺点：趋向子节点多的分类<br>无法处理连续特征<br>无法处理缺失值<br>可能过拟合<br><strong>信息增益率</strong><br>C4.5<br>$$<br>Gain_ratio = \frac{Gain(D,a)}{IV(a)}<br>$$</p><p>$$<br>IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}<br>$$<br>缺点：趋向子节点少的</p><p><strong>基尼系数</strong><br>CART<br>$$<br>Gini(D) = \sum_{k=1}^{|y|}a\sum_{k’\neq k}p_kp_{k’}<br>$$<br><strong>CART回归树</strong><br>对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。</p><p><strong>剪枝</strong><br>预剪枝：划分前计算准确率，如果分类后的准确率降低就不进行分类<br>后剪枝：划分后坍塌，计算准确率，如果准确率提升就不分类</p><p><strong>连续值的处理：</strong><br>将连续值划分为N个区间，计算信息增益</p><p><strong>缺失值的处理</strong><br>1）对于含有缺失值的数据，分类的属性需要做修正<br>$$<br>Gain = e\times Gain( \tilde{D},a)<br>$$<br>2）对于含有缺失值的数据，以属性各值所占的概率分配到不同的值当中,概率为$\tilde{\gamma _v}$</p><p><strong>多变量决策树</strong><br>非叶节点为分类器$\sum _{d}^{i=1} w_ia_i=t$</p><p><strong>SKLEARN实现决策树</strong><br>class sklearn.tree.DecisionTreeClassifier(<br>criterion=’gini’,  /分类特征选择标准，可以用基尼系数’gini’或者熵’entropy’<br>splitter=’best’,   /遍历所有特征用’best’,随机选取局部最优解用’random’,’random’适用数据量大时<br>max_depth=None,    /最大深度<br>min_samples_split=2,/多少个特征以下停止划分<br> min_samples_leaf=1, /最小叶子数，子节点少于多少进行剪枝<br>min_weight_fraction_leaf=0.0,/针对缺失值，权重小于多少进行剪枝<br>max_features=None, /最大特征数，’auto’,’sqrt’,’log2’或者数字，寻找最佳分割点时的最大特征数。<br>random_state=None,<br>max_leaf_nodes=None,<br>min_impurity_decrease=0.0,\不纯度减少多少就要进行分类<br>min_impurity_split=None, \最小不纯度，即纯度大时不再生成子树<br>class_weight=None, /样本权重,可选’balanced’进行自动计算权重，样本量少的类权重高<br>presort=False /是否预先排序<br>)</p><p>决策树可视化样例，使用sklearn中的鸢尾花数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn import tree</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier, export_graphviz</span><br><span class="line">import subprocess</span><br><span class="line">import pydot</span><br><span class="line"># 使用dot文件进行可视化</span><br><span class="line"># sklearn.tree下面的export_graphviz可以输出dot文件</span><br><span class="line"># 定义决策树，使用默认参数</span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">iris = load_iris()</span><br><span class="line"># 进行训练</span><br><span class="line">clf = clf.fit(iris.data, iris.target)</span><br><span class="line"># 输出tree.dot</span><br><span class="line">tree.export_graphviz(clf, out_file=&apos;tree.dot&apos;)</span><br></pre></td></tr></table></figure><p>生成的决策树如下：</p><p><img src="/2018/04/22/决策树/tree.png" alt="re"></p><p>回归树生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">rng = np.random.RandomState(1)</span><br><span class="line"># rng.rand(80, 1)生成一个80行1列的随机数，范围为0到1</span><br><span class="line">X = np.sort(5 * rng.rand(80, 1), axis=0)</span><br><span class="line"># 生成y并展开</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line"># 以5为步长进行切片，这些位置的数为原来的数字加3*（0.5-随机数）</span><br><span class="line">y[::5] += 3 * (0.5 - rng.rand(16))</span><br><span class="line"># 生成模型判别</span><br><span class="line"># 这里regr_1深度为2，regr_2深度为5</span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=2)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line"># 测试数据，从0到5生成500个</span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line"># 对x进行预测</span><br><span class="line"># 深度为2的树</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line"># 深度为5的树</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line"># Plot the results</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=20, edgecolor=&quot;black&quot;,</span><br><span class="line">c=&quot;darkorange&quot;, label=&quot;data&quot;)</span><br><span class="line">plt.plot(X_test, y_1, color=&quot;cornflowerblue&quot;,</span><br><span class="line">label=&quot;max_depth=2&quot;, linewidth=2)</span><br><span class="line">plt.plot(X_test, y_2, color=&quot;yellowgreen&quot;, label=&quot;max_depth=5&quot;, linewidth=2)</span><br><span class="line">plt.xlabel(&quot;data&quot;)</span><br><span class="line">plt.ylabel(&quot;target&quot;)</span><br><span class="line">plt.title(&quot;Decision Tree Regression&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/22/决策树/决策树.JPG" alt="策"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录决策树的相关知识和两个例子。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LR算法</title>
    <link href="https://frankindf.github.io/2018/04/22/LR%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/22/LR算法/</id>
    <published>2018-04-22T12:36:45.000Z</published>
    <updated>2018-04-22T15:26:14.301Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要分析逻辑回归的原理和自己的一点思考。</p><a id="more"></a><p>算法原理：</p><p>选取sigmoid函数作为logistic的概率分布函数<br>$$<br>P(Y=1|x) = \frac{\mathrm{exp}(wx+b)}{1+\mathrm{exp}(wx+b)}<br>$$</p><p>$$<br>P(Y=0|x) = \frac{1}{1+\mathrm{exp}(wx+b)}<br>$$</p><p>用线性模型$$wx+b$$逼近对数几率$$log \frac{y}{1-y}$$即<br>$$<br>log\frac{P(Y=1|x)}{P(Y=0|x)} = wx+b<br>$$<br>对于0-1二分类，概率为p和1-p,符合伯努利分布，参数的似然函数为：<br>$$<br>L(W)=P(D|p_{yi})=\prod p^{yi}(1-p)^{1-yi}<br>$$<br>取对数<br>$$<br>logL = \sum  [ y_ilogp + (1-y_i)log(1-p) ]<br>$$</p><p>$$<br>logL = \sum [ y_ilog \frac{p}{1-p} +log(1-p) ]<br>$$</p><p>$$<br>logL = \sum [ y_i(w_ix+b) +\mathrm log(1+e^{w_ix+b)}<br>]<br>$$</p><p>采用梯度下降法和牛顿法可以求解，下面介绍梯度下降法。</p><p>对w求偏导,其中：<br>$$<br>\frac{\partial logL}{\partial w_i} = \sum x_i(yi-p_i)<br>$$<br>得到梯度后就可以迭代下个w<br>$$<br>w_{new}  =  w_{old} + \alpha \frac{\partial logL}{\partial w_i}<br>$$<br>也可以从损失函数的角度理解，$$y_i=1$$和$$y_i = 0$$时对数损失函数$$log(p(y|x))$$如下（将yi为0和1带入对数内部）：</p><p>$$<br>cost  ( h  _ \theta (x) , y )  =  \left{\begin {matrix} -y_ilog(h_ \theta(x)) ! \: \: \: \:  if\:  y = 1\ -(1-y_i)log(1-h_ \theta(x)) ! \: \: \: \:  if\:  y = 0\end{matrix}\right.<br>$$</p><p>联合起来，用一个式子表示：<br>$$<br>cost(h_ \theta(x),y ) = -y_ilog(h_ \theta(x)) -(1-y_i)log(1-h_ \theta(x))<br>$$</p><p>$$<br>cost(h_ \theta(x),y ) =  -\frac{1}{2}\sum (y_ilog(h_ \theta(x)) +(1-y_i)log(1-h_ \theta(x)))<br>$$</p><p>接下来计算和之前一样。</p><p><strong>LR损失函数的形式，其实就是交叉熵。</strong></p><p><strong>损失函数：</strong><br>LR使用对数损失对于sigmoid函数损失函数也可以表示成：<br>$$<br>L(y_i(wx+b)) = log_2(1+e^{y_i(wx+b)})<br>$$<br>损失函数的图像如下：</p><p><img src="/2018/04/22/LR算法/损失函数曲线.png" alt="失函数曲"></p><p><strong>损失函数的意义:</strong></p><p>损失函数的值总大于0-1分段函数，这保证了求解的准确性，使损失函数向正方向变化。可以从损失函数看到，即使预测值与实际值完全一样也就是横坐标大于0，损失函数还是不为0，说明有一个正方向的梯度，这就使LR即使全部预测值分类正确，但还是要优化到分界曲线的位置，如果有异常值，也会将其考虑进去，这点和SVM不同。</p><p><strong>优点</strong>:速度快</p><p><strong>一点思考：</strong><br>sigmoid函数图像接近分段函数，但是这个函数有个缺点，在两端导数很小，所以用梯度下降法时，偏离正确值时可能出现迭代太慢，但是对于sigmoid函数y’ = y(1-y)，对损失函数求导后与sigmoid的导数无关，函数的特性使其方便计算，如果用的是欧式距离，求导会出现y’，对迭代求解不利。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要分析逻辑回归的原理和自己的一点思考。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="https://frankindf.github.io/2018/04/21/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://frankindf.github.io/2018/04/21/SVM支持向量机/</id>
    <published>2018-04-21T10:46:53.000Z</published>
    <updated>2018-04-22T10:51:03.359Z</updated>
    
    <content type="html"><![CDATA[<p>SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。</p><a id="more"></a><p>SVM分类使用sigmoid函数判断结果。</p><p><strong>几何角度的解释</strong>：不同分类之边界之间间隔最大化<br>$$<br> {  min _ { w , b } } \frac{1}{2}\left | w \right |^2<br>$$</p><p>$$<br>s . t .  y_i(w^Tx_i+b)\geqslant 1 , i=1,2…,m.<br>$$</p><p><strong>损失函数最小解释</strong>：$1/2||w||^2$为结构化风险的L2正则项，损失函数为合页函数$l_{0/1}$为$max(0,f(x))$，<br>$$<br>{\min_{w,b}}\frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}(y_i(w^Tx_i+b)-1)<br>$$</p><p>$$<br>s.t     (y_i(w^Tx_i+b)-1)\leqslant 1-\xi _i<br>$$</p><p>$$<br>\xi _ i \geqslant 0<br>$$</p><p>采用拉格朗日法：<br>$$<br>L(w,b,\xi,\alpha,\mu) = \frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}+\sum_{m}^{i=1}\alpha_i[y_i(w^Tx_i+b-1+l_{0/1})]-\sum_{m}^{i=1}{\mu_i\xi_i}<br>$$<br>依次对$w,b,\xi_i$求偏导，得到<br>$$<br>\min_{w,b,\xi}L = \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$<br>原始问题满足KKT条件，对偶问题变为：<br>$$<br>\max \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$</p><p>$$<br>s.t.\space\sum_{j=1}^{N}a_iy_i=0<br>$$</p><p>$$<br>0 \leqslant a_iy_i \leqslant C<br>$$</p><p><strong>惩罚因子</strong>C: C代表对损失函数的惩罚程度，C越大损失函数对优化影响越大，损失函数取的系数C正无穷时，不允许有异常点，即为硬间隔。</p><p><strong>损失函数</strong>： $l_{0/1}$理解为$y_i(w^Tx_i+b)-1&gt;0$时无损失，无需继续优化，这里可以看到$l_{0/1}$是“没有追求”的，只要预测正确就不再努力使预测值向正方向增加，这也使SVM算法可以对异常值有一定“容忍”。</p><p><strong>核函数：</strong>把低维度的数据映射到高维度，用空间变换找数据的分割面，找到分割面后再转换到原空间，即可画出分界边界，例如$x^2+y^2+b$ 在新空间$\alpha+\beta+b$为直线，在原空间线性不可分的点在新空间里线性可分。</p><p>RBF核$K(x,x_i)=exp(\frac{||x-x_i||^2}{σ^2})$会将原始空间映射为无穷维空间,$σ$ 选得大，高次特征上的权衰减变快， <em>σ</em> 选得小，可以将任意的数据映射为线性可分，可能过拟合。<strong>高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。</strong></p><p><strong>SMO算法：</strong>简单理解SMO就是选定$a_i,a_j$将其他项看做常数，可以用$a_i$表示$a_j$,再在[0,C]这个区间求解极值，依次循环。</p><p><strong>注意</strong></p><p>SVM没有处理缺失值的策略，使用SVM前需要将数据进行处理</p><p>例1：SVM示例</p><p>绘制SVM分类示意图，用clf.coef_和clf.intercept获得w和b，用clf.support_vectors_获得支持向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import svm</span><br><span class="line">import pylab as pl</span><br><span class="line">np.random.seed(0) # 使用相同的seed()值，则每次生成的随即数都相同</span><br><span class="line"># 创建可线性分类的数据集与结果集</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20,2) + [2, 2]]</span><br><span class="line">Y = [0] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line"># 构造 SVM 模型</span><br><span class="line">clf = svm.SVC(kernel=&apos;linear&apos;)</span><br><span class="line">clf.fit(X, Y) # 训练 </span><br><span class="line">#利用clf.coef_得到系数w</span><br><span class="line">#clf.intercept_[0]是b</span><br><span class="line">xx = np.linspace(-5, 5) # 在区间[-5, 5] 中产生连续的值，用于画线</span><br><span class="line">yy =  -w[0] * xx / w[1]  - (clf.intercept_[0]) / w[1]</span><br><span class="line">b = clf.support_vectors_[0] # 第一个分类的支持向量，通过调整系数b</span><br><span class="line">yy_down = a * xx + (b[1] - a * b[0])</span><br><span class="line">b = clf.support_vectors_[-1] # 第二个分类中的支持向量</span><br><span class="line">yy_up = a * xx + (b[1] - a * b[0])</span><br><span class="line"></span><br><span class="line">pl.plot(xx, yy, &apos;k-&apos;)</span><br><span class="line">pl.plot(xx, yy_down, &apos;k--&apos;)</span><br><span class="line">pl.plot(xx, yy_up, &apos;k--&apos;)</span><br><span class="line">pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],</span><br><span class="line">           s=80, facecolors=&apos;none&apos;)</span><br><span class="line">pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)</span><br><span class="line"></span><br><span class="line">pl.axis(&apos;tight&apos;)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/21/SVM支持向量机/blog\source\_posts\SVM支持向量机\下载.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow中的softmax</title>
    <link href="https://frankindf.github.io/2018/04/20/tensorflow%E4%B8%AD%E7%9A%84softmax/"/>
    <id>https://frankindf.github.io/2018/04/20/tensorflow中的softmax/</id>
    <published>2018-04-20T13:58:53.000Z</published>
    <updated>2018-04-21T15:14:04.683Z</updated>
    
    <content type="html"><![CDATA[<p>softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。</p><a id="more"></a><p>神经网络中需要将正向传播的结果和的正确结果进行进行对比，softmax函数定义如下，它将分类结果映射到[0,1]这个区间,Vi、Vj表示V中第i，j个元素，ai可以看成第i个分类结果：<br>$$<br>a_i =\frac{e^{V _i}}{\sum_je^{V_j} }<br>$$<br>交叉熵C如下，其中yi代表真实值，ai在这里为softmax：<br>$$<br>C = -\sum_i{y_i{log(a_i)}}<br>$$<br>计算两者之间的差距,每项的Loss可以用下式表示，对于<strong>只有1个正确分类i的分类</strong>,softmax交叉熵计算公式如下：<br>$$<br>L_i = -log(\frac{e^{f _{y_i}}}{\sum_je^{f_{y_j}} }  )<br>$$<br>可以看到，括号里即为softmax的值，它越大，样本的Loss就越小，即与真实分布的差距越小。</p><p>在TensorFlow中交叉熵有下面几种计算方法：</p><ul><li><p>tf.nn.softmax_cross_entropy_with_logits（label, logits）</p><p>logits的shape=(m,n)，label的shape=(m,n),，如果真实分类logits为一维数组，则需要进行one-hot编码。</p></li></ul><ul><li>tf.nn.sparse_softmax_cross_entropy_with_logits（label, logits）</li><li>logits的shape=(m,n)，label的shape=(m,1),若label的shape=(m,n)阶则需要使用argmax函数变为(m,1)</li><li>tf.nn.sigmoid_cross_entropy_with_logits：张量中标量与标量间的运算,求两点分布 之间的交叉熵。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面举例进行交叉熵计算</span></span><br><span class="line"><span class="comment">#分别用softmax_cross_entropy_with_logits、tf.nn.sparse_softmax_cross_entropy_with_logits</span></span><br><span class="line"><span class="comment">#手算和tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment">#label代表真实分布</span></span><br><span class="line"><span class="comment">#采用one-hot编码，即真实分类分别为[3,2,1,1,2]</span></span><br><span class="line">labels = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=np.float32)</span><br><span class="line"><span class="comment">#logits代表前向传播的结果</span></span><br><span class="line"><span class="comment">#代表每个值得权重</span></span><br><span class="line">logits = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>],</span><br><span class="line">                   [<span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                   [<span class="number">8</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>]], dtype=np.float32)</span><br><span class="line">num_classes = labels.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#tf.nn.softmax用来求softmax值，即映射到[0,1]区间上的概率</span></span><br><span class="line">predicts = tf.nn.softmax(logits=logits, dim=<span class="number">-1</span>)</span><br><span class="line">sess.run(predicts)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out:array([[  2.45611509e-03,   6.67641265e-03,   9.90867496e-01],</span><br><span class="line">       [  1.14195190e-01,   8.43794703e-01,   4.20100652e-02],</span><br><span class="line">       [  9.46499169e-01,   6.37746137e-03,   4.71234173e-02],</span><br><span class="line">       [  9.97193694e-01,   2.47179624e-03,   3.34521203e-04],</span><br><span class="line">       [  4.71234173e-02,   9.46499169e-01,   6.37746137e-03]], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.softmax_cross_entropy_with_logits计算交叉熵，labels可以直接用one-hot编码的数组</span></span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)</span><br><span class="line">sess.run(cross_entropy)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.sparse_softmax_cross_entropy_with_logits计算交叉熵，labels要处理为(m,1)维</span></span><br><span class="line">classes = tf.argmax(labels, axis=<span class="number">1</span>)</span><br><span class="line">sess.run(classes)</span><br><span class="line">cross_entropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=classes)</span><br><span class="line">sess.run(cross_entropy2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用reduce_sum计算，其中</span></span><br><span class="line"><span class="comment">#用clip_by_value将取值限制在1e-10以上，防止出log(0)</span></span><br><span class="line"><span class="comment">#用labels/predicts相当于前面加负号</span></span><br><span class="line">labels = tf.clip_by_value(labels, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">predicts = tf.clip_by_value(predicts, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">cross_entropy4 = tf.reduce_sum(labels * tf.log(labels/predicts), axis=<span class="number">1</span>)</span><br><span class="line">sess.run(cross_entropy4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0.8</span></span><br><span class="line">x = <span class="number">1.3</span></span><br><span class="line">cross_entropy3 = tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)</span><br><span class="line"><span class="comment"># tf.nn.sigmoid_cross_entropy_with_logits的具体实现:</span></span><br><span class="line">cross_entropy5 = - z * tf.log(tf.nn.sigmoid(x))  - (<span class="number">1</span>-z) * tf.log(<span class="number">1</span>-tf.nn.sigmoid(x))</span><br><span class="line">sess.run(cross_entropy3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.50100845</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorboard应用</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorboard%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorboard用法/</id>
    <published>2018-04-01T09:28:48.000Z</published>
    <updated>2018-04-22T12:30:36.516Z</updated>
    
    <content type="html"><![CDATA[<p>通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。</p><a id="more"></a><p>标量数据汇总和记录使用</p><p>tf.summary.scalar(tags, values, collections=None, name=None)  </p><p>直接记录变量var的直方图</p><p>tf.summary.histogram(tag, values, collections=None, name=None）  </p><p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ <em>tag</em> /image/0’, ‘ <em>tag</em> /image/1’, etc.，如：input/image/0等</p><p>tf.summary.image(tag, tensor, max_images=3, collections=None, name=None)  </p><p>汇总再进行一次合并</p><p>tf.summary.merge(inputs, collections=None, name=None)</p><p>合并默认图形中的所有汇总</p><p>tf.summaries.merge_all(key=’summaries’)  </p><p>下面看一个TensorBoard的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">通过</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#输入数据，这里用的是mnist数据集</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">#学习率</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">#训练集的训练次数</span><br><span class="line">training_epochs = 25</span><br><span class="line">#每次输入的数据个数</span><br><span class="line">batch_size = 100</span><br><span class="line">display_epoch = 1</span><br><span class="line">#数据存储位置</span><br><span class="line">logs_path = &apos;/tmp/tensorflow_logs/example/&apos;</span><br><span class="line"># mnist数据集是28*28的图片，一共784个像素</span><br><span class="line">#训练集x</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784], name=&apos;InputData&apos;)</span><br><span class="line">#真实值y，对数字分类有10个类别</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10], name=&apos;LabelData&apos;)</span><br><span class="line"></span><br><span class="line">#定义神经网络参数y=wx+b中的x和b</span><br><span class="line">W = tf.Variable(tf.zeros([784, 10]), name=&apos;Weights&apos;)</span><br><span class="line">b = tf.Variable(tf.zeros([10]), name=&apos;Bias&apos;)</span><br><span class="line"></span><br><span class="line"># 在命名空间内定义模型、损失、优化方法</span><br><span class="line">with tf.name_scope(&apos;Model&apos;):</span><br><span class="line">    # 训练模型，采用全连接神经网络</span><br><span class="line">    #matmul进行矩阵相乘</span><br><span class="line">    #softmax得到输出结果</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W) + b) </span><br><span class="line">with tf.name_scope(&apos;Loss&apos;):</span><br><span class="line">#损失函数使用交叉熵，这里也可以用TF自带的函数</span><br><span class="line">    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span><br><span class="line">with tf.name_scope(&apos;SGD&apos;):</span><br><span class="line">    #用梯度下降法对损失函数进行优化</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line">with tf.name_scope(&apos;Accuracy&apos;):</span><br><span class="line">    # 计算准确率，这里argmax的参数就对应分类</span><br><span class="line">    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"># 计算之前需要将全局变量初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"># 添加变量到TensorBoard</span><br><span class="line"># 这里添加了损失、准确率，在输出的文件中可以看到这些参数的变化情况</span><br><span class="line">tf.summary.scalar(&quot;loss&quot;, cost)</span><br><span class="line">tf.summary.scalar(&quot;accuracy&quot;, acc)</span><br><span class="line"># Merge all summaries into a single op</span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">    # 初始化数据</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # 初始化TB，路径为logs_path</span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    # 开始训练过程</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        #所有数据训练一次要分成total_batch个数据集</span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # 训练数据batch_xs,batch_ys</span><br><span class="line">            _, c, summary = sess.run([optimizer, cost, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            # 将数据写入之前初始化的TB中</span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            #计算训练一次的平均损失函数</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # 显示每个Epoch的cost</span><br><span class="line">        if (epoch+1) % display_epoch == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &apos;%04d&apos; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # 测试集上显示准确率</span><br><span class="line">    print(&quot;Accuracy:&quot;, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    print(&quot;Run the command line:\n&quot; \</span><br><span class="line">          &quot;--&gt; tensorboard --logdir=/tmp/tensorflow_logs &quot; \</span><br><span class="line">          &quot;\nThen open http://0.0.0.0:6006/ into your web browser&quot;)</span><br></pre></td></tr></table></figure><p>运行完程序后，输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[out]：Epoch: 0001 cost= 1.183585649</span><br><span class="line">Epoch: 0002 cost= 0.665355021</span><br><span class="line">Epoch: 0003 cost= 0.552772734</span><br><span class="line">Epoch: 0004 cost= 0.498669290</span><br><span class="line">Epoch: 0005 cost= 0.465467163</span><br><span class="line">Epoch: 0006 cost= 0.442601420</span><br><span class="line">Epoch: 0007 cost= 0.425460528</span><br><span class="line">Epoch: 0008 cost= 0.412206892</span><br><span class="line">Epoch: 0009 cost= 0.401397175</span><br><span class="line">Epoch: 0010 cost= 0.392420013</span><br><span class="line">Epoch: 0011 cost= 0.384768393</span><br><span class="line">Epoch: 0012 cost= 0.378172010</span><br><span class="line">Epoch: 0013 cost= 0.372432202</span><br><span class="line">Epoch: 0014 cost= 0.367334918</span><br><span class="line">Epoch: 0015 cost= 0.362694857</span><br><span class="line">Epoch: 0016 cost= 0.358602089</span><br><span class="line">Epoch: 0017 cost= 0.354879144</span><br><span class="line">Epoch: 0018 cost= 0.351492124</span><br><span class="line">Epoch: 0019 cost= 0.348284597</span><br><span class="line">Epoch: 0020 cost= 0.345425291</span><br><span class="line">Epoch: 0021 cost= 0.342768804</span><br><span class="line">Epoch: 0022 cost= 0.340257174</span><br><span class="line">Epoch: 0023 cost= 0.337940815</span><br><span class="line">Epoch: 0024 cost= 0.335757064</span><br><span class="line">Epoch: 0025 cost= 0.333699012</span><br><span class="line">Optimization Finished!</span><br><span class="line">Accuracy: 0.9135</span><br></pre></td></tr></table></figure><p>运行tensorboard –logdir=/tmp/tensorflow_logs，打开浏览器进入<a href="http://localhost:6006查看结果。" target="_blank" rel="noopener">http://localhost:6006查看结果。</a></p><p>​    <img src="/2018/04/01/tensorboard用法/1.png" alt="1"></p><p>‘    <img src="/2018/04/01/tensorboard用法/2.png" alt="2"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow卷积函数</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorflow%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorflow卷积过程/</id>
    <published>2018-04-01T08:36:34.000Z</published>
    <updated>2018-04-22T10:47:18.279Z</updated>
    
    <content type="html"><![CDATA[<p>卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。</p><a id="more"></a><p>tensorflow卷积函数：</p><p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</p><p>input为输入的数组，假设维度为[batch,矩阵高x,矩阵宽y,矩阵深度z]</p><p>filterw为卷积核，维度[矩阵高x,矩阵宽y,矩阵深度z, out_channels]</p><p>strides：卷积时在图像每一维的步长</p><p>padding：是否补全空白”SAME”或”VALID”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#定义全1的矩阵，维数[3,3,1]</span><br><span class="line">x1 = tf.constant(1.0, shape=[1,6,6,2])  </span><br><span class="line">#定义核函数，维度[1,1,1]</span><br><span class="line">#第一通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#第二通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">kernel = tf.constant(1.0, shape=[3,2,2,1])   </span><br><span class="line">y2 = tf.nn.conv2d(x1, kernel,strides=[1,1,1,1],padding=&apos;VALID&apos;)  </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run(session=sess)</span><br><span class="line">    #显示y2</span><br><span class="line">    print(sess.run(tf.shape(y2))</span><br><span class="line">#行6-3+1，列6-2+1</span><br><span class="line">OUT:[1 4 5 1]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>adaboost算法</title>
    <link href="https://frankindf.github.io/2018/03/29/adaboost%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/29/adaboost算法/</id>
    <published>2018-03-29T14:28:37.000Z</published>
    <updated>2018-04-21T16:17:56.985Z</updated>
    
    <content type="html"><![CDATA[<p>​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。</p><a id="more"></a><p>该算法首先定义辅助函数stumpClassify，输入数据，分界值后输出分类的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def stumpClassify(dataIn,dimen,threshVal,threshIneq):</span><br><span class="line">    #输入数据、采用哪列特征分类、分界的限值，是大于还是小于</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    retArray=np.ones((m,1))</span><br><span class="line">    if threshIneq==&apos;lt&apos;:</span><br><span class="line">    #该分类是小于，则大于分界值得数据预测错误，定义为-1</span><br><span class="line">        retArray[dataIn[:,dimen]&lt;=threshVal]=-1.0</span><br><span class="line">    else:</span><br><span class="line">    #该分类是大于，则小于分界值得数据预测错误</span><br><span class="line">        retArray[dataIn[:,dimen]&gt;threshVal]=-1.0</span><br><span class="line">    return retArray</span><br></pre></td></tr></table></figure><p>函数使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#定义输入数据</span><br><span class="line">dataIn=np.arange((12)).reshape(6,2)</span><br><span class="line">Out[43]: </span><br><span class="line">array([[ 0,  1],</span><br><span class="line">       [ 2,  3],</span><br><span class="line">       [ 4,  5],</span><br><span class="line">       [ 6,  7],</span><br><span class="line">       [ 8,  9],</span><br><span class="line">       [10, 11]])</span><br><span class="line">#以小于2的值作为第2列的分界点</span><br><span class="line">stumpClassify(dataIn,1,2,&apos;lt&apos;)</span><br><span class="line">Out[47]: </span><br><span class="line">array([[-1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.]])</span><br></pre></td></tr></table></figure><p>​          有了分类函数，接着需要定义buildStump选取当前情况下，最佳的分类点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">def buildStump(dataIn,classLabel,D):</span><br><span class="line">#D为输入的分类权重，后面会用到</span><br><span class="line">    #输入数据，正确分类，迭代系数</span><br><span class="line">    n=dataIn.shape[1]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    numStep=10.0</span><br><span class="line">    bestStump=&#123;&#125;</span><br><span class="line">    bestClass=np.zeros((m,1))</span><br><span class="line">    minError=np.inf</span><br><span class="line">    for i in range(0,n):</span><br><span class="line">        iAxis=dataIn[:,i]</span><br><span class="line">        minAxis=min(iAxis)</span><br><span class="line">        maxAxis=max(iAxis)</span><br><span class="line">        stepSize=(maxAxis-minAxis)/numStep</span><br><span class="line">    #考虑大于和小于两种分类情况</span><br><span class="line">        for ineq in [&apos;lt&apos;,&apos;gt&apos;]:</span><br><span class="line">            for j in range(-1,int(numStep)+1):</span><br><span class="line">            #每次计算错误值</span><br><span class="line">                threshVal=minAxis+stepSize*float(j)</span><br><span class="line">                #从threshVal值处对i特征值对应数据进行分类</span><br><span class="line">                predictCategory=stumpClassify(dataIn,i,threshVal,ineq)</span><br><span class="line">                #初始化误差值矩阵</span><br><span class="line">                errArr=np.ones((m,1))</span><br><span class="line">                #预测正确的点误差矩阵值为0，其他点为1</span><br><span class="line">                errArr[predictCategory==classLabel]=0</span><br><span class="line">                #误差矩阵乘以权重得到该threshVal的分类总误差，</span><br><span class="line">                errSum=np.dot(D.T,errArr)</span><br><span class="line">                #更新误差最小的threshVal</span><br><span class="line">                if errSum&lt;minError:</span><br><span class="line">                    minError=errSum</span><br><span class="line">                    bestClass=predictCategory.copy()</span><br><span class="line">                    bestStump[&apos;dim&apos;]=i</span><br><span class="line">                    bestStump[&apos;threshVal&apos;]=threshVal</span><br><span class="line">                    bestStump[&apos;ineq&apos;]=ineq</span><br><span class="line">    print(&apos;j&apos;,j,&apos;ineq&apos;,ineq,&apos;split&apos;,i,&apos;\nthresh&apos;,threshVal,&apos;\nerrorsum&apos;,errSum)</span><br><span class="line">    return bestStump,minError,bestClass</span><br><span class="line">    </span><br><span class="line">   classLabel=np.array(([[1],[-1],[-1],[1],[1],[1]]))    </span><br><span class="line">   buildStump(dataIn,classLabel,D)</span><br><span class="line">Out[56]: </span><br><span class="line">#得到当前权重下最佳分类</span><br><span class="line">#分类的特征为0列对应的特征，分类值为4，错误率0.167，分类结果[-1,-1,-1,1,1,1]</span><br><span class="line">(&#123;&apos;dim&apos;: 0, &apos;ineq&apos;: &apos;lt&apos;, &apos;threshVal&apos;: 4.0&#125;,</span><br><span class="line"> array([[ 0.16666667]]),</span><br><span class="line"> array([[-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.]]))</span><br></pre></td></tr></table></figure><p>​        得到了一列一种情况下的分类方法，就相当于有了一个分类器，adaboost的核心是将多个分类器的结果按照权重相加，得到最后的结果。下面通过addBoostTrainDS来训练得到各个分类器的权重。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def addBoostTrainDS(dataIn,classLabels,numIt=40):</span><br><span class="line">    weakClassArr=[]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">#初始化D为1/m，m为数据个数</span><br><span class="line">    D=np.ones((m,1))/m</span><br><span class="line">    aggBestClass=np.zeros((m,1))</span><br><span class="line">    for i in range(numIt):</span><br><span class="line">     #得到每次更新权重值后的最佳分类</span><br><span class="line">        bestStump,error,bestClass=buildStump(dataIn,classLabel,D)</span><br><span class="line">        #求出误差率alpha</span><br><span class="line">        alpha=0.5*np.log((1-error)/max(error,1e-12))</span><br><span class="line">        bestStump[&apos;alpha&apos;]=alpha</span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line">        #每一项的权重</span><br><span class="line">        expon=-1*alpha*(classLabels.T*bestClass)</span><br><span class="line">        print(&apos;expon&apos;,expon)</span><br><span class="line">        D=D*np.exp(expon)</span><br><span class="line">        D=D/D.sum()#更新D值</span><br><span class="line">        aggBestClass+=alpha*bestClass</span><br><span class="line">        #类别为1和-1</span><br><span class="line">        aggErrors=np.ones((m,1))</span><br><span class="line">        samePred=np.where(np.sign(aggBestClass)==classLabel)</span><br><span class="line">        difPred=np.where(np.sign(aggBestClass)!=classLabel)</span><br><span class="line">        aggErrors[samePred]=0</span><br><span class="line">        aggErrors[difPred]=1</span><br><span class="line">        print(&apos;D&apos;,D,&apos;best&apos;,bestClass,&apos;\naggBestClass&apos;,aggBestClass  </span><br><span class="line">        errorRate=aggErrors.sum()/m</span><br><span class="line">        print(&apos;totalError&apos;,errorRate)</span><br><span class="line">        if errorRate==0: break</span><br><span class="line">    return weakClassArr,aggErrors</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://frankindf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>numpy的argsort函数用法</title>
    <link href="https://frankindf.github.io/2018/03/28/numpy%E7%9A%84argsort%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/28/numpy的argsort函数用法/</id>
    <published>2018-03-28T14:17:28.000Z</published>
    <updated>2018-04-21T15:12:39.022Z</updated>
    
    <content type="html"><![CDATA[<p>数组操作时有时如果需要获取数组的索引可以使用argsort函数。</p><a id="more"></a><p>numpy中的argsort可以返回一个索引，具体参数如下</p><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)</p><p>a数组</p><p>axis行或者列，默认为-1，即最后一个维度，0为列，1为行</p><p>kind排序方式{‘quicksort’, ‘mergesort’, ‘heapsort’}</p><p>order</p><p>返回的是排序后的数组在原数组中索引的数组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.array(([[1,6,3,4,2]]))</span><br><span class="line">a.argsort()</span><br><span class="line">#返回的是ARRAY从小到大的索引</span><br><span class="line">Out[35]: array([[0, 4, 2, 3, 1]], dtype=int64)</span><br><span class="line">#默认为行排序的索引</span><br><span class="line">np.argsort(a)</span><br><span class="line">Out[41]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br><span class="line">#axis为1按列排序</span><br><span class="line">np.argsort(a,axis=0)</span><br><span class="line">Out[39]: </span><br><span class="line">array([[0, 1, 0, 0],</span><br><span class="line">       [1, 0, 1, 1]], dtype=int64)</span><br><span class="line">#axis为1，按行排序</span><br><span class="line">np.argsort(a,axis=1)</span><br><span class="line">Out[40]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数组操作时有时如果需要获取数组的索引可以使用argsort函数。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
