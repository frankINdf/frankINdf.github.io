<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阳阳的博客</title>
  
  <subtitle>学无止境</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://frankindf.github.io/"/>
  <updated>2018-04-26T16:01:32.375Z</updated>
  <id>https://frankindf.github.io/</id>
  
  <author>
    <name>阳阳</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>jieba分词实践</title>
    <link href="https://frankindf.github.io/2018/04/27/jieba%E5%88%86%E8%AF%8D%E5%AE%9E%E8%B7%B5/"/>
    <id>https://frankindf.github.io/2018/04/27/jieba分词实践/</id>
    <published>2018-04-26T16:00:22.000Z</published>
    <updated>2018-04-26T16:01:32.375Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">s = u&apos;我想要在中广核的海滩边上走一走&apos;</span><br><span class="line">#cut方法</span><br><span class="line">cut = jieba.cut(s)</span><br><span class="line">list(cut)</span><br><span class="line">[&apos;我&apos;, &apos;想要&apos;, &apos;在&apos;, &apos;中广核&apos;, &apos;的&apos;, &apos;海滩&apos;, &apos;边上&apos;, &apos;走&apos;, &apos;一&apos;, &apos;走&apos;]</span><br><span class="line">s = u&apos;武汉市长江大桥和武汉市长姜大桥。&apos;</span><br></pre></td></tr></table></figure><p>全模式<br>尽量分成更多的词</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&apos;,&apos;.join(jieba.cut(s,cut_all = True))</span><br><span class="line">&apos;武汉,武汉市,市长,长江,长江大桥,大桥,和,武汉,武汉市,市长,姜,大桥&apos;</span><br><span class="line">&apos;,&apos;.join(jieba.cut_for_search(s))</span><br><span class="line">&apos;武汉,武汉市,长江,大桥,长江大桥,和,武汉,武汉市,长姜,大桥&apos;</span><br></pre></td></tr></table></figure><p>获取词性可以用jieba.posseg</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import jieba.posseg as psg</span><br><span class="line">print([(x.word,x.flag) for x in psg.cut(s)])</span><br><span class="line">[(&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长江大桥&apos;, &apos;ns&apos;), (&apos;和&apos;, &apos;c&apos;), (&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长&apos;, &apos;a&apos;), (&apos;姜&apos;, &apos;n&apos;), (&apos;大桥&apos;, &apos;ns&apos;)]</span><br></pre></td></tr></table></figure><p>把姜前面的长识别成了形容词，哈哈哈<br>显示了每个词的词性<br>还可以对分词进行筛选，用startswith，获得名词</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print([(x.word,x.flag) for x in psg.cut(s) if x.flag.startswith(&apos;n&apos;)])</span><br><span class="line">[(&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长江大桥&apos;, &apos;ns&apos;), (&apos;武汉市&apos;, &apos;ns&apos;), (&apos;姜&apos;, &apos;n&apos;), (&apos;大桥&apos;, &apos;ns&apos;)]</span><br></pre></td></tr></table></figure><p>获取词频<br>Counter().most_common(20)<br>添加用户字典，定义用户字典<br>词语    词频 词性<br>姜大桥   5    ‘ns’<br>其中词频是一个数字，词性为自定义的词性，要注意的是词频数字和空格都要是半角的。<br>再进行分词</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jieba.load_userdict(&apos;user_dict.txt&apos;)</span><br><span class="line">print([(x.word,x.flag) for x in psg.cut(s)])</span><br><span class="line">[(&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长江大桥&apos;, &apos;ns&apos;), (&apos;和&apos;, &apos;c&apos;), (&apos;武汉市&apos;, &apos;ns&apos;), (&apos;长&apos;, &apos;a&apos;), (&apos;姜大桥&apos;, &apos;ns&apos;)]姜大桥已经变成人名了</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>隐马尔科夫模型</title>
    <link href="https://frankindf.github.io/2018/04/26/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>https://frankindf.github.io/2018/04/26/隐马尔科夫模型/</id>
    <published>2018-04-26T15:51:53.000Z</published>
    <updated>2018-04-26T15:59:40.206Z</updated>
    
    <content type="html"><![CDATA[<p>使用隐马尔科夫模型可以解决有隐藏状态的观测序列问题，本文是对隐马尔科夫模型的学习记录。</p><a id="more"></a><p>隐马尔科夫要解决的问题：<br>前提：<br>初始状态矩阵$\pi$<br>状态转移矩阵A<br>观测概率矩阵B</p><p>假设条件：<br>齐次假设：$t$时刻的状态只依赖于$t-1$时刻的状态<br>观测独立性假设：任意时刻状态值依赖此刻的马尔科夫链的状态</p><p>在有观测不到的隐藏状态时，解决下面3个问题：<br>1）给定一个观测序列，怎计算这个序列出现的概率,即概率问题<br>2）已经有一个观测序列，怎么估计出A、B、$\pi$使其在该序列下出现可能最大，即学习问题<br>3）在给定观测序列后计算条件概率最大的<strong>状态序列</strong>，预测问题<br>针对问题1通常使用前向算法和后向算法：<br>定义<br>$$<br>\alpha_t(i) = P(O_1,O_2,…,O_t,i_t=q_i|\lambda)<br>$$<br>$A{ij}$代表状态由i转移到j的概率，$b_{j}(o_1)$代表状态j时，观测到$o_1$的概率</p><p>对于初始值,在i状态下观测到o_1<br>$$<br>\alpha_1 = \pi_ib_i(o1)<br>$$</p><p>对于$t=1,2,…,T-1$,t时刻在j状态，t+1时刻在i状态，在t+1观测到$o_{t+1}$的概率如下：<br>$$<br>\alpha_{t+1}(i) = [\sum _{j=1} ^{N} \alpha_{t}(j)a_ji]b_i(o_{t+1})<br>$$</p><p>观测到序列O：<br>$$<br>P(O|\lambda) = \sum_{i=1}^{N}\alpha_{T}(i)<br>$$<br>后向算法：<br>$$<br>\beta_t(i) = P(o_{t+1},o_{t+2},…,o_T|i_t=q_i,\lambda)<br>$$</p><p>$t$时刻的状态为$q_i$,从$t+1$到$T$部分观测序列为$o_t+1,o_t+2,…,o_T$的概率为后向概率<br>最终时刻为T，T+1不存在，所以令$\beta_T(i) = 1$<br>在$t = T-1,T-2,…,1$可以看到从$T-1$开始递减<br>$\beta_t(i) = \sum_{j=1}^{N}a_ijb_j(o_{t+1}\beta_{t+1}(j))$<br>将前面的项代入：<br>$$<br>P(O|\lambda) = \sum_{i=1}^{N}\pi_ib_i(o_1)\beta_1(i)<br>$$<br>可以统一写成<br>$$<br>P(O|\lambda) = \sum_{i=1} ^{N}\sum_{j=1} ^{N}\alpha _t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)<br>$$<br>单个状态概率的计算公式：<br>时刻t处于状态$q_i$的概率，<br>$$<br>\gamma_t(i) = P(i_t = q_i|O,\lambda)=\frac{i_t=q_i,O|\lambda}{P(O|lambda)}<br>$$</p><p>由于前t个数据观测到qi的概率乘以从后面开始观察到$q_i$的概率<br>$$<br>\alpha_ t(i)\beta _t(i) = P(i_t=q_i,O|\lambda)<br>$$<br>得到：<br>$$<br>\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)}<br>$$<br>在t时刻处于q_i,t+1时刻处于q_j的概率<br>$$<br>\xi_t(i,j) = \frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i,i_{t+1}=q_j,O|\lambda)}<br>$$</p><p>$$<br>P(i_t=q_i,i_{t+1}=q_j,O|\lambda) = \alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)<br>$$</p><p>$$<br>\xi_t(i,j) = \frac{\alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)}<br>$$</p><p>Baum-Welch算法<br>1.确定对数似然函数$logP(O,I|\lambda)$<br>2.E步，推断概率分布，并求期望<br>$$<br>Q(\lambda,\bar{\lambda}) = E_Ilog(P(O,I|\lambda))<br>$$</p><p>$$<br>Q(\lambda,\bar{\lambda}) = \sum_{I}logP(O,I|\lambda)P(O,I|\bar{\lambda})<br>$$</p><p>3.M步,计算估计参数下<br>$$<br>\pi_i = \frac{P(O,i_1=i|\bar{\lambda})}{P(O|\bar{\lambda})}=\gamma_1(i)<br>$$</p><p>$$<br>a_{ij} = \frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\bar{lambda})}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\gamma _t(i)}<br>$$</p><p>$$<br>b_j(k) = \frac{\sum_{t=1}^{T}P(O,i_t=j)|\bar{\lambda})I(o_t=v_k)}{\sum_{t=1}^{T}P(O,i_t=j|bar{lambda})} = \frac{\sum _{t=1} ^{T}\gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}<br>$$</p><p>维比特算法<br>贪心算法，如果当前路径为最优路径，则当前路径前一步也是最优路径。<br>初始化<br>$$<br>\delta_1(i) = pi_ib_i(o_1)<br>$$</p><p>$$<br>\psi_1(i)=0<br>$$</p><p>递推<br>$$<br>\delta_t(i) = max_{1\leqslant j \leqslant N} [\delta _{t-1}(j)a_{ji}]b_i(o_1)<br>$$</p><p>$$<br>\psi_t(i) = arg max_{1\leqslant j \leqslant N} [\delta _{t-1}(j)a_{ji}]<br>$$</p><p>终止<br>$$<br>\delta_t(i) = max_{1\leqslant j \leqslant N} \delta _T(i)<br>$$</p><p>$$<br>\psi_t(i) = arg max_{1\leqslant j \leqslant N} [\delta _{T}(i)]<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用隐马尔科夫模型可以解决有隐藏状态的观测序列问题，本文是对隐马尔科夫模型的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>拉格朗日对偶</title>
    <link href="https://frankindf.github.io/2018/04/26/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/"/>
    <id>https://frankindf.github.io/2018/04/26/拉格朗日对偶/</id>
    <published>2018-04-26T15:49:44.000Z</published>
    <updated>2018-04-26T15:50:58.736Z</updated>
    
    <content type="html"><![CDATA[<p>在求解约束条件下最优化的问题时，需要使用拉格朗日乘子法，本文是拉格朗日乘子法的学习记录。</p><a id="more"></a><p>拉格朗日乘子法可以分为3个问题：<br>(1)原始问题，即求f(x)最小值的问题等价于求拉格朗日乘子的最大值的最小值。<br>在求解下面约束条件最小值时<br>$$<br>min f(x)​<br>$$</p><p>$$<br>s.t. c_i(x) \leqslant 0,i=1,2,…,k<br>h_i(x) = 0,i=1,2,…,l<br>$$</p><p>拉格朗日乘子可以写成：<br>$$<br>L = max_{\alpha \beta} [ f(x) + \sum _{i=1} ^{k}\alpha _ic_i(x) + \sum _{j=1} ^{l}\beta _j h_j(x)<br>$$</p><p>max(L)表达式如下<br>$$<br>\theta _p(x) = max _{\alpha ,\beta }\left [ f(x) + \sum _{i=1} ^{k}\alpha _ic_i(x) + \sum _{j=1} ^{l}\beta _j h_j(x) \right ]<br>$$</p><p>若要满足边界条件$c_i(x) \leqslant 0, h_i(x) = 0$<br>$$<br>\sum _{i=1} ^{k}\alpha _ic_i(x) = 0<br>$$</p><p>$$<br>\sum _{j=1} ^{l}\beta _j h_j(x) = 0<br>$$</p><p>否则$\theta _p(x) $最大值为正无穷<br>此时：<br>$min f(x) $与 $min _{x}max{L(x,\alpha,\beta)}$等价。</p><p>(2)对偶问题，对偶问题如果和原始问题等价可以方便求解。<br>$$<br>max _{x}min_{L(x,\alpha,\beta)}<br>$$<br>(3) 什么时候等价？<br>假设函数f和c是凸函数，h是仿射函数<br>对偶问题小于等于$f(x)$,$f(x)$又小于等于原始问题,但是当满足KTT条件时，对偶问题和原始问题是等价的。<br>KTT条件如下:<br>$$<br>\bigtriangledown _xL(x, \alpha ,\beta )=0<br>$$</p><p>$$<br>\bigtriangledown _\alpha L(x, \alpha ,\beta )=0<br>$$</p><p>$$<br>\bigtriangledown _\beta L(x, \alpha ,\beta )=0<br>$$</p><p>$$<br>\alpha_i c_i(x) = 0<br>$$</p><p>$$<br>c_i(x) \leqslant  0<br>$$</p><p>$$<br>\alpha_i \geqslant  0<br>$$</p><p>$$<br>h_j(x)=0<br>$$</p><p>解决了上面问题，就可以将原始问题和对偶问题等价进行求解，问题也变成了凸函数。</p><p>SVM问题里面都是满足KKT条件的，所以SVM里面求取对偶解就相当于求取原问题的解。为什么要求它的对偶呢？<br>因为kernel，通过对偶之后得到一个向量内积的形式，也就是$xTx$这种形式，而这种形式是kernel所擅长处理的。如果没有对偶，就没有后面的kernel映射，SVM也实现不了非线性分割。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在求解约束条件下最优化的问题时，需要使用拉格朗日乘子法，本文是拉格朗日乘子法的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>最大熵算法</title>
    <link href="https://frankindf.github.io/2018/04/23/%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/23/最大熵算法/</id>
    <published>2018-04-23T12:24:20.000Z</published>
    <updated>2018-04-23T13:35:49.659Z</updated>
    
    <content type="html"><![CDATA[<p>最大熵原理和$logistic$、最大熵马科夫模型都有关，本文是最大熵模型的学习记录。</p><a id="more"></a><p>模型在满足已有的约束条件的情况下，约束外的部分熵越大，模型越好。<br>即对于已知部分，要尽可能确定。<br>对于未知的部分，要保证不确定性。<br>任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。<br>举个例子，若只有约束$P_a+P_b+P_c=1$，则最优的模型每次a、b、c出现的概率均相同，因为只有这种情况才没有新增信息。</p><p>最大熵模型就是要使熵最大化，同时要满足经验分布的期望和模型的期望相同即$E_p = E _\tilde p$:<br>$$<br>{max} H(P) = - \sum _{x,y}\tilde{P}(x)P(x|y)logP(x|y)<br>$$</p><p>$$<br>s.t.\: \: E_p(f_i) = E_{\tilde{p}}(f_i) , i=1,2,3…<br>$$</p><p>$$<br>\sum _{y} P(y|x) = 1<br>$$</p><p>其中,$\tilde{p}$为经验分布<br>约束条件下求极值，可以用拉格朗日乘子法进行求解。</p><p>1）写出拉格朗日乘子方程：<br>$$<br>L(P,W) = -H(P) + w_0(1-\sum _{y}P(y|x)) + \sum _{i=1}^{n}(E_{\tilde{p}}(f_i) - E_p(f_i))<br>$$<br>由$E_\tilde   p = \sum \tilde{P}(x,y)f(x,y)$和$E_p =\sum \tilde{P}(x)P(y|x)f(x,y)$可得：<br>$$<br>L(P,W) = \sum\tilde{P}(x) P(y|x)logP(y|x)+w_0(\sum_{y} P(y|x)-1) \\</p><ul><li>w_i(\sum \tilde{P}(x,y)f(x,y)-\sum \tilde{P}(x)P(y|x)f(x,y))<br>$$</li></ul><p>L是相当于求$min _{P\in C}max _{w}L(P,w)$</p><p>根据对偶原理，可以将$minL(P,w)$转化为求$maxL(P,w)$,问题转化为：<br>$$\min _{P\in C}\max _{w}L(P,w) = \max _{w} \min _{P\in C}L(P,w)$$<br>最终需要求解：<br>$$P_w = arg \min _{P\in C}L(P,w) = L(P,w)$$<br>求解方法一：<br>求偏导<br>$$\frac{\partial L}{\partial P(y|x)} =\sum _{x,y}\tilde{P}(x)(logP(y|x)+1-w_0-\sum ^{n}_{i=1}w_if_i(x,y))$$<br>偏导为0，可以求出$P(y|x)$:<br>$$P(y|x) = exp(\sum ^{n} _{i=1}w_if_i(x,y) + w_0-1)$$<br>由$\sum _{y} P(y|x) = 1$可得<br>$$exp(1-w_0) = \sum exp(\sum^{n} _{i=1}w_if_i(x,y)) $$<br>可以得到<br>$$<br>P(y|x) = \frac{exp(\sum ^{n} _{i=1}w_if_i(x,y))}{Z_w(x)}<br>$$<br>代入L(x)可以求得:<br>$$<br>L(P,W) = \sum\tilde{P}(x) P(y|x)logP(y|x) +  w_i(\sum \tilde{P}(x,y)f(x,y)-\sum \tilde{P}(x)P(y|x)f(x,y))<br>\\=\sum_{x,y}\tilde{P}(x) P(y|x)(logP(y|x) - \sum _{i=1} ^{n}w_if_i(x,y))+\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)<br>\\=\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x,y}\tilde{P}(x) P(y|x)logZ_w(x)<br>\\=\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x}\tilde{P}(x) logZ_w(x)<br>$$<br>求解方法二：<br>最大似然估计求解对数似然函数，参考$logistic$回归中的内容。</p><p>条件概率分布$P(X|Y)$的对数似然函数为：<br>$$<br>L_\tilde{p}(P_w) = log \prod_{x,y}P(y|x)^{\tilde{p}(x,y)}<br>$$<br>将$P(y|x) = \frac{exp(\sum ^{n} _{i=1}w_if_i(x,y))}{Z_w(x)}$代入<br>$$<br>L_\tilde{p}(P_w) =\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x}\tilde{P}(x) logZ_w(x)<br>$$<br><strong>一点思考</strong><br>1.最大熵为什么可以保证模型最优化<br>2.最大熵算法和LR的联系</p><p>可以看到当</p><p>3.最大熵模型原理比较重要的推导：</p><ul><li>p(y|x)求和为1</li><li>经验分布和模型的期望相等<br>​</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最大熵原理和$logistic$、最大熵马科夫模型都有关，本文是最大熵模型的学习记录。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://frankindf.github.io/2018/04/22/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://frankindf.github.io/2018/04/22/决策树/</id>
    <published>2018-04-22T15:27:25.000Z</published>
    <updated>2018-04-22T15:36:29.089Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录决策树的相关知识和两个例子。</p><a id="more"></a><p>决策树划分<br><strong>信息增益</strong><br>ID3<br>$$<br>Gain = Ent(D)-\sum_{v = 1}^{V}\frac{|D^v|}{|D|}Ent(D^v)<br>$$</p><p>$$<br>Ent (D) = \sum_{k=1}^{|y|}p_klog_{2} p_k<br>$$<br>缺点：趋向子节点多的分类<br>无法处理连续特征<br>无法处理缺失值<br>可能过拟合<br><strong>信息增益率</strong><br>C4.5<br>$$<br>Gain_ratio = \frac{Gain(D,a)}{IV(a)}<br>$$</p><p>$$<br>IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}<br>$$<br>缺点：趋向子节点少的</p><p><strong>基尼系数</strong><br>CART<br>$$<br>Gini(D) = \sum_{k=1}^{|y|}a\sum_{k’\neq k}p_kp_{k’}<br>$$<br><strong>CART回归树</strong><br>对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。</p><p><strong>剪枝</strong><br>预剪枝：划分前计算准确率，如果分类后的准确率降低就不进行分类<br>后剪枝：划分后坍塌，计算准确率，如果准确率提升就不分类</p><p><strong>连续值的处理：</strong><br>将连续值划分为N个区间，计算信息增益</p><p><strong>缺失值的处理</strong><br>1）对于含有缺失值的数据，分类的属性需要做修正<br>$$<br>Gain = e\times Gain( \tilde{D},a)<br>$$<br>2）对于含有缺失值的数据，以属性各值所占的概率分配到不同的值当中,概率为$\tilde{\gamma _v}$</p><p><strong>多变量决策树</strong><br>非叶节点为分类器$\sum _{d}^{i=1} w_ia_i=t$</p><p><strong>SKLEARN实现决策树</strong><br>class sklearn.tree.DecisionTreeClassifier(<br>criterion=’gini’,  /分类特征选择标准，可以用基尼系数’gini’或者熵’entropy’<br>splitter=’best’,   /遍历所有特征用’best’,随机选取局部最优解用’random’,’random’适用数据量大时<br>max_depth=None,    /最大深度<br>min_samples_split=2,/多少个特征以下停止划分<br> min_samples_leaf=1, /最小叶子数，子节点少于多少进行剪枝<br>min_weight_fraction_leaf=0.0,/针对缺失值，权重小于多少进行剪枝<br>max_features=None, /最大特征数，’auto’,’sqrt’,’log2’或者数字，寻找最佳分割点时的最大特征数。<br>random_state=None,<br>max_leaf_nodes=None,<br>min_impurity_decrease=0.0,\不纯度减少多少就要进行分类<br>min_impurity_split=None, \最小不纯度，即纯度大时不再生成子树<br>class_weight=None, /样本权重,可选’balanced’进行自动计算权重，样本量少的类权重高<br>presort=False /是否预先排序<br>)</p><p>决策树可视化样例，使用sklearn中的鸢尾花数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn import tree</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier, export_graphviz</span><br><span class="line">import subprocess</span><br><span class="line">import pydot</span><br><span class="line"># 使用dot文件进行可视化</span><br><span class="line"># sklearn.tree下面的export_graphviz可以输出dot文件</span><br><span class="line"># 定义决策树，使用默认参数</span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">iris = load_iris()</span><br><span class="line"># 进行训练</span><br><span class="line">clf = clf.fit(iris.data, iris.target)</span><br><span class="line"># 输出tree.dot</span><br><span class="line">tree.export_graphviz(clf, out_file=&apos;tree.dot&apos;)</span><br></pre></td></tr></table></figure><p>生成的决策树如下：</p><p><img src="/2018/04/22/决策树/tree.png" alt="re"></p><p>回归树生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">rng = np.random.RandomState(1)</span><br><span class="line"># rng.rand(80, 1)生成一个80行1列的随机数，范围为0到1</span><br><span class="line">X = np.sort(5 * rng.rand(80, 1), axis=0)</span><br><span class="line"># 生成y并展开</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line"># 以5为步长进行切片，这些位置的数为原来的数字加3*（0.5-随机数）</span><br><span class="line">y[::5] += 3 * (0.5 - rng.rand(16))</span><br><span class="line"># 生成模型判别</span><br><span class="line"># 这里regr_1深度为2，regr_2深度为5</span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=2)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=5)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line"># 测试数据，从0到5生成500个</span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line"># 对x进行预测</span><br><span class="line"># 深度为2的树</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line"># 深度为5的树</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line"># Plot the results</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=20, edgecolor=&quot;black&quot;,</span><br><span class="line">c=&quot;darkorange&quot;, label=&quot;data&quot;)</span><br><span class="line">plt.plot(X_test, y_1, color=&quot;cornflowerblue&quot;,</span><br><span class="line">label=&quot;max_depth=2&quot;, linewidth=2)</span><br><span class="line">plt.plot(X_test, y_2, color=&quot;yellowgreen&quot;, label=&quot;max_depth=5&quot;, linewidth=2)</span><br><span class="line">plt.xlabel(&quot;data&quot;)</span><br><span class="line">plt.ylabel(&quot;target&quot;)</span><br><span class="line">plt.title(&quot;Decision Tree Regression&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/22/决策树/决策树.JPG" alt="策"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录决策树的相关知识和两个例子。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LR算法</title>
    <link href="https://frankindf.github.io/2018/04/22/LR%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/22/LR算法/</id>
    <published>2018-04-22T12:36:45.000Z</published>
    <updated>2018-04-22T15:26:14.301Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要分析逻辑回归的原理和自己的一点思考。</p><a id="more"></a><p>算法原理：</p><p>选取sigmoid函数作为logistic的概率分布函数<br>$$<br>P(Y=1|x) = \frac{\mathrm{exp}(wx+b)}{1+\mathrm{exp}(wx+b)}<br>$$</p><p>$$<br>P(Y=0|x) = \frac{1}{1+\mathrm{exp}(wx+b)}<br>$$</p><p>用线性模型$$wx+b$$逼近对数几率$$log \frac{y}{1-y}$$即<br>$$<br>log\frac{P(Y=1|x)}{P(Y=0|x)} = wx+b<br>$$<br>对于0-1二分类，概率为p和1-p,符合伯努利分布，参数的似然函数为：<br>$$<br>L(W)=P(D|p_{yi})=\prod p^{yi}(1-p)^{1-yi}<br>$$<br>取对数<br>$$<br>logL = \sum  [ y_ilogp + (1-y_i)log(1-p) ]<br>$$</p><p>$$<br>logL = \sum [ y_ilog \frac{p}{1-p} +log(1-p) ]<br>$$</p><p>$$<br>logL = \sum [ y_i(w_ix+b) +\mathrm log(1+e^{w_ix+b)}<br>]<br>$$</p><p>采用梯度下降法和牛顿法可以求解，下面介绍梯度下降法。</p><p>对w求偏导,其中：<br>$$<br>\frac{\partial logL}{\partial w_i} = \sum x_i(yi-p_i)<br>$$<br>得到梯度后就可以迭代下个w<br>$$<br>w_{new}  =  w_{old} + \alpha \frac{\partial logL}{\partial w_i}<br>$$<br>也可以从损失函数的角度理解，$$y_i=1$$和$$y_i = 0$$时对数损失函数$$log(p(y|x))$$如下（将yi为0和1带入对数内部）：</p><p>$$<br>cost  ( h  _ \theta (x) , y )  =  \left{\begin {matrix} -y_ilog(h_ \theta(x)) ! \: \: \: \:  if\:  y = 1\ -(1-y_i)log(1-h_ \theta(x)) ! \: \: \: \:  if\:  y = 0\end{matrix}\right.<br>$$</p><p>联合起来，用一个式子表示：<br>$$<br>cost(h_ \theta(x),y ) = -y_ilog(h_ \theta(x)) -(1-y_i)log(1-h_ \theta(x))<br>$$</p><p>$$<br>cost(h_ \theta(x),y ) =  -\frac{1}{2}\sum (y_ilog(h_ \theta(x)) +(1-y_i)log(1-h_ \theta(x)))<br>$$</p><p>接下来计算和之前一样。</p><p><strong>LR损失函数的形式，其实就是交叉熵。</strong></p><p><strong>损失函数：</strong><br>LR使用对数损失对于sigmoid函数损失函数也可以表示成：<br>$$<br>L(y_i(wx+b)) = log_2(1+e^{y_i(wx+b)})<br>$$<br>损失函数的图像如下：</p><p><img src="/2018/04/22/LR算法/损失函数曲线.png" alt="失函数曲"></p><p><strong>损失函数的意义:</strong></p><p>损失函数的值总大于0-1分段函数，这保证了求解的准确性，使损失函数向正方向变化。可以从损失函数看到，即使预测值与实际值完全一样也就是横坐标大于0，损失函数还是不为0，说明有一个正方向的梯度，这就使LR即使全部预测值分类正确，但还是要优化到分界曲线的位置，如果有异常值，也会将其考虑进去，这点和SVM不同。</p><p><strong>优点</strong>:速度快</p><p><strong>一点思考：</strong><br>sigmoid函数图像接近分段函数，但是这个函数有个缺点，在两端导数很小，所以用梯度下降法时，偏离正确值时可能出现迭代太慢，但是对于sigmoid函数y’ = y(1-y)，对损失函数求导后与sigmoid的导数无关，函数的特性使其方便计算，如果用的是欧式距离，求导会出现y’，对迭代求解不利。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要分析逻辑回归的原理和自己的一点思考。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="https://frankindf.github.io/2018/04/21/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://frankindf.github.io/2018/04/21/SVM支持向量机/</id>
    <published>2018-04-21T10:46:53.000Z</published>
    <updated>2018-04-22T10:51:03.359Z</updated>
    
    <content type="html"><![CDATA[<p>SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。</p><a id="more"></a><p>SVM分类使用sigmoid函数判断结果。</p><p><strong>几何角度的解释</strong>：不同分类之边界之间间隔最大化<br>$$<br> {  min _ { w , b } } \frac{1}{2}\left | w \right |^2<br>$$</p><p>$$<br>s . t .  y_i(w^Tx_i+b)\geqslant 1 , i=1,2…,m.<br>$$</p><p><strong>损失函数最小解释</strong>：$1/2||w||^2$为结构化风险的L2正则项，损失函数为合页函数$l_{0/1}$为$max(0,f(x))$，<br>$$<br>{\min_{w,b}}\frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}(y_i(w^Tx_i+b)-1)<br>$$</p><p>$$<br>s.t     (y_i(w^Tx_i+b)-1)\leqslant 1-\xi _i<br>$$</p><p>$$<br>\xi _ i \geqslant 0<br>$$</p><p>采用拉格朗日法：<br>$$<br>L(w,b,\xi,\alpha,\mu) = \frac{1}{2}\left | w \right |^2 + C\sum_{i=1}^{m} l _{0/1}+\sum_{m}^{i=1}\alpha_i[y_i(w^Tx_i+b-1+l_{0/1})]-\sum_{m}^{i=1}{\mu_i\xi_i}<br>$$<br>依次对$w,b,\xi_i$求偏导，得到<br>$$<br>\min_{w,b,\xi}L = \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$<br>原始问题满足KKT条件，对偶问题变为：<br>$$<br>\max \sum_{i=1}^{N}a_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}a_i a_j y_i y_jK(x_i,x_j)<br>$$</p><p>$$<br>s.t.\space\sum_{j=1}^{N}a_iy_i=0<br>$$</p><p>$$<br>0 \leqslant a_iy_i \leqslant C<br>$$</p><p><strong>惩罚因子</strong>C: C代表对损失函数的惩罚程度，C越大损失函数对优化影响越大，损失函数取的系数C正无穷时，不允许有异常点，即为硬间隔。</p><p><strong>损失函数</strong>： $l_{0/1}$理解为$y_i(w^Tx_i+b)-1&gt;0$时无损失，无需继续优化，这里可以看到$l_{0/1}$是“没有追求”的，只要预测正确就不再努力使预测值向正方向增加，这也使SVM算法可以对异常值有一定“容忍”。</p><p><strong>核函数：</strong>把低维度的数据映射到高维度，用空间变换找数据的分割面，找到分割面后再转换到原空间，即可画出分界边界，例如$x^2+y^2+b$ 在新空间$\alpha+\beta+b$为直线，在原空间线性不可分的点在新空间里线性可分。</p><p>RBF核$K(x,x_i)=exp(\frac{||x-x_i||^2}{σ^2})$会将原始空间映射为无穷维空间,$σ$ 选得大，高次特征上的权衰减变快， <em>σ</em> 选得小，可以将任意的数据映射为线性可分，可能过拟合。<strong>高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。</strong></p><p><strong>SMO算法：</strong>简单理解SMO就是选定$a_i,a_j$将其他项看做常数，可以用$a_i$表示$a_j$,再在[0,C]这个区间求解极值，依次循环。</p><p><strong>注意</strong></p><p>SVM没有处理缺失值的策略，使用SVM前需要将数据进行处理</p><p>例1：SVM示例</p><p>绘制SVM分类示意图，用clf.coef_和clf.intercept获得w和b，用clf.support_vectors_获得支持向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import svm</span><br><span class="line">import pylab as pl</span><br><span class="line">np.random.seed(0) # 使用相同的seed()值，则每次生成的随即数都相同</span><br><span class="line"># 创建可线性分类的数据集与结果集</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20,2) + [2, 2]]</span><br><span class="line">Y = [0] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line"># 构造 SVM 模型</span><br><span class="line">clf = svm.SVC(kernel=&apos;linear&apos;)</span><br><span class="line">clf.fit(X, Y) # 训练 </span><br><span class="line">#利用clf.coef_得到系数w</span><br><span class="line">#clf.intercept_[0]是b</span><br><span class="line">xx = np.linspace(-5, 5) # 在区间[-5, 5] 中产生连续的值，用于画线</span><br><span class="line">yy =  -w[0] * xx / w[1]  - (clf.intercept_[0]) / w[1]</span><br><span class="line">b = clf.support_vectors_[0] # 第一个分类的支持向量，通过调整系数b</span><br><span class="line">yy_down = a * xx + (b[1] - a * b[0])</span><br><span class="line">b = clf.support_vectors_[-1] # 第二个分类中的支持向量</span><br><span class="line">yy_up = a * xx + (b[1] - a * b[0])</span><br><span class="line"></span><br><span class="line">pl.plot(xx, yy, &apos;k-&apos;)</span><br><span class="line">pl.plot(xx, yy_down, &apos;k--&apos;)</span><br><span class="line">pl.plot(xx, yy_up, &apos;k--&apos;)</span><br><span class="line">pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],</span><br><span class="line">           s=80, facecolors=&apos;none&apos;)</span><br><span class="line">pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)</span><br><span class="line"></span><br><span class="line">pl.axis(&apos;tight&apos;)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/04/21/SVM支持向量机/blog\source\_posts\SVM支持向量机\下载.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVM是经典机器学习方法，本文主要是对SVM原理、求解等方面的理解。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow中的softmax</title>
    <link href="https://frankindf.github.io/2018/04/20/tensorflow%E4%B8%AD%E7%9A%84softmax/"/>
    <id>https://frankindf.github.io/2018/04/20/tensorflow中的softmax/</id>
    <published>2018-04-20T13:58:53.000Z</published>
    <updated>2018-04-21T15:14:04.683Z</updated>
    
    <content type="html"><![CDATA[<p>softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。</p><a id="more"></a><p>神经网络中需要将正向传播的结果和的正确结果进行进行对比，softmax函数定义如下，它将分类结果映射到[0,1]这个区间,Vi、Vj表示V中第i，j个元素，ai可以看成第i个分类结果：<br>$$<br>a_i =\frac{e^{V _i}}{\sum_je^{V_j} }<br>$$<br>交叉熵C如下，其中yi代表真实值，ai在这里为softmax：<br>$$<br>C = -\sum_i{y_i{log(a_i)}}<br>$$<br>计算两者之间的差距,每项的Loss可以用下式表示，对于<strong>只有1个正确分类i的分类</strong>,softmax交叉熵计算公式如下：<br>$$<br>L_i = -log(\frac{e^{f _{y_i}}}{\sum_je^{f_{y_j}} }  )<br>$$<br>可以看到，括号里即为softmax的值，它越大，样本的Loss就越小，即与真实分布的差距越小。</p><p>在TensorFlow中交叉熵有下面几种计算方法：</p><ul><li><p>tf.nn.softmax_cross_entropy_with_logits（label, logits）</p><p>logits的shape=(m,n)，label的shape=(m,n),，如果真实分类logits为一维数组，则需要进行one-hot编码。</p></li></ul><ul><li>tf.nn.sparse_softmax_cross_entropy_with_logits（label, logits）</li><li>logits的shape=(m,n)，label的shape=(m,1),若label的shape=(m,n)阶则需要使用argmax函数变为(m,1)</li><li>tf.nn.sigmoid_cross_entropy_with_logits：张量中标量与标量间的运算,求两点分布 之间的交叉熵。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面举例进行交叉熵计算</span></span><br><span class="line"><span class="comment">#分别用softmax_cross_entropy_with_logits、tf.nn.sparse_softmax_cross_entropy_with_logits</span></span><br><span class="line"><span class="comment">#手算和tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment">#label代表真实分布</span></span><br><span class="line"><span class="comment">#采用one-hot编码，即真实分类分别为[3,2,1,1,2]</span></span><br><span class="line">labels = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=np.float32)</span><br><span class="line"><span class="comment">#logits代表前向传播的结果</span></span><br><span class="line"><span class="comment">#代表每个值得权重</span></span><br><span class="line">logits = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>],</span><br><span class="line">                   [<span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                   [<span class="number">8</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>]], dtype=np.float32)</span><br><span class="line">num_classes = labels.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#tf.nn.softmax用来求softmax值，即映射到[0,1]区间上的概率</span></span><br><span class="line">predicts = tf.nn.softmax(logits=logits, dim=<span class="number">-1</span>)</span><br><span class="line">sess.run(predicts)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out:array([[  2.45611509e-03,   6.67641265e-03,   9.90867496e-01],</span><br><span class="line">       [  1.14195190e-01,   8.43794703e-01,   4.20100652e-02],</span><br><span class="line">       [  9.46499169e-01,   6.37746137e-03,   4.71234173e-02],</span><br><span class="line">       [  9.97193694e-01,   2.47179624e-03,   3.34521203e-04],</span><br><span class="line">       [  4.71234173e-02,   9.46499169e-01,   6.37746137e-03]], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.softmax_cross_entropy_with_logits计算交叉熵，labels可以直接用one-hot编码的数组</span></span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)</span><br><span class="line">sess.run(cross_entropy)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.nn.sparse_softmax_cross_entropy_with_logits计算交叉熵，labels要处理为(m,1)维</span></span><br><span class="line">classes = tf.argmax(labels, axis=<span class="number">1</span>)</span><br><span class="line">sess.run(classes)</span><br><span class="line">cross_entropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=classes)</span><br><span class="line">sess.run(cross_entropy2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out:array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用reduce_sum计算，其中</span></span><br><span class="line"><span class="comment">#用clip_by_value将取值限制在1e-10以上，防止出log(0)</span></span><br><span class="line"><span class="comment">#用labels/predicts相当于前面加负号</span></span><br><span class="line">labels = tf.clip_by_value(labels, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">predicts = tf.clip_by_value(predicts, <span class="number">1e-10</span>, <span class="number">1.0</span>)</span><br><span class="line">cross_entropy4 = tf.reduce_sum(labels * tf.log(labels/predicts), axis=<span class="number">1</span>)</span><br><span class="line">sess.run(cross_entropy4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 0.00917445,  0.16984604,  0.05498521,  0.00281022,  0.05498521], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0.8</span></span><br><span class="line">x = <span class="number">1.3</span></span><br><span class="line">cross_entropy3 = tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)</span><br><span class="line"><span class="comment"># tf.nn.sigmoid_cross_entropy_with_logits的具体实现:</span></span><br><span class="line">cross_entropy5 = - z * tf.log(tf.nn.sigmoid(x))  - (<span class="number">1</span>-z) * tf.log(<span class="number">1</span>-tf.nn.sigmoid(x))</span><br><span class="line">sess.run(cross_entropy3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.50100845</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;softmax函数在机器学习中应用很广泛，本文主要记录TensorFlow中的交叉熵计算函数用法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorboard应用</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorboard%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorboard用法/</id>
    <published>2018-04-01T09:28:48.000Z</published>
    <updated>2018-04-22T12:30:36.516Z</updated>
    
    <content type="html"><![CDATA[<p>通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。</p><a id="more"></a><p>标量数据汇总和记录使用</p><p>tf.summary.scalar(tags, values, collections=None, name=None)  </p><p>直接记录变量var的直方图</p><p>tf.summary.histogram(tag, values, collections=None, name=None）  </p><p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ <em>tag</em> /image/0’, ‘ <em>tag</em> /image/1’, etc.，如：input/image/0等</p><p>tf.summary.image(tag, tensor, max_images=3, collections=None, name=None)  </p><p>汇总再进行一次合并</p><p>tf.summary.merge(inputs, collections=None, name=None)</p><p>合并默认图形中的所有汇总</p><p>tf.summaries.merge_all(key=’summaries’)  </p><p>下面看一个TensorBoard的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">通过</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#输入数据，这里用的是mnist数据集</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">#学习率</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">#训练集的训练次数</span><br><span class="line">training_epochs = 25</span><br><span class="line">#每次输入的数据个数</span><br><span class="line">batch_size = 100</span><br><span class="line">display_epoch = 1</span><br><span class="line">#数据存储位置</span><br><span class="line">logs_path = &apos;/tmp/tensorflow_logs/example/&apos;</span><br><span class="line"># mnist数据集是28*28的图片，一共784个像素</span><br><span class="line">#训练集x</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784], name=&apos;InputData&apos;)</span><br><span class="line">#真实值y，对数字分类有10个类别</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10], name=&apos;LabelData&apos;)</span><br><span class="line"></span><br><span class="line">#定义神经网络参数y=wx+b中的x和b</span><br><span class="line">W = tf.Variable(tf.zeros([784, 10]), name=&apos;Weights&apos;)</span><br><span class="line">b = tf.Variable(tf.zeros([10]), name=&apos;Bias&apos;)</span><br><span class="line"></span><br><span class="line"># 在命名空间内定义模型、损失、优化方法</span><br><span class="line">with tf.name_scope(&apos;Model&apos;):</span><br><span class="line">    # 训练模型，采用全连接神经网络</span><br><span class="line">    #matmul进行矩阵相乘</span><br><span class="line">    #softmax得到输出结果</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W) + b) </span><br><span class="line">with tf.name_scope(&apos;Loss&apos;):</span><br><span class="line">#损失函数使用交叉熵，这里也可以用TF自带的函数</span><br><span class="line">    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span><br><span class="line">with tf.name_scope(&apos;SGD&apos;):</span><br><span class="line">    #用梯度下降法对损失函数进行优化</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line">with tf.name_scope(&apos;Accuracy&apos;):</span><br><span class="line">    # 计算准确率，这里argmax的参数就对应分类</span><br><span class="line">    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"># 计算之前需要将全局变量初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"># 添加变量到TensorBoard</span><br><span class="line"># 这里添加了损失、准确率，在输出的文件中可以看到这些参数的变化情况</span><br><span class="line">tf.summary.scalar(&quot;loss&quot;, cost)</span><br><span class="line">tf.summary.scalar(&quot;accuracy&quot;, acc)</span><br><span class="line"># Merge all summaries into a single op</span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">    # 初始化数据</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # 初始化TB，路径为logs_path</span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    # 开始训练过程</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        #所有数据训练一次要分成total_batch个数据集</span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # 训练数据batch_xs,batch_ys</span><br><span class="line">            _, c, summary = sess.run([optimizer, cost, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            # 将数据写入之前初始化的TB中</span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            #计算训练一次的平均损失函数</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # 显示每个Epoch的cost</span><br><span class="line">        if (epoch+1) % display_epoch == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &apos;%04d&apos; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # 测试集上显示准确率</span><br><span class="line">    print(&quot;Accuracy:&quot;, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    print(&quot;Run the command line:\n&quot; \</span><br><span class="line">          &quot;--&gt; tensorboard --logdir=/tmp/tensorflow_logs &quot; \</span><br><span class="line">          &quot;\nThen open http://0.0.0.0:6006/ into your web browser&quot;)</span><br></pre></td></tr></table></figure><p>运行完程序后，输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[out]：Epoch: 0001 cost= 1.183585649</span><br><span class="line">Epoch: 0002 cost= 0.665355021</span><br><span class="line">Epoch: 0003 cost= 0.552772734</span><br><span class="line">Epoch: 0004 cost= 0.498669290</span><br><span class="line">Epoch: 0005 cost= 0.465467163</span><br><span class="line">Epoch: 0006 cost= 0.442601420</span><br><span class="line">Epoch: 0007 cost= 0.425460528</span><br><span class="line">Epoch: 0008 cost= 0.412206892</span><br><span class="line">Epoch: 0009 cost= 0.401397175</span><br><span class="line">Epoch: 0010 cost= 0.392420013</span><br><span class="line">Epoch: 0011 cost= 0.384768393</span><br><span class="line">Epoch: 0012 cost= 0.378172010</span><br><span class="line">Epoch: 0013 cost= 0.372432202</span><br><span class="line">Epoch: 0014 cost= 0.367334918</span><br><span class="line">Epoch: 0015 cost= 0.362694857</span><br><span class="line">Epoch: 0016 cost= 0.358602089</span><br><span class="line">Epoch: 0017 cost= 0.354879144</span><br><span class="line">Epoch: 0018 cost= 0.351492124</span><br><span class="line">Epoch: 0019 cost= 0.348284597</span><br><span class="line">Epoch: 0020 cost= 0.345425291</span><br><span class="line">Epoch: 0021 cost= 0.342768804</span><br><span class="line">Epoch: 0022 cost= 0.340257174</span><br><span class="line">Epoch: 0023 cost= 0.337940815</span><br><span class="line">Epoch: 0024 cost= 0.335757064</span><br><span class="line">Epoch: 0025 cost= 0.333699012</span><br><span class="line">Optimization Finished!</span><br><span class="line">Accuracy: 0.9135</span><br></pre></td></tr></table></figure><p>运行tensorboard –logdir=/tmp/tensorflow_logs，打开浏览器进入<a href="http://localhost:6006查看结果。" target="_blank" rel="noopener">http://localhost:6006查看结果。</a></p><p>​    <img src="/2018/04/01/tensorboard用法/1.png" alt="1"></p><p>‘    <img src="/2018/04/01/tensorboard用法/2.png" alt="2"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过TensorBoard可以对神经网络结构和参数收敛有更深刻的理解，本文记录了TensorBoard的应用方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow卷积函数</title>
    <link href="https://frankindf.github.io/2018/04/01/tensorflow%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B/"/>
    <id>https://frankindf.github.io/2018/04/01/tensorflow卷积过程/</id>
    <published>2018-04-01T08:36:34.000Z</published>
    <updated>2018-04-22T10:47:18.279Z</updated>
    
    <content type="html"><![CDATA[<p>卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。</p><a id="more"></a><p>tensorflow卷积函数：</p><p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</p><p>input为输入的数组，假设维度为[batch,矩阵高x,矩阵宽y,矩阵深度z]</p><p>filterw为卷积核，维度[矩阵高x,矩阵宽y,矩阵深度z, out_channels]</p><p>strides：卷积时在图像每一维的步长</p><p>padding：是否补全空白”SAME”或”VALID”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#定义全1的矩阵，维数[3,3,1]</span><br><span class="line">x1 = tf.constant(1.0, shape=[1,6,6,2])  </span><br><span class="line">#定义核函数，维度[1,1,1]</span><br><span class="line">#第一通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#第二通道</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">#11</span><br><span class="line">kernel = tf.constant(1.0, shape=[3,2,2,1])   </span><br><span class="line">y2 = tf.nn.conv2d(x1, kernel,strides=[1,1,1,1],padding=&apos;VALID&apos;)  </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run(session=sess)</span><br><span class="line">    #显示y2</span><br><span class="line">    print(sess.run(tf.shape(y2))</span><br><span class="line">#行6-3+1，列6-2+1</span><br><span class="line">OUT:[1 4 5 1]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积是卷积神经网络CNN中的重要操作，下面是TensorFlow中卷积函数的用法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://frankindf.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>adaboost算法</title>
    <link href="https://frankindf.github.io/2018/03/29/adaboost%E7%AE%97%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/29/adaboost算法/</id>
    <published>2018-03-29T14:28:37.000Z</published>
    <updated>2018-04-21T16:17:56.985Z</updated>
    
    <content type="html"><![CDATA[<p>​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。</p><a id="more"></a><p>该算法首先定义辅助函数stumpClassify，输入数据，分界值后输出分类的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def stumpClassify(dataIn,dimen,threshVal,threshIneq):</span><br><span class="line">    #输入数据、采用哪列特征分类、分界的限值，是大于还是小于</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    retArray=np.ones((m,1))</span><br><span class="line">    if threshIneq==&apos;lt&apos;:</span><br><span class="line">    #该分类是小于，则大于分界值得数据预测错误，定义为-1</span><br><span class="line">        retArray[dataIn[:,dimen]&lt;=threshVal]=-1.0</span><br><span class="line">    else:</span><br><span class="line">    #该分类是大于，则小于分界值得数据预测错误</span><br><span class="line">        retArray[dataIn[:,dimen]&gt;threshVal]=-1.0</span><br><span class="line">    return retArray</span><br></pre></td></tr></table></figure><p>函数使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#定义输入数据</span><br><span class="line">dataIn=np.arange((12)).reshape(6,2)</span><br><span class="line">Out[43]: </span><br><span class="line">array([[ 0,  1],</span><br><span class="line">       [ 2,  3],</span><br><span class="line">       [ 4,  5],</span><br><span class="line">       [ 6,  7],</span><br><span class="line">       [ 8,  9],</span><br><span class="line">       [10, 11]])</span><br><span class="line">#以小于2的值作为第2列的分界点</span><br><span class="line">stumpClassify(dataIn,1,2,&apos;lt&apos;)</span><br><span class="line">Out[47]: </span><br><span class="line">array([[-1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.],</span><br><span class="line">       [ 1.]])</span><br></pre></td></tr></table></figure><p>​          有了分类函数，接着需要定义buildStump选取当前情况下，最佳的分类点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">def buildStump(dataIn,classLabel,D):</span><br><span class="line">#D为输入的分类权重，后面会用到</span><br><span class="line">    #输入数据，正确分类，迭代系数</span><br><span class="line">    n=dataIn.shape[1]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">    numStep=10.0</span><br><span class="line">    bestStump=&#123;&#125;</span><br><span class="line">    bestClass=np.zeros((m,1))</span><br><span class="line">    minError=np.inf</span><br><span class="line">    for i in range(0,n):</span><br><span class="line">        iAxis=dataIn[:,i]</span><br><span class="line">        minAxis=min(iAxis)</span><br><span class="line">        maxAxis=max(iAxis)</span><br><span class="line">        stepSize=(maxAxis-minAxis)/numStep</span><br><span class="line">    #考虑大于和小于两种分类情况</span><br><span class="line">        for ineq in [&apos;lt&apos;,&apos;gt&apos;]:</span><br><span class="line">            for j in range(-1,int(numStep)+1):</span><br><span class="line">            #每次计算错误值</span><br><span class="line">                threshVal=minAxis+stepSize*float(j)</span><br><span class="line">                #从threshVal值处对i特征值对应数据进行分类</span><br><span class="line">                predictCategory=stumpClassify(dataIn,i,threshVal,ineq)</span><br><span class="line">                #初始化误差值矩阵</span><br><span class="line">                errArr=np.ones((m,1))</span><br><span class="line">                #预测正确的点误差矩阵值为0，其他点为1</span><br><span class="line">                errArr[predictCategory==classLabel]=0</span><br><span class="line">                #误差矩阵乘以权重得到该threshVal的分类总误差，</span><br><span class="line">                errSum=np.dot(D.T,errArr)</span><br><span class="line">                #更新误差最小的threshVal</span><br><span class="line">                if errSum&lt;minError:</span><br><span class="line">                    minError=errSum</span><br><span class="line">                    bestClass=predictCategory.copy()</span><br><span class="line">                    bestStump[&apos;dim&apos;]=i</span><br><span class="line">                    bestStump[&apos;threshVal&apos;]=threshVal</span><br><span class="line">                    bestStump[&apos;ineq&apos;]=ineq</span><br><span class="line">    print(&apos;j&apos;,j,&apos;ineq&apos;,ineq,&apos;split&apos;,i,&apos;\nthresh&apos;,threshVal,&apos;\nerrorsum&apos;,errSum)</span><br><span class="line">    return bestStump,minError,bestClass</span><br><span class="line">    </span><br><span class="line">   classLabel=np.array(([[1],[-1],[-1],[1],[1],[1]]))    </span><br><span class="line">   buildStump(dataIn,classLabel,D)</span><br><span class="line">Out[56]: </span><br><span class="line">#得到当前权重下最佳分类</span><br><span class="line">#分类的特征为0列对应的特征，分类值为4，错误率0.167，分类结果[-1,-1,-1,1,1,1]</span><br><span class="line">(&#123;&apos;dim&apos;: 0, &apos;ineq&apos;: &apos;lt&apos;, &apos;threshVal&apos;: 4.0&#125;,</span><br><span class="line"> array([[ 0.16666667]]),</span><br><span class="line"> array([[-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [-1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.],</span><br><span class="line">        [ 1.]]))</span><br></pre></td></tr></table></figure><p>​        得到了一列一种情况下的分类方法，就相当于有了一个分类器，adaboost的核心是将多个分类器的结果按照权重相加，得到最后的结果。下面通过addBoostTrainDS来训练得到各个分类器的权重。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def addBoostTrainDS(dataIn,classLabels,numIt=40):</span><br><span class="line">    weakClassArr=[]</span><br><span class="line">    m=dataIn.shape[0]</span><br><span class="line">#初始化D为1/m，m为数据个数</span><br><span class="line">    D=np.ones((m,1))/m</span><br><span class="line">    aggBestClass=np.zeros((m,1))</span><br><span class="line">    for i in range(numIt):</span><br><span class="line">     #得到每次更新权重值后的最佳分类</span><br><span class="line">        bestStump,error,bestClass=buildStump(dataIn,classLabel,D)</span><br><span class="line">        #求出误差率alpha</span><br><span class="line">        alpha=0.5*np.log((1-error)/max(error,1e-12))</span><br><span class="line">        bestStump[&apos;alpha&apos;]=alpha</span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line">        #每一项的权重</span><br><span class="line">        expon=-1*alpha*(classLabels.T*bestClass)</span><br><span class="line">        print(&apos;expon&apos;,expon)</span><br><span class="line">        D=D*np.exp(expon)</span><br><span class="line">        D=D/D.sum()#更新D值</span><br><span class="line">        aggBestClass+=alpha*bestClass</span><br><span class="line">        #类别为1和-1</span><br><span class="line">        aggErrors=np.ones((m,1))</span><br><span class="line">        samePred=np.where(np.sign(aggBestClass)==classLabel)</span><br><span class="line">        difPred=np.where(np.sign(aggBestClass)!=classLabel)</span><br><span class="line">        aggErrors[samePred]=0</span><br><span class="line">        aggErrors[difPred]=1</span><br><span class="line">        print(&apos;D&apos;,D,&apos;best&apos;,bestClass,&apos;\naggBestClass&apos;,aggBestClass  </span><br><span class="line">        errorRate=aggErrors.sum()/m</span><br><span class="line">        print(&apos;totalError&apos;,errorRate)</span><br><span class="line">        if errorRate==0: break</span><br><span class="line">    return weakClassArr,aggErrors</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        adaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://frankindf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>numpy的argsort函数用法</title>
    <link href="https://frankindf.github.io/2018/03/28/numpy%E7%9A%84argsort%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/"/>
    <id>https://frankindf.github.io/2018/03/28/numpy的argsort函数用法/</id>
    <published>2018-03-28T14:17:28.000Z</published>
    <updated>2018-04-21T15:12:39.022Z</updated>
    
    <content type="html"><![CDATA[<p>数组操作时有时如果需要获取数组的索引可以使用argsort函数。</p><a id="more"></a><p>numpy中的argsort可以返回一个索引，具体参数如下</p><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)</p><p>a数组</p><p>axis行或者列，默认为-1，即最后一个维度，0为列，1为行</p><p>kind排序方式{‘quicksort’, ‘mergesort’, ‘heapsort’}</p><p>order</p><p>返回的是排序后的数组在原数组中索引的数组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.array(([[1,6,3,4,2]]))</span><br><span class="line">a.argsort()</span><br><span class="line">#返回的是ARRAY从小到大的索引</span><br><span class="line">Out[35]: array([[0, 4, 2, 3, 1]], dtype=int64)</span><br><span class="line">#默认为行排序的索引</span><br><span class="line">np.argsort(a)</span><br><span class="line">Out[41]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br><span class="line">#axis为1按列排序</span><br><span class="line">np.argsort(a,axis=0)</span><br><span class="line">Out[39]: </span><br><span class="line">array([[0, 1, 0, 0],</span><br><span class="line">       [1, 0, 1, 1]], dtype=int64)</span><br><span class="line">#axis为1，按行排序</span><br><span class="line">np.argsort(a,axis=1)</span><br><span class="line">Out[40]: </span><br><span class="line">array([[0, 3, 2, 1],</span><br><span class="line">       [1, 0, 3, 2]], dtype=int64)</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数组操作时有时如果需要获取数组的索引可以使用argsort函数。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
